{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import re\n",
    "\n",
    "# Challenge: https://adventofcode.com/2020/day/7\n",
    "    \n",
    "# Import csv\n",
    "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
    "\n",
    "column = ['luggage_rules']\n",
    "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
    "                       header = None, \n",
    "                       names = column, \n",
    "                       skip_blank_lines=False\n",
    "                      )\n",
    "\n",
    "bag_parents = []\n",
    "bag_children = []\n",
    "luggage_rule_string = 'contain'\n",
    "characters_to_remove = [' bags', ' bag', '.']\n",
    "#,'1','2','3','4','5','6','7','8','9','  ']\n",
    "\n",
    "for x in range(len(day7data)):\n",
    "    # First, find the position of the separator between key and value\n",
    "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
    "    # Now take the left-most number of characters based on that value\n",
    "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
    "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
    "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
    "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
    "    # Has to be an easier way!\n",
    "    for char in characters_to_remove:\n",
    "        bag_parent = bag_parent.replace(char, '').strip()\n",
    "        bag_child = bag_child.replace(char, '').strip()\n",
    "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
    "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
    "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
    "    # turn the values into a list\n",
    "    bag_child = bag_child.split(',')\n",
    "    bag_parents.append(bag_parent)\n",
    "    bag_children.append(bag_child)\n",
    "\n",
    "day7data['bag_parents'] = bag_parents\n",
    "day7data['bag_children'] = bag_children\n",
    "\n",
    "#print(day7data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now break apart the lists of children into individual rows\n",
    "\n",
    "pairwise_df = pd.DataFrame()\n",
    "bag_quantity_list = []\n",
    "bag_parent_list = []\n",
    "bag_child_list = []\n",
    "\n",
    "for x in range(len(day7data)):\n",
    "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
    "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
    "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
    "        # Now separate the quantity from the name of the color, same exercise as above\n",
    "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
    "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
    "\n",
    "pairwise_df['bag_parent'] = bag_parent_list\n",
    "pairwise_df['bag_child'] = bag_child_list\n",
    "pairwise_df['bag_quantity'] = bag_quantity_list\n",
    "\n",
    "# Replace 'no' with zero in the bag_quantity field\n",
    "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
    "# Then make sure it's an integer\n",
    "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
    "#print(pairwise_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 1 answer: 222\n"
     ]
    }
   ],
   "source": [
    "# for part 1:\n",
    "\n",
    "def number_of_bags(bag_color,master_list):\n",
    "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
    "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
    "    parents = []\n",
    "    for z in range(len(bag_color)):\n",
    "        # filter pairwise_df up to any rows that have the bag color as the child\n",
    "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
    "        # Add the parents to the list of parents, and keep the list unique\n",
    "        parents.extend(subset['bag_parent'].tolist())\n",
    "        parents = list(set(parents))\n",
    "#        print('bag color:',bag_color[z])\n",
    "#        print('parents:',parents)\n",
    "        # The master_list just keeps growing with the number of parents\n",
    "        master_list.extend(parents)\n",
    "#        print('master list:',master_list)\n",
    "#        print('')\n",
    "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
    "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
    "    if len(parents) > 0:\n",
    "        number_of_bags(parents,master_list)\n",
    "    return(list(set(master_list)))\n",
    "\n",
    "final_master_list = number_of_bags(['shiny gold'],[])\n",
    "print('Part 1 answer:',len(final_master_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 answer: 28476\n"
     ]
    }
   ],
   "source": [
    "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
    "\n",
    "full_dict = {}\n",
    "\n",
    "target = ['shiny gold']\n",
    "\n",
    "def determine_fit(target):\n",
    "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
    "#    print(subset)\n",
    "    # Create the next round of children:\n",
    "    next_round = list(set(subset['bag_child'].tolist()))\n",
    "    for k in range(len(subset)):\n",
    "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
    "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
    "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
    "        # Now we increment each child's key value as we iterate through the data frame.\n",
    "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
    "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
    "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
    "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
    "        else:\n",
    "            multiplier = 1\n",
    "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
    "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
    "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
    "#        print('multiplier:',multiplier)\n",
    "    if len(next_round) > 0:\n",
    "        determine_fit(next_round)\n",
    "#    print(full_dict)\n",
    "    return(sum(full_dict.values()))\n",
    "\n",
    "part_2_answer = determine_fit(['shiny gold'])\n",
    "\n",
    "print('Part 2 answer:',part_2_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_append:     bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "676  drab gray    clear violet             4                    4\n",
      "677  drab gray    mirrored red             3                    3\n",
      "678  drab gray    light silver             1                    1\n",
      "679  drab gray  wavy turquoise             1                    1\n",
      "to_append:        bag_parent    bag_child  bag_quantity  multiplied_quantity\n",
      "1134  light coral  pale bronze             2                    2\n",
      "1135  light coral   clear aqua             1                    1\n",
      "level_bom:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "676     drab gray    clear violet             4                   16\n",
      "677     drab gray    mirrored red             3                   12\n",
      "678     drab gray    light silver             1                    4\n",
      "679     drab gray  wavy turquoise             1                    4\n",
      "1134  light coral     pale bronze             2                    8\n",
      "1135  light coral      clear aqua             1                    4\n",
      "to_append:        bag_parent        bag_child  bag_quantity  multiplied_quantity\n",
      "555  clear violet   wavy turquoise             5                    5\n",
      "556  clear violet      light black             5                    5\n",
      "557  clear violet  mirrored indigo             1                    1\n",
      "558  clear violet      faded white             2                    2\n",
      "to_append:        bag_parent    bag_child  bag_quantity  multiplied_quantity\n",
      "315  mirrored red  muted beige             5                    5\n",
      "316  mirrored red  faded white             2                    2\n",
      "to_append:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "379  light silver     light green             1                    1\n",
      "380  light silver     pale bronze             2                    2\n",
      "381  light silver  bright crimson             1                    1\n",
      "382  light silver    vibrant aqua             1                    1\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "262  pale bronze  light lavender             5                    5\n",
      "263  pale bronze      dull beige             4                    4\n",
      "264  pale bronze  bright crimson             3                    3\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "496  clear aqua     other             0                    0\n",
      "level_bom:          bag_parent        bag_child  bag_quantity  multiplied_quantity\n",
      "555    clear violet   wavy turquoise             5                   20\n",
      "556    clear violet      light black             5                   20\n",
      "557    clear violet  mirrored indigo             1                    4\n",
      "558    clear violet      faded white             2                    8\n",
      "315    mirrored red      muted beige             5                   15\n",
      "316    mirrored red      faded white             2                    6\n",
      "379    light silver      light green             1                    1\n",
      "380    light silver      pale bronze             2                    2\n",
      "381    light silver   bright crimson             1                    1\n",
      "382    light silver     vibrant aqua             1                    1\n",
      "0    wavy turquoise            other             0                    0\n",
      "262     pale bronze   light lavender             5                   10\n",
      "263     pale bronze       dull beige             4                    8\n",
      "264     pale bronze   bright crimson             3                    6\n",
      "496      clear aqua            other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "895  bright crimson    wavy tan             2                    2\n",
      "896  bright crimson  shiny teal             4                    4\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "683  muted beige     other             0                    0\n",
      "to_append:        bag_parent      bag_child  bag_quantity  multiplied_quantity\n",
      "1439  faded white      clear red             4                    4\n",
      "1440  faded white  faded fuchsia             4                    4\n",
      "1441  faded white     dull beige             1                    1\n",
      "to_append:        bag_parent    bag_child  bag_quantity  multiplied_quantity\n",
      "1220  light green   plaid teal             2                    2\n",
      "1221  light green  pale bronze             5                    5\n",
      "1222  light green    dull teal             3                    3\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "262  pale bronze  light lavender             5                    5\n",
      "263  pale bronze      dull beige             4                    4\n",
      "264  pale bronze  bright crimson             3                    3\n",
      "to_append:          bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "895  bright crimson    wavy tan             2                    2\n",
      "896  bright crimson  shiny teal             4                    4\n",
      "to_append:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "260  vibrant aqua  wavy turquoise             3                    3\n",
      "261  vibrant aqua      dull beige             4                    4\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:      bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "82  light black  light lavender             2                    2\n",
      "83  light black   dotted bronze             2                    2\n",
      "to_append:            bag_parent    bag_child  bag_quantity  multiplied_quantity\n",
      "1478  mirrored indigo  wavy orange             3                    3\n",
      "1479  mirrored indigo    posh plum             5                    5\n",
      "to_append:        bag_parent      bag_child  bag_quantity  multiplied_quantity\n",
      "1439  faded white      clear red             4                    4\n",
      "1440  faded white  faded fuchsia             4                    4\n",
      "1441  faded white     dull beige             1                    1\n",
      "level_bom:            bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "925    light lavender           other             0                    0\n",
      "606        dull beige           other             0                    0\n",
      "895    bright crimson        wavy tan             2                    6\n",
      "896    bright crimson      shiny teal             4                   12\n",
      "683       muted beige           other             0                    0\n",
      "1439      faded white       clear red             4                    8\n",
      "1440      faded white   faded fuchsia             4                    8\n",
      "1441      faded white      dull beige             1                    2\n",
      "1220      light green      plaid teal             2                    2\n",
      "1221      light green     pale bronze             5                    5\n",
      "1222      light green       dull teal             3                    3\n",
      "262       pale bronze  light lavender             5                   10\n",
      "263       pale bronze      dull beige             4                    8\n",
      "264       pale bronze  bright crimson             3                    6\n",
      "895    bright crimson        wavy tan             2                    2\n",
      "896    bright crimson      shiny teal             4                    4\n",
      "260      vibrant aqua  wavy turquoise             3                    3\n",
      "261      vibrant aqua      dull beige             4                    4\n",
      "0      wavy turquoise           other             0                    0\n",
      "82        light black  light lavender             2                   10\n",
      "83        light black   dotted bronze             2                   10\n",
      "1478  mirrored indigo     wavy orange             3                    3\n",
      "1479  mirrored indigo       posh plum             5                    5\n",
      "1439      faded white       clear red             4                    8\n",
      "1440      faded white   faded fuchsia             4                    8\n",
      "1441      faded white      dull beige             1                    2\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n",
      "to_append:          bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "1058  dotted bronze       clear red             3                    3\n",
      "1059  dotted bronze       posh plum             4                    4\n",
      "1060  dotted bronze  light lavender             4                    4\n",
      "1061  dotted bronze   faded fuchsia             4                    4\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "895  bright crimson    wavy tan             2                    2\n",
      "896  bright crimson  shiny teal             4                    4\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "1484  plaid teal      shiny teal             4                    4\n",
      "1485  plaid teal  wavy turquoise             2                    2\n",
      "1486  plaid teal    vibrant aqua             2                    2\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "262  pale bronze  light lavender             5                    5\n",
      "263  pale bronze      dull beige             4                    4\n",
      "264  pale bronze  bright crimson             3                    3\n",
      "to_append:     bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "308  dull teal  vibrant salmon             5                    5\n",
      "309  dull teal    vibrant aqua             3                    3\n",
      "310  dull teal        wavy tan             5                    5\n",
      "311  dull teal  striped purple             5                    5\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1294  clear red     other             0                    0\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "59  faded fuchsia     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:       bag_parent    bag_child  bag_quantity  multiplied_quantity\n",
      "821  wavy orange   plaid teal             5                    5\n",
      "822  wavy orange  pale bronze             1                    1\n",
      "823  wavy orange     wavy tan             4                    4\n",
      "824  wavy orange    clear red             1                    1\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1426  posh plum     other             0                    0\n",
      "level_bom:           bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "925   light lavender           other             0                    0\n",
      "1058   dotted bronze       clear red             3                    6\n",
      "1059   dotted bronze       posh plum             4                    8\n",
      "1060   dotted bronze  light lavender             4                    8\n",
      "1061   dotted bronze   faded fuchsia             4                    8\n",
      "0     wavy turquoise           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "925   light lavender           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "895   bright crimson        wavy tan             2                    6\n",
      "896   bright crimson      shiny teal             4                   12\n",
      "377         wavy tan           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "1484      plaid teal      shiny teal             4                    8\n",
      "1485      plaid teal  wavy turquoise             2                    4\n",
      "1486      plaid teal    vibrant aqua             2                    4\n",
      "262      pale bronze  light lavender             5                   25\n",
      "263      pale bronze      dull beige             4                   20\n",
      "264      pale bronze  bright crimson             3                   15\n",
      "308        dull teal  vibrant salmon             5                   15\n",
      "309        dull teal    vibrant aqua             3                    9\n",
      "310        dull teal        wavy tan             5                   15\n",
      "311        dull teal  striped purple             5                   15\n",
      "1294       clear red           other             0                    0\n",
      "59     faded fuchsia           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "821      wavy orange      plaid teal             5                   15\n",
      "822      wavy orange     pale bronze             1                    3\n",
      "823      wavy orange        wavy tan             4                   12\n",
      "824      wavy orange       clear red             1                    3\n",
      "1426       posh plum           other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "895  bright crimson    wavy tan             2                    2\n",
      "896  bright crimson  shiny teal             4                    4\n",
      "to_append:           bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "1110  vibrant salmon  shiny teal             1                    1\n",
      "to_append:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "260  vibrant aqua  wavy turquoise             3                    3\n",
      "261  vibrant aqua      dull beige             4                    4\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:          bag_parent      bag_child  bag_quantity  multiplied_quantity\n",
      "870  striped purple  faded fuchsia             2                    2\n",
      "871  striped purple      posh plum             5                    5\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "1484  plaid teal      shiny teal             4                    4\n",
      "1485  plaid teal  wavy turquoise             2                    2\n",
      "1486  plaid teal    vibrant aqua             2                    2\n",
      "to_append:       bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "262  pale bronze  light lavender             5                    5\n",
      "263  pale bronze      dull beige             4                    4\n",
      "264  pale bronze  bright crimson             3                    3\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1294  clear red     other             0                    0\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1294  clear red     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1426  posh plum     other             0                    0\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "59  faded fuchsia     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "260  vibrant aqua  wavy turquoise             3                    3\n",
      "261  vibrant aqua      dull beige             4                    4\n",
      "level_bom:           bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "925   light lavender           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "895   bright crimson        wavy tan             2                    6\n",
      "896   bright crimson      shiny teal             4                   12\n",
      "1110  vibrant salmon      shiny teal             1                    5\n",
      "260     vibrant aqua  wavy turquoise             3                    9\n",
      "261     vibrant aqua      dull beige             4                   12\n",
      "377         wavy tan           other             0                    0\n",
      "870   striped purple   faded fuchsia             2                   10\n",
      "871   striped purple       posh plum             5                   25\n",
      "1484      plaid teal      shiny teal             4                   20\n",
      "1485      plaid teal  wavy turquoise             2                   10\n",
      "1486      plaid teal    vibrant aqua             2                   10\n",
      "262      pale bronze  light lavender             5                    5\n",
      "263      pale bronze      dull beige             4                    4\n",
      "264      pale bronze  bright crimson             3                    3\n",
      "377         wavy tan           other             0                    0\n",
      "1294       clear red           other             0                    0\n",
      "377         wavy tan           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "1294       clear red           other             0                    0\n",
      "1426       posh plum           other             0                    0\n",
      "925   light lavender           other             0                    0\n",
      "59     faded fuchsia           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "0     wavy turquoise           other             0                    0\n",
      "260     vibrant aqua  wavy turquoise             3                    6\n",
      "261     vibrant aqua      dull beige             4                    8\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "925  light lavender     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append:          bag_parent   bag_child  bag_quantity  multiplied_quantity\n",
      "895  bright crimson    wavy tan             2                    2\n",
      "896  bright crimson  shiny teal             4                    4\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "59  faded fuchsia     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1426  posh plum     other             0                    0\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:        bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "260  vibrant aqua  wavy turquoise             3                    3\n",
      "261  vibrant aqua      dull beige             4                    4\n",
      "level_bom:           bag_parent       bag_child  bag_quantity  multiplied_quantity\n",
      "0     wavy turquoise           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "925   light lavender           other             0                    0\n",
      "606       dull beige           other             0                    0\n",
      "895   bright crimson        wavy tan             2                    6\n",
      "896   bright crimson      shiny teal             4                   12\n",
      "59     faded fuchsia           other             0                    0\n",
      "1426       posh plum           other             0                    0\n",
      "377         wavy tan           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "1417      shiny teal           other             0                    0\n",
      "0     wavy turquoise           other             0                    0\n",
      "260     vibrant aqua  wavy turquoise             3                    6\n",
      "261     vibrant aqua      dull beige             4                    8\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:        bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0  wavy turquoise     other             0                    0\n",
      "to_append:      bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "606  dull beige     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append:     bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "377   wavy tan     other             0                    0\n",
      "to_append:       bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "1417  shiny teal     other             0                    0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "level_bom:           bag_parent bag_child  bag_quantity  multiplied_quantity\n",
      "0     wavy turquoise     other             0                    0\n",
      "606       dull beige     other             0                    0\n",
      "377         wavy tan     other             0                    0\n",
      "1417      shiny teal     other             0                    0\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "to_append: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "level_bom: Empty DataFrame\n",
      "Columns: [bag_parent, bag_child, bag_quantity, multiplied_quantity]\n",
      "Index: []\n",
      "Part 2 answer: 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# Part 2 again\n",
    "\n",
    "#target = ['shiny gold']\n",
    "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
    "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
    "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
    "#print('bill of materials outside function',bill_of_materials)\n",
    "\n",
    "def total_fit(target,bill_of_materials):\n",
    "#    print('first bill of materials',bill_of_materials)\n",
    "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
    "#    print(target_data)\n",
    "    level_bom = pd.DataFrame()\n",
    "    for k in range(len(target_data)):\n",
    "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
    "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
    "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
    "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
    "        print('to_append:',to_append)\n",
    "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
    "        level_bom = level_bom.append(to_append)\n",
    "    print('level_bom:',level_bom)\n",
    "    bill_of_materials = bill_of_materials.append(level_bom)\n",
    "    if len(level_bom) > 0:\n",
    "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
    "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
    "#        print('')\n",
    "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
    "#    print('level_bom:',level_bom)\n",
    "#    print('bill_of_materials:',bill_of_materials)\n",
    "    return(bill_of_materials['multiplied_quantity'].sum())\n",
    "\n",
    "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
    "\n",
    "print('Part 2 answer:',part_2_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part 2 answer: 13264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:46: RuntimeWarning: overflow encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Finally going to do part 2 right!\n",
    "\n",
    "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
    "# Those are the only bags that we initially know the contents of (0 bags).\n",
    "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
    "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
    "# we know how many bags are inside ALL of its children\n",
    "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
    "\n",
    "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
    "# They go into the dictionary with a value of zero in the dictionary.\n",
    "zero_dict = {}\n",
    "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
    "for k in range(len(lowest_level)):\n",
    "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
    "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
    "\n",
    "def bag_quantities(quantity_dict,color,iterations):\n",
    "    iterations +=1\n",
    "    # Make a list of all parents who have a child in the dictionary.\n",
    "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
    "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
    "    filtered_parent_candidates = []\n",
    "    for a in parent_candidates:\n",
    "        if a not in quantity_dict:\n",
    "            filtered_parent_candidates.append(a)\n",
    "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
    "    temp_dict={}\n",
    "    # Now go through each parent in the filtered parent candidates individually\n",
    "    for j in range(len(filtered_parent_candidates)):\n",
    "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
    "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
    "        # Turn the list of the parent's children into a list\n",
    "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
    "#        print('')\n",
    "#        print('parent_df:',parent_df)\n",
    "#        print('parent_list before test:',parent_list)\n",
    "#        print('quantity_dict before test:',quantity_dict)\n",
    "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
    "        if all(x in quantity_dict.keys() for x in parent_list):\n",
    "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
    "            quantity = 0\n",
    "            for x in range(len(parent_list)):\n",
    "                # So for each parent, we want to add the quantity of each child bag\n",
    "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
    "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
    "            # Then append this entry to the temporary dictionary\n",
    "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
    "#            print('temp_dict:',temp_dict)\n",
    "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
    "    # Otherwise it could mess up the next candidate in the list\n",
    "    quantity_dict.update(temp_dict)\n",
    "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
    "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
    "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
    "    filtered_parent_candidates = []\n",
    "    for a in parent_candidates:\n",
    "        if a not in quantity_dict:\n",
    "            filtered_parent_candidates.append(a)\n",
    "    if (len(filtered_parent_candidates)) > 0:\n",
    "        color = color\n",
    "        bag_quantities(quantity_dict,color,iterations)\n",
    "#    print(iterations)\n",
    "    return(quantity_dict[color])\n",
    "\n",
    "part_2_answer = bag_quantities(zero_dict,'shiny gold',0)\n",
    "print('Part 2 answer:',part_2_answer)\n",
    "\n",
    "# Write my final dictionary to a csv\n",
    "#pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/1:\n",
      "import numpy as np\n",
      "\n",
      "#create a list\n",
      "l = [1,2,3]\n",
      "\n",
      "#create an equivalent numpy array\n",
      "a = np.array([1,2,3])\n",
      "\n",
      "#to add items to a list, use append\n",
      "l.append(4)\n",
      "l.extend([5,6,7])\n",
      "l = l + [8]\n",
      "l = l + [9,10,11]\n",
      "print('l:', l)\n",
      "\n",
      "#to add items to an array\n",
      "a = np.append(a,[4,5,6,7])\n",
      "print('a:',a)\n",
      " 1/2:\n",
      "#Now how about some vector addition. We can't just add the lists, Python will concatenate. We need to create a new empty list\n",
      "l2 = []\n",
      "\n",
      "#Adding the element twice effectively adds the vector to itself\n",
      "for i in l:\n",
      "    l2.append(i + i)\n",
      "    \n",
      "print('l2:',l2)\n",
      "\n",
      "#Working with arrays, adding them is vector addition, not just concatenation\n",
      "a2 = a + a\n",
      "print('a2:',a2)\n",
      " 1/3:\n",
      "#Now add to itself using multiplication instead of addition\n",
      "a3 = a*2\n",
      "print(a3)\n",
      "\n",
      "#A list will concatentate if you just multiply by 2; you'd need to do a for loop\n",
      " 1/4:\n",
      "#Now let's square every value in a list/array\n",
      "lsq = []\n",
      "for i in l:\n",
      "    lsq.append(i*i)\n",
      "print(\"lsq: \",lsq)\n",
      "\n",
      "asq = a*a\n",
      "print(\"asq: \",asq)\n",
      "#or\n",
      "asq2= a**2\n",
      "print(\"asq2: \",asq2)\n",
      " 1/5:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.log(a)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.exp(a)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      " 2/1:\n",
      "import numpy as np\n",
      "\n",
      "#create a list\n",
      "l = [1,2,3]\n",
      "\n",
      "#create an equivalent numpy array\n",
      "a = np.array([1,2,3])\n",
      "\n",
      "#to add items to a list, use append\n",
      "l.append(4)\n",
      "l.extend([5,6,7])\n",
      "l = l + [8]\n",
      "l = l + [9,10,11]\n",
      "print('l:', l)\n",
      "\n",
      "#to add items to an array\n",
      "a = np.append(a,[4,5,6,7])\n",
      "print('a:',a)\n",
      " 2/2:\n",
      "#Now how about some vector addition. We can't just add the lists, Python will concatenate. We need to create a new empty list\n",
      "l2 = []\n",
      "\n",
      "#Adding the element twice effectively adds the vector to itself\n",
      "for i in l:\n",
      "    l2.append(i + i)\n",
      "    \n",
      "print('l2:',l2)\n",
      "\n",
      "#Working with arrays, adding them is vector addition, not just concatenation\n",
      "a2 = a + a\n",
      "print('a2:',a2)\n",
      " 2/3:\n",
      "#Now add to itself using multiplication instead of addition\n",
      "a3 = a*2\n",
      "print('a3:', a3)\n",
      "\n",
      "#A list will concatentate if you just multiply by 2; you'd need to do a for loop\n",
      " 2/4:\n",
      "#Now how about some vector addition. We can't just add the lists, Python will concatenate. We need to create a new empty list\n",
      "l2 = []\n",
      "\n",
      "#Adding the element twice effectively adds the vector to itself\n",
      "for i in l:\n",
      "    l2.append(i + i)\n",
      "    \n",
      "print('l2:',l2)\n",
      "\n",
      "#Working with arrays, adding them is vector addition, not just concatenation\n",
      "a2 = a + a\n",
      "print('a2:',a2)\n",
      " 2/5:\n",
      "#Now add to itself using multiplication instead of addition\n",
      "a3 = a*2\n",
      "print('a3:', a3)\n",
      "\n",
      "#A list will concatentate if you just multiply by 2; you'd need to do a for loop\n",
      " 2/6:\n",
      "#Now let's square every value in a list/array\n",
      "lsq = []\n",
      "for i in l:\n",
      "    lsq.append(i*i)\n",
      "print(\"lsq: \",lsq)\n",
      "\n",
      "asq = a*a\n",
      "print(\"asq: \",asq)\n",
      "#or\n",
      "asq2= a**2\n",
      "print(\"asq2: \",asq2)\n",
      " 2/7:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.log(a)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.exp(a)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      " 2/8:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a))\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.exp(a)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      " 2/9:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),2)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.exp(a)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/10:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.exp(a)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/11:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.round(np.exp(a),4)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/12:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.round(np.exp(a),-1)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/13:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.round(np.exp(a),-2)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/14:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.round(np.exp(a),1)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      "2/15:\n",
      "#take the square root; most numpy functions work element-wise\n",
      "asqrt = np.sqrt(asq)\n",
      "print(\"asqrt: \",asqrt)\n",
      "\n",
      "#how about the log/exponential?\n",
      "alog = np.round(np.log(a),3)\n",
      "print(\"alog: \",alog)\n",
      "aexp = np.round(np.exp(a),4)\n",
      "print(\"aexp: \",aexp)\n",
      "\n",
      "#Basic lesson: with numpy you can treat an array like a vector\n",
      " 3/1:\n",
      "#apply direct definition of dot product via looping\n",
      "dot_product = 0\n",
      "for i,j in zip(n1,n2):\n",
      "    dot_product += i*j\n",
      "    \n",
      "print('dot_product:',dot_product)\n",
      "\n",
      "dp = np.sum(n1 * n2)\n",
      "print('dp:',dp)\n",
      "\n",
      "#I guess the np.sum function keeps me from having to transpose? Or maybe the dot product doesn't depend on row vs column\n",
      "#Easiest way though, use the dot product function:\n",
      "print(\"np.dot:\",np.dot(n1,n2))\n",
      "# also could just say n1.dot(n2)\n",
      " 3/2:\n",
      "import numpy as np\n",
      "\n",
      "#create two numpy arrays\n",
      "n1 = np.array([1,2])\n",
      "n2 = np.array([2,1])\n",
      " 3/3:\n",
      "#apply direct definition of dot product via looping\n",
      "dot_product = 0\n",
      "for i,j in zip(n1,n2):\n",
      "    dot_product += i*j\n",
      "    \n",
      "print('dot_product:',dot_product)\n",
      "\n",
      "dp = np.sum(n1 * n2)\n",
      "print('dp:',dp)\n",
      "\n",
      "#I guess the np.sum function keeps me from having to transpose? Or maybe the dot product doesn't depend on row vs column\n",
      "#Easiest way though, use the dot product function:\n",
      "print(\"np.dot:\",np.dot(n1,n2))\n",
      "# also could just say n1.dot(n2)\n",
      " 3/4:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n2))\n",
      "print(n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine:',n1n2cos)\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle:',n1n2angle)\n",
      " 3/5:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print(n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine:',n1n2cos)\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle:',n1n2angle)\n",
      " 3/6:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine:',n1n2cos)\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle:',n1n2angle)\n",
      " 3/7:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dog(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle:',n1n2angle)\n",
      " 3/8:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle:',n1n2angle)\n",
      " 3/9:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "#Looks like there are two ways to use the dot function.\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle cos:',n1n2angle)\n",
      "\n",
      "n1n2angle = np.arcsin(n1n2sin)\n",
      "print('angle sin:',n1n2angle)\n",
      "3/10:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "#Looks like there are two ways to use the dot function.\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle cos:',n1n2angle)\n",
      "\n",
      "n1n2angle = np.arcsin(n1n2sine)\n",
      "print('angle sin:',n1n2angle)\n",
      "3/11:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "#Looks like there are two ways to use the dot function.\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle cos:',n1n2angle)\n",
      "3/12:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "#Looks like there are two ways to use the dot function.\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',n1n2cos)\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',n1n2cos)\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle cos:',np.round(n1n2angle,3))\n",
      "3/13:\n",
      "#Now calculate the angle of a vector\n",
      "\n",
      "#Start by calculating the length - the square root of the sum of each element squared\n",
      "n1mag = np.sqrt(np.sum(n1*n1))\n",
      "print('n1magnitude:', n1mag)\n",
      "#Of course, numpy has a function to do this:\n",
      "np.linalg.norm(n1)\n",
      "\n",
      "#Now let's figure the cosine between the two - it's the dot product divided by the products of the magnitude\n",
      "#Looks like there are two ways to use the dot function.\n",
      "n1n2cos = n1.dot(n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 1:',np.round(n1n2cos,4))\n",
      "\n",
      "n1n2cos = np.dot(n1,n2)/(np.linalg.norm(n1)*np.linalg.norm(n2))\n",
      "print('angle cosine method 2:',np.round(n1n2cos,4))\n",
      "\n",
      "\n",
      "#And the angle\n",
      "n1n2angle = np.arccos(n1n2cos)\n",
      "print('angle cos:',np.round(n1n2angle,3))\n",
      " 4/1:\n",
      "import numpy as np\n",
      "\n",
      "#create a matrix as an array of two lists (have to be the same length)\n",
      "a = np.array([[1,2],[3,4]])\n",
      "\n",
      "#first index is row, second is column\n",
      "#backup list\n",
      "l = [[1,2],[3,4]]\n",
      " 4/2:\n",
      "import numpy as np\n",
      "\n",
      "#create a matrix as an array of two lists (have to be the same length)\n",
      "a = np.array([[1,2],[3,4]])\n",
      "\n",
      "#first index is row, second is column\n",
      "#backup list\n",
      "l = [[1,2],[3,4]]\n",
      " 4/3:\n",
      "import numpy as np\n",
      "\n",
      "#create a matrix as an array of two lists (have to be the same length)\n",
      "a = np.array([[1,2],[3,4]])\n",
      "\n",
      "#first index is row, second is column\n",
      "#backup list\n",
      "l = [[1,2],[3,4]]\n",
      "\n",
      "print(a)\n",
      " 4/4:\n",
      "import numpy as np\n",
      "\n",
      "#create a matrix as an array of two lists (have to be the same length)\n",
      "a = np.array([[1,2],[3,4]])\n",
      "\n",
      "#first index is row, second is column\n",
      "#backup list\n",
      "l = [[1,2],[3,4]]\n",
      "\n",
      "print('a:',a)\n",
      "print('l:',l)\n",
      " 4/5:\n",
      "#now let's extract the 2\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print(a[0][1])\n",
      "#but there is an easier way\n",
      "print(a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      " 4/6:\n",
      "#now let's extract the 2\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way to print first row of a:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      " 4/7:\n",
      "#now let's extract the 2\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "type(l)\n",
      "#we can do the same thing with the matrix\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      " 4/8:\n",
      "#now let's extract the 2\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "print(type(l))\n",
      "#we can do the same thing with the matrix\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      " 4/9:\n",
      "#now let's extract the 2\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "print('l is:',type(l))\n",
      "#we can do the same thing with the matrix\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/10:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/11:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m), m)\n",
      "\n",
      "j = np.array(m)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/12:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m), m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j), j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/13:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m), m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j), j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/14:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m), m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/15:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m), m)\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/16:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m))\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "j\n",
      "4/17:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m))\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "print(\"transposed j:', j)\n",
      "4/18:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m))\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "print('transposed j:', j)\n",
      "4/19:\n",
      "#now let's extract the 2\n",
      "print('l is:',type(l))\n",
      "#start by extracting the first row\n",
      "print('first row of l:', l[0])\n",
      "#now get the second element...\n",
      "print('second element of first row of l:', l[0][1])\n",
      "\n",
      "#we can do the same thing with the matrix\n",
      "print('')\n",
      "print('a is:',type(a))\n",
      "print('second element of first row of a:', a[0][1])\n",
      "#but there is an easier way\n",
      "print('easier way:',a[0,1])\n",
      "\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m))\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "print('')\n",
      "print('transposed j:')\n",
      "print(j)\n",
      "4/20:\n",
      "import numpy as np\n",
      "\n",
      "#create a matrix as an array of two lists (have to be the same length)\n",
      "a = np.array([[1,2],[3,4]])\n",
      "\n",
      "#first index is row, second is column\n",
      "#backup list\n",
      "l = [[1,2],[3,4]]\n",
      "\n",
      "print('a:',a)\n",
      "print('l:',l)\n",
      "4/21:\n",
      "#we can also just create a matrix with numpy, but usually just easier to work with array\n",
      "m = np.matrix([[1,2],[3,4]])\n",
      "print('m:',type(m))\n",
      "print(m)\n",
      "print('')\n",
      "j = np.array(m)\n",
      "print('j:',type(j))\n",
      "print(j)\n",
      "4/22:\n",
      "\n",
      "#transpose it\n",
      "j = j.T\n",
      "print('')\n",
      "print('transposed j:')\n",
      "print(j)\n",
      " 5/1:\n",
      "import numpy as np\n",
      "\n",
      "#how would we create an array of length 100 filled with random numbers?\n",
      "\n",
      "#here is one of all zeroes:\n",
      "zeros = np.zeros(20)\n",
      "\n",
      "zero_matrix = np.zeros((5,5))\n",
      "\n",
      "#works for ones as well\n",
      "ones = np.ones((4,4))\n",
      "ones\n",
      "\n",
      "#now how about a matrix of random numbers\n",
      "random = np.random.random((3,3))\n",
      "\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.random.randn(10,10)\n",
      "\n",
      "randn.mean()\n",
      "randn.var()\n",
      " 5/2:\n",
      "zero_matrix = np.zeros((5,5))\n",
      "print(zero_matrix)\n",
      " 5/3:\n",
      "#works for ones as well\n",
      "ones = np.ones((4,4))\n",
      "ones\n",
      " 5/4:\n",
      "#now how about a matrix of random numbers\n",
      "random = np.random.random((3,3))\n",
      "print(random)\n",
      " 5/5:\n",
      "#now how about a matrix of random numbers\n",
      "random = np.random.random((3,3))\n",
      "print(random)\n",
      " 5/6:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.random.randn(10,10)\n",
      "print(randn)\n",
      " 5/7:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.random.randn(10,10)\n",
      "print(randn)\n",
      "\n",
      "randn.mean()\n",
      "randn.var()\n",
      " 5/8:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.round(np.random.randn(10,10),2)\n",
      "print(randn)\n",
      "\n",
      "randn.mean()\n",
      "randn.var()\n",
      " 5/9:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.round(np.random.randn(10,10),2)\n",
      "print(randn)\n",
      "\n",
      "print('mean:',randn.mean())\n",
      "print('variance:',randn.var())\n",
      "5/10:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.round(np.random.randn(10,10),2)\n",
      "print(randn)\n",
      "\n",
      "print('mean:',randn.mean())\n",
      "print('variance:',randn.var())\n",
      "5/11:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.round(np.random.randn(10,10),2)\n",
      "print(randn)\n",
      "\n",
      "print('mean:',randn.mean())\n",
      "print('variance:',randn.var())\n",
      "5/12:\n",
      "#gaussian distributed randoms - requires each dimension as an argument while the others take tuples...\n",
      "randn = np.round(np.random.randn(10,10),2)\n",
      "print(randn)\n",
      "\n",
      "print('mean:',randn.mean())\n",
      "print('variance:',randn.var())\n",
      " 6/1:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "print('a:',a)\n",
      "print('b:',b)\n",
      "print('aXb:' a.dot(b))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print(c*d)\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 6/2:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "print('a:',a)\n",
      "print('b:',b)\n",
      "print('aXb:', a.dot(b))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print(c*d)\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 6/3:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "#print('a:',a)\n",
      "#print('b:',b)\n",
      "print('aXb:', a.dot(b))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print(c*d)\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 6/4:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "#print('a:',a)\n",
      "#print('b:',b)\n",
      "print('aXb:', np.round(a.dot(b),2))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print(c*d)\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 6/5:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "#print('a:',a)\n",
      "#print('b:',b)\n",
      "print('aXb:', np.round(a.dot(b),2))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "print('')\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print('c*d:', c*d)\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 6/6:\n",
      "import numpy as np\n",
      "\n",
      "#inner dimensions have to match\n",
      "#in numpy, dot means actual matrix multiplication\n",
      "#asterisk is element by element multiplication\n",
      "\n",
      "a = np.random.random((3,4))\n",
      "b = np.random.random((4,1))\n",
      "#print('a:',a)\n",
      "#print('b:',b)\n",
      "print('aXb:', np.round(a.dot(b),2))\n",
      "#results in a 3x1 matrix\n",
      "#a*b will not work because matrices are not the same size.\n",
      "print('')\n",
      "\n",
      "c = np.random.random((2,3))\n",
      "d = np.random.random((2,3))\n",
      "\n",
      "print('c*d:', np.round(c*d,2))\n",
      "#just multiplies the elements together\n",
      "#print(c.dot(d)) will not work because inner dimensions do not match\n",
      " 8/1:\n",
      "#makes more sense as word problem\n",
      "#admission at a fair = $1.50 for kids, $4 for adults\n",
      "#2200 people enter the fair, $5050 is collected\n",
      "#How many were adults and how many were kids?\n",
      "\n",
      "# x1 + x2 = 2200\n",
      "# 1.5X1 + 4X2 = 5050\n",
      "\n",
      "a = np.array([[1,1],[1.5,4]])\n",
      "b = np.array([2200,5050])\n",
      "solution = np.linalg.solve(a,b)\n",
      "print('solution:', solution)\n",
      " 8/2:\n",
      "import numpy as np\n",
      "\n",
      "#just like in algebra, if Ax = B, then x = A-1B (A inverse)\n",
      "\n",
      "a = np.array([[1,2],[3,4]])\n",
      "b = np.array([1,2])\n",
      "\n",
      "x = np.linalg.inv(a).dot(b)\n",
      "print(x)\n",
      "\n",
      "#can also just use solve\n",
      "np.linalg.solve(a,b)\n",
      " 8/3:\n",
      "#makes more sense as word problem\n",
      "#admission at a fair = $1.50 for kids, $4 for adults\n",
      "#2200 people enter the fair, $5050 is collected\n",
      "#How many were adults and how many were kids?\n",
      "\n",
      "# x1 + x2 = 2200\n",
      "# 1.5X1 + 4X2 = 5050\n",
      "\n",
      "a = np.array([[1,1],[1.5,4]])\n",
      "b = np.array([2200,5050])\n",
      "solution = np.linalg.solve(a,b)\n",
      "print('solution:', solution)\n",
      " 8/4: print('kids', solution[1])\n",
      " 8/5:\n",
      "print('kids:', solution[0])\n",
      "print('adults:', solution[1])\n",
      " 9/1:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Joel_Work/Udemy/Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      " 9/2:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      " 9/3:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      " 9/4:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "#path = 'C:/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      " 9/5:\n",
      "#easier to load data with pandas\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "x = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "type(x) #tells us it's a data frame\n",
      "x.info() #shows us info about the column types\n",
      "x.head()\n",
      " 9/6:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "m = x.as_matrix()\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      " 9/7:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "# m = x.as_matrix()\n",
      "# Note: as_matrix() has been deprecated. Use DataFrame.values()\n",
      "m = x.values()\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      " 9/8:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "# m = x.as_matrix()\n",
      "# Note: as_matrix() has been deprecated. Use DataFrame.values()\n",
      "m = x.DataFrame.values()\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      " 9/9:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "# m = x.as_matrix()\n",
      "# Note: as_matrix() has been deprecated. Use DataFrame.values()\n",
      "m = x.values()\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      "9/10:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "# m = x.as_matrix()\n",
      "# Note: as_matrix() has been deprecated. Use DataFrame.values()\n",
      "m = x.values\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      "9/11:\n",
      "#easier to load data with pandas\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "x = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "type(x) #tells us it's a data frame\n",
      "x.info() #shows us info about the column types\n",
      "x.head()\n",
      "9/12:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      "9/13:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('x.head:',x.head())\n",
      "print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "m[0]\n",
      "\n",
      "type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/14:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "class(x)\n",
      "print('x.head:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/15:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "x.class\n",
      "print('x.head:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/16:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "print('x.head:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/17:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "#print('x.head:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/18:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "#print('x.head:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/19:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "print('head of numpy array:',x.[head:])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/20:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "print('head of numpy array:',x[head:])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/21:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "print('head of numpy array:',x[:head])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/22:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "#print('head of numpy array:',x[:head])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/23:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "print('head of numpy array:',x.head())\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/24:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "#print('head of numpy array:',x.head())\n",
      "x[:5]\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/25:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "type(x)\n",
      "#print('head of numpy array:',x.head())\n",
      "print('head of numpy array:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/26:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      "9/27:\n",
      "#easier to load data with pandas\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "x = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "type(x) #tells us it's a data frame\n",
      "x.info() #shows us info about the column types\n",
      "x.head()\n",
      "9/28:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "#print('head of numpy array:',x.head())\n",
      "print('head of numpy array:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/29:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',head(x))\n",
      "#print('head of numpy array:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/30:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('head of numpy array:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/31:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#or print('first five rows of DataFrame:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/32:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "print('first five rows of DataFrame:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/33:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "#print('x[0].head():', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/34:\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "print('x[0].head():', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/35:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/36:\n",
      "# So how do we get a row from a pandas dataframe? iloc or ix, both return series\n",
      "# ix is being deprecated. Use .loc for label based indexing, .iloc for positional\n",
      "\n",
      "print(x.iloc[0])\n",
      "# can also do x.iloc[0:2]\n",
      "# print(x.ix[1])\n",
      "9/37:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "#m[0]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/38:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/39:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first column\n",
      "print('first row of m:',m[0])\n",
      "print(head(m))\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/40:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first column\n",
      "print(m[:,[0]])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/41:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first column\n",
      "print(m[0:5,[0]])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/42:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "\n",
      "# m[0] - first three values of first column\n",
      "print(m[0:3,[0]])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/43:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# m[0] - first three values of first column\n",
      "print(m[0:3],[0:1]])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/44:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# m[0] - first three values of first column\n",
      "print(m[0:3,0:1])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/45:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# m[0] - first three values of first column\n",
      "print(m[0:3,0:2])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/46:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# m[0] - first three values of first two columns\n",
      "print(m[0:3,0:2])\n",
      "\n",
      "m[:head]\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/47:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column with name 0\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# m[0] - first three values of first two columns\n",
      "print(m[0:3,0:2])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "9/48:\n",
      "# So how do we get a row from a pandas dataframe? iloc or ix, both return series\n",
      "# ix is being deprecated. Use .loc for label based indexing, .iloc for positional\n",
      "\n",
      "print(x.iloc[0])\n",
      "# can also do x.iloc[0:2]\n",
      "# print(x.ix[1])\n",
      "9/49:\n",
      "# To get 0th and 2nd columns\n",
      "x[[0,2]].head()\n",
      "\n",
      "# Double brackets just allows us to select multiple columns?\n",
      "9/50:\n",
      "# Specific columns based on rules\n",
      "# Select rows with 0th value of less than 5\n",
      "\n",
      "x[x[0]<5]\n",
      "\n",
      "# Just x[0]<5 would just be a boolean test on each room\n",
      "10/1:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      "10/2:\n",
      "# So how do we get a row from a pandas dataframe? iloc or ix, both return series\n",
      "# ix is being deprecated. Use .loc for label based indexing, .iloc for positional\n",
      "\n",
      "print(x.iloc[0])\n",
      "# can also do x.iloc[0:2]\n",
      "# print(x.ix[1])\n",
      "10/3:\n",
      "import numpy as np\n",
      "\n",
      "#read in a file assuming we have no idea how many rows it has\n",
      "#start with a list\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "#loop through\n",
      "for line in open(path + 'data_2d.csv'):\n",
      "    row = line.split(',')\n",
      "    #cast strings into floats\n",
      "    #sample = map(float,row) This is what works for Python 2\n",
      "    sample = list(map(float,row))\n",
      "    x.append(sample)\n",
      "    \n",
      "x = np.array(x)    \n",
      "x.shape\n",
      "10/4:\n",
      "#easier to load data with pandas\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "x = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "type(x) #tells us it's a data frame\n",
      "x.info() #shows us info about the column types\n",
      "x.head()\n",
      "10/5:\n",
      "# Try to extract element [0,0] from X\n",
      "# x(0,0) will not work on a data frame\n",
      "print('type of x:',type(x))\n",
      "\n",
      "# How about if we convert it into a matrix?\n",
      "# m = x.as_matrix()\n",
      "# Note: as_matrix() has been deprecated. Use DataFrame.values()\n",
      "m = x.values\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# as_matrix() actually changes it into a numpy array, which is good because numpy matrices are not fun to work with\n",
      "10/6:\n",
      "print('type of x:',type(x))\n",
      "print('head of Pandas DataFrame:',x.head())\n",
      "#print('first five rows of DataFrame:',x[:5])\n",
      "# x[0] - using this function on a pandas dataframe returns the column in position 0 (first column)\n",
      "print('')\n",
      "print('head of first column only:', x[0].head())\n",
      "\n",
      "print('')\n",
      "\n",
      "print('type of m:',type(m))\n",
      "\n",
      "# m[0] - using this function on a numpy array returns the first row\n",
      "print('first row of m:',m[0])\n",
      "print('')\n",
      "\n",
      "# first three values of first two columns\n",
      "print(m[0:3,0:2])\n",
      "\n",
      "#type(x[0].head())\n",
      "# Returns a series. In pandas, 2D objects are data frames, 1D objects are series\n",
      "10/7:\n",
      "# So how do we get a row from a pandas dataframe? iloc or ix, both return series\n",
      "# ix is being deprecated. Use .loc for label based indexing, .iloc for positional\n",
      "\n",
      "print(x.iloc[0])\n",
      "# can also do x.iloc[0:2]\n",
      "# print(x.ix[1])\n",
      "10/8: x.iloc[0:1]\n",
      "10/9: x.iloc[0:2]\n",
      "10/10:\n",
      "# To get 0th and 2nd columns\n",
      "x[[0,2]].head()\n",
      "\n",
      "# Double brackets just allows us to select multiple columns?\n",
      "10/11:\n",
      "# Specific columns based on rules\n",
      "# Select rows with value of the 0th (1st) column less than 5\n",
      "\n",
      "x[x[0]<5]\n",
      "\n",
      "# Just x[0]<5 would just be a boolean test on each room\n",
      "11/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Joel_Work/Udemy/Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "\n",
      "# reload\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# look at column names\n",
      "print('original columns: ',df.columns)\n",
      "\n",
      "# rename\n",
      "df.columns = ['month','passengers']\n",
      "print('updated columns: ',df.columns)\n",
      "\n",
      "#select first column\n",
      "df['passengers'].head()\n",
      "\n",
      "# or (if column name is a string)\n",
      "df.passengers.head()\n",
      "\n",
      "#add column\n",
      "df['ones'] = 1\n",
      "\n",
      "# check\n",
      "df.head()\n",
      "11/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "\n",
      "# reload\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# look at column names\n",
      "print('original columns: ',df.columns)\n",
      "\n",
      "# rename\n",
      "df.columns = ['month','passengers']\n",
      "print('updated columns: ',df.columns)\n",
      "\n",
      "#select first column\n",
      "df['passengers'].head()\n",
      "\n",
      "# or (if column name is a string)\n",
      "df.passengers.head()\n",
      "\n",
      "#add column\n",
      "df['ones'] = 1\n",
      "\n",
      "# check\n",
      "df.head()\n",
      "11/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/5:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/6:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/7:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/8:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/9:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "#df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/10:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/11:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "11/12:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "11/13:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "df.tail()\n",
      "11/14:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "#df.tail()\n",
      "\n",
      "df\n",
      "11/15:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "df.tail()\n",
      "11/16:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/17:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "df.tail()\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/18:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "df.tail()\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/19:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/20:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/21:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "df.tail(1)\n",
      "11/22:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "df.tail(0)\n",
      "11/23:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/airline/'\n",
      "\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df.info()\n",
      "df.tail()\n",
      "11/24:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =1)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/25:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =2)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/26:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/27:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =4)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/28:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/29:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =2)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/30:\n",
      "# reload file to ignore those rows at the bottom\n",
      "df = pd.read_csv(path + 'international-airline-passengers.csv', engine = \"python\", skipfooter =3)\n",
      "\n",
      "# instructor says 3 but only looks like two rows need to be removed?\n",
      "\n",
      "#check tail to make sure proper removal:\n",
      "print(df.tail())\n",
      "print('')\n",
      "\n",
      "# then look at column names\n",
      "print('original columns: ',df.columns)\n",
      "11/31:\n",
      "# those are ugly names, let's reassign them columns\n",
      "df.columns = ['month','passengers']\n",
      "print('updated columns: ',df.columns)\n",
      "\n",
      "#select first column\n",
      "df['passengers'].head()\n",
      "\n",
      "# or (if column name is a string)\n",
      "df.passengers.head()\n",
      "11/32:\n",
      "#add column\n",
      "df['ones'] = 1\n",
      "\n",
      "# check\n",
      "df.head()\n",
      "11/33:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "11/34:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return(row['x1']*row['x2'])\n",
      "df('joel') = df.apply(get_interaction, axis =1)\n",
      "\n",
      "df.head()\n",
      "11/35:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return(row['x1']*row['x2'])\n",
      "df('joel') = df.apply(get_interaction, axis=1)\n",
      "\n",
      "df.head()\n",
      "11/36:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return row['x1']*row['x2'] \n",
      "df('joel') = df.apply(get_interaction, axis=1)\n",
      "\n",
      "df.head()\n",
      "11/37:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return row['x1']*row['x2'] \n",
      "\n",
      "df('joel') = df.apply(get_interaction, axis=1)\n",
      "\n",
      "df.head()\n",
      "11/38:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return row['x1']*row['x2'] \n",
      "\n",
      "df('joel') = df.apply(get_interaction(axis=1))\n",
      "\n",
      "df.head()\n",
      "11/39:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "def get_interaction(row):\n",
      "    return row['x1']*row['x2'] \n",
      "\n",
      "df('x1x2') = df.apply(get_interaction, axis=1)\n",
      "\n",
      "df.head()\n",
      "11/40:\n",
      "# now create a new column based on other columns:\n",
      "# To do a calculation based on two other fields, use the apply function\n",
      "# axis = 1 says to do it across rows, not columns...\n",
      "\n",
      "df['newfield'] =df.apply(lambda row: row['passengers']+row['ones'],axis = 1)\n",
      "\n",
      "# convert month to date time\n",
      "\n",
      "from datetime import datetime\n",
      "print(df.info())\n",
      "\n",
      "df['dt'] = df.apply(lambda row: datetime.strptime(row['month'], '%Y-%m'),axis=1)\n",
      "\n",
      "df.head()\n",
      "\n",
      "# lambda is exactly like creating a function. We could also do:\n",
      "#def get_interaction(row):\n",
      "#    return row['x1']*row['x2'] \n",
      "\n",
      "#df('x1x2') = df.apply(get_interaction, axis=1)\n",
      "\n",
      "#df.head()\n",
      "12/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "x = []\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "df1 = pd.read_csv(path + 'table1.csv')\n",
      "df2 = pd.read_csv(path + 'table2')\n",
      "\n",
      "#has ugly column names and three rows at the bottom that appear to be irrelevant\n",
      "#no need to require headers; pandas assumes they are there unless parameter added specifies none\n",
      "#skip footer only works with the default engine of C, so we have to set it to be python\n",
      "df1.info()\n",
      "df2.info()\n",
      "12/2:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "df1 = pd.read_csv(path + 'table1.csv')\n",
      "df2 = pd.read_csv(path + 'table2.csv')\n",
      "\n",
      "df1.info()\n",
      "df2.info()\n",
      "12/3:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "df1 = pd.read_csv(path + 'table1.csv')\n",
      "df2 = pd.read_csv(path + 'table2.csv')\n",
      "\n",
      "df1.info()\n",
      "df2.info()\n",
      "\n",
      "df1\n",
      "12/4:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "t1 = pd.read_csv(path + 'table1.csv')\n",
      "t2 = pd.read_csv(path + 'table2.csv')\n",
      "\n",
      "t1.info()\n",
      "t2.info()\n",
      "12/5:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "t1 = pd.read_csv(path + 'table1.csv')\n",
      "t2 = pd.read_csv(path + 'table2.csv')\n",
      "\n",
      "print('t1:', t1)\n",
      "print('')\n",
      "print('t2:', t2)\n",
      "12/6:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = pd.merge(t1,t2,'user_id')\n",
      "12/7:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = t1.merge(t2,t2,left = 'user_id', right = 'user_id')\n",
      "12/8:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = t1.merge(t2,t2,left_on = 'user_id', right = 'user_id')\n",
      "12/9:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = t1.merge(t2,t2,left_on = 'user_id', right_on = 'user_id')\n",
      "12/10:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = pd.merge(t1,t2,on = 'user_id')\n",
      "12/11:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = pd.merge(t1,t2,on = 'user_id')\n",
      "m\n",
      "12/12:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "t1 = pd.read_csv(path + 'table1.csv')\n",
      "t2 = pd.read_csv(path + 'table2.csv')\n",
      "type(t1)\n",
      "print('t1:', t1)\n",
      "print('')\n",
      "print('t2:', t2)\n",
      "12/13:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/numpy_class/'\n",
      "\n",
      "t1 = pd.read_csv(path + 'table1.csv')\n",
      "t2 = pd.read_csv(path + 'table2.csv')\n",
      "print(type(t1))\n",
      "print('t1:', t1)\n",
      "print('')\n",
      "print('t2:', t2)\n",
      "12/14:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = pd.merge(t1,t2,on = 'user_id')\n",
      "m\n",
      "\n",
      "# could also do \n",
      "joel = t1.merge(t2, on = 'user_id')\n",
      "12/15:\n",
      "# merge the two files, joining on user_id\n",
      "\n",
      "m = pd.merge(t1,t2,on = 'user_id')\n",
      "m\n",
      "\n",
      "# could also do \n",
      "joel = t1.merge(t2, on = 'user_id')\n",
      "joel\n",
      "13/1: import matplotlib.pyplot as plt\n",
      "13/2: import matplotlib.pyplot as plt\n",
      "13/3:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "x = np.linspace(1,10,10)\n",
      "13/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.linspace(1,10,10)\n",
      "13/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "# linspace arguments (min, max, number of points)\n",
      "x = np.linspace(1,10,10)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/6:\n",
      "# add some labels\n",
      "plt.xlabel('time')\n",
      "13/7:\n",
      "# add some labels\n",
      "x = np.linspace(1,10,10)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/8:\n",
      "# add some labels\n",
      "x = np.linspace(1,10,10)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.xlabel('time')\n",
      "plt.show()\n",
      "13/9:\n",
      "# add some labels (need to redo other arguments)\n",
      "x = np.linspace(1,10,10)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.xlabel('time')\n",
      "plt.ylabel('function')\n",
      "plt.title(\"Joel's First Matplotlib Chart\")\n",
      "plt.show()\n",
      "13/10:\n",
      "# looks choppy, so let's add more points to it.\n",
      "\n",
      "x = np.linspace(1,10,100)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/11:\n",
      "# looks choppy, so let's add more points to it.\n",
      "\n",
      "x = np.linspace(1,10,10000)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/12:\n",
      "# looks choppy, so let's add more points to it.\n",
      "\n",
      "x = np.linspace(1,10,100)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/13:\n",
      "# add some labels (need to redo other arguments)\n",
      "# and since it looks choppy, let's add more points\n",
      "x = np.linspace(1,10,100)\n",
      "y = np.sin(x)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.xlabel('time')\n",
      "plt.ylabel('function')\n",
      "plt.title(\"Joel's First Matplotlib Chart\")\n",
      "plt.show()\n",
      "13/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "\n",
      "#plt.plot(x,y)\n",
      "#plt.show()\n",
      "13/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "x\n",
      "#plt.plot(x,y)\n",
      "#plt.show()\n",
      "13/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "y = np.random.random(10)\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "13/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "y = np.random.random(10)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "13/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "y = np.random.random(10)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "13/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "y = np.random.random(10)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "16/1: path = 'C:\\Users\\jlahrman\\OneDrive - LMI\\Documents\\Clockwork_Files\\Joel_Work\\Udemy\\1_Numpy_Stack_Python\\machine_learning_examples\\linear_regression_class'\n",
      "16/2: path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "16/3:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "16/4:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "\n",
      "x = pd.read.csv(path + 'data_1d.csv')\n",
      "16/5:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "\n",
      "x = pd.readcsv(path + 'data_1d.csv')\n",
      "16/6:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv')\n",
      "16/7:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "16/8:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "16/9:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x.head()\n",
      "16/10:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x.info()\n",
      "16/11:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x.tail()\n",
      "16/12:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "x = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x[0].head()\n",
      "16/13:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x = df[0]\n",
      "y = df[1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "16/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "\n",
      "x = np.random.random(10)\n",
      "y = np.random.random(10)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "16/15:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "x = df[0]\n",
      "y = df[1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "16/16:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values()\n",
      "16/17:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "16/18:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "x = df[0]\n",
      "x\n",
      "16/19:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "x = m[0]\n",
      "x\n",
      "16/20:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "x = m[:,0]\n",
      "x\n",
      "16/21:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.scatter(xm,ym)\n",
      "plt.show()\n",
      "16/22:\n",
      "# or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.scatter(xm,ym)\n",
      "plt.xlabel('xm')\n",
      "plt.ylabel('ym')\n",
      "plt.show()\n",
      "16/23:\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "xdf = df[0]\n",
      "ydf = df[1]\n",
      "\n",
      "plt.scatter(xdf,ydf)\n",
      "plt.xlabel('xdf')\n",
      "plt.ylabel('ydf')\n",
      "plt.show()\n",
      "16/24:\n",
      "## or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.scatter(xm,ym)\n",
      "plt.xlabel('xm')\n",
      "plt.ylabel('ym')\n",
      "\n",
      "# cheating ahead, we know that the correct linear regression equation is y = 2x + 1\n",
      "\n",
      "xline = np.linspace(0,100,100)\n",
      "yline = 2*xline +1\n",
      "plt.plot(xline,yline)\n",
      "plt.show()\n",
      "20/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "20/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "#ym = m[:,1]\n",
      "\n",
      "plt.hist(xm)\n",
      "20/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "#ym = m[:,1]\n",
      "\n",
      "plt.hist(xm)\n",
      "plt.show()\n",
      "20/4:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(100)\n",
      "20/5:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(100)\n",
      "r\n",
      "20/6:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(100)\n",
      "\n",
      "plt.hist(r)\n",
      "plt.show()\n",
      "20/7:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(10000)\n",
      "\n",
      "plt.hist(r)\n",
      "plt.show()\n",
      "20/8:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(np.random.random)\n",
      "\n",
      "plt.hist(r)\n",
      "plt.show()\n",
      "20/9:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "\n",
      "r = np.random.random(1000)\n",
      "\n",
      "plt.hist(r)\n",
      "plt.show()\n",
      "20/10:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "# default is 10 bins\n",
      "\n",
      "r = np.random.random(10000, bins = 20)\n",
      "\n",
      "plt.hist(r)\n",
      "plt.show()\n",
      "20/11:\n",
      "# let's visually test whether \"random\" data looks random when visualized\n",
      "# default is 10 bins\n",
      "\n",
      "r = np.random.random(10000)\n",
      "\n",
      "plt.hist(r, bins = 20)\n",
      "plt.show()\n",
      "20/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.hist(xm)\n",
      "plt.show()\n",
      "16/25:\n",
      "## or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "m.head()\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.scatter(xm,ym)\n",
      "plt.xlabel('xm')\n",
      "plt.ylabel('ym')\n",
      "\n",
      "# cheating ahead, we know that the correct linear regression equation is y = 2x + 1\n",
      "# let's plot by hand\n",
      "\n",
      "xline = np.linspace(0,100,100)\n",
      "yline = 2*xline +1\n",
      "plt.plot(xline,yline)\n",
      "plt.show()\n",
      "16/26:\n",
      "## or convert it to a matrix? Or numpy array?\n",
      "\n",
      "m = df.values\n",
      "m.head\n",
      "# remember, x[0] is from a pandas df, that would return the first column. m[0] is from a numpy array, that's the first row\n",
      "\n",
      "xm = m[:,0]\n",
      "ym = m[:,1]\n",
      "\n",
      "plt.scatter(xm,ym)\n",
      "plt.xlabel('xm')\n",
      "plt.ylabel('ym')\n",
      "\n",
      "# cheating ahead, we know that the correct linear regression equation is y = 2x + 1\n",
      "# let's plot by hand\n",
      "\n",
      "xline = np.linspace(0,100,100)\n",
      "yline = 2*xline +1\n",
      "plt.plot(xline,yline)\n",
      "plt.show()\n",
      "20/13:\n",
      "# how about a scatterplot of the linear regression errors?\n",
      "\n",
      "ym_actual = 2*xm+1\n",
      "20/14: ym_actual\n",
      "20/15:\n",
      "# how about a scatterplot of the linear regression errors?\n",
      "\n",
      "ym_actual = 2*xm+1\n",
      "ym_error = ym_actual - ym\n",
      "20/16:\n",
      "# how about a scatterplot of the linear regression errors?\n",
      "\n",
      "ym_actual = 2*xm+1\n",
      "ym_error = ym_actual - ym\n",
      "plt.hist(ym_error)\n",
      "plt.show()\n",
      "20/17:\n",
      "# how about a scatterplot of the linear regression errors?\n",
      "\n",
      "ym_actual = 2*xm+1\n",
      "ym_error = ym - ym_actual\n",
      "plt.hist(ym_error)\n",
      "plt.show()\n",
      "21/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "21/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "m.info()\n",
      "21/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "print(m.info())\n",
      "21/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "m = df.values\n",
      "print(m.info())\n",
      "21/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "21/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape())\n",
      "\n",
      "m = df.values\n",
      "21/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape)\n",
      "\n",
      "m = df.values\n",
      "21/8:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "21/9:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "im.shape\n",
      "21/10:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "im.head()\n",
      "21/11:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "im.head\n",
      "21/12:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "im[0:5]\n",
      "21/13:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,:1]\n",
      "im[0:5]\n",
      "21/14:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[0,1:]\n",
      "im[0:5]\n",
      "21/15:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[1,1:]\n",
      "im[0:5]\n",
      "21/16:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "\n",
      "im = m[1,1:]\n",
      "im[0:5]\n",
      "im.shape\n",
      "21/17:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "print('im shape:',im.shape)\n",
      "21/18:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "print('im shape:',im.shape)\n",
      "\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "21/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape)\n",
      "\n",
      "m = df.values\n",
      "21/20:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "print('im shape:',im.shape)\n",
      "\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "21/21:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "print('im shape:',im.shape)\n",
      "\n",
      "im\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/22:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "21/23:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.type\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/24:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "type(im)\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/25:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "class(im)\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/26:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.info\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/27:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.info()\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/28:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.dtype\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/29:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.dtype()\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/30:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.dtype\n",
      "#plt.imshow(im)\n",
      "#plt.show()\n",
      "21/31:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.dtype\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/32:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape)\n",
      "\n",
      "m = df.values\n",
      "m.dtype\n",
      "21/33:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape)\n",
      "\n",
      "m = df.values\n",
      "print('dtype:', m.dtype)\n",
      "21/34:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "print(df.shape)\n",
      "\n",
      "m = df.values\n",
      "print('dtype:', m.dtype)\n",
      "21/35:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "im.dtype\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/36:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vector = m[1,1:]\n",
      "print('vector shape:',vector.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/37:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = m[1,1:]\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vector.reshape(28,28)\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/38:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = m[1,1:]\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/39:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = m[1,1:]\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/40:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "\n",
      "m = df.values\n",
      "21/41:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = m[1,1:]\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/42:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:], dtype = float)\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/43:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:])\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/44:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "#plt.show()\n",
      "21/45:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "21/46:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "plt.show()\n",
      "21/47:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[101,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.show()\n",
      "21/48:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: ',m[1,0])\n",
      "plt.show()\n",
      "21/49:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "vc = np.array(m[1,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ m[1,0])\n",
      "plt.show()\n",
      "21/50:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 100\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ m[image,0])\n",
      "plt.show()\n",
      "21/51:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 103\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ m[image,0])\n",
      "plt.show()\n",
      "21/52:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 144\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ m[image,0])\n",
      "plt.show()\n",
      "21/53:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 41000\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ m[image,0])\n",
      "plt.show()\n",
      "21/54:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 41000\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "title_value = m[image,0]\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/55:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 4100\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "title_value = m[image,0]\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/56:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 100\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "title_value = m[image,0]\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/57:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 100\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/58:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 4100\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/59:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 200\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('label: '+ title_value)\n",
      "plt.show()\n",
      "21/60:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 200\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + image + ', label: '+ title_value)\n",
      "plt.show()\n",
      "21/61:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 200\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + ', label: '+ title_value)\n",
      "plt.show()\n",
      "21/62:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 200\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + image + ', label: '+ title_value)\n",
      "plt.show()\n",
      "21/63:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 1\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + image + ', label: '+ title_value)\n",
      "plt.show()\n",
      "21/64:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = 1\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/65:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/66:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/67:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/68:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image,0]\n",
      "\n",
      "vc = np.array(m[image,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/69:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image_number = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "print('vector shape:',vc.shape)\n",
      "print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/70:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image_number = np.random.randint(1,42000)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/71:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "print(df.info())\n",
      "print('')\n",
      "\n",
      "rows = len(df.shape)\n",
      "m = df.values\n",
      "21/72:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.shape)\n",
      "print(rows)\n",
      "m = df.values\n",
      "21/73:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "print(rows)\n",
      "m = df.values\n",
      "21/74:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/75:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "21/76:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "plt.show()\n",
      "22/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "print(rows)\n",
      "m = df.values\n",
      "22/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "22/3:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'grey')\n",
      "plt.show()\n",
      "22/4:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/5:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/6:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/7:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/8:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/9:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "# make it black-on-white:\n",
      "plt.imshow(255-im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/10:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "# make it black-on-white:\n",
      "plt.imshow(255-im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/11:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "# make it black-on-white:\n",
      "plt.imshow(255-im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/12:\n",
      "# first field is just a label, then 784 columns of pixels\n",
      "# there are 784 columns, which is 28x28. The images from this data set are made up of 28x28.\n",
      "# each row is an image. We can plot this data by creating a vector of the first row, but leaving out the value in the first column\n",
      "# but it's considering the first row to be the header?\n",
      "\n",
      "# pick a random row (have to substract 1 to account for 42000 rows?)\n",
      "image_number = np.random.randint(1,rows-1)\n",
      "\n",
      "# assign value in the first column as the title\n",
      "title_value = m[image_number,0]\n",
      "\n",
      "vc = np.array(m[image_number,1:],dtype = float)\n",
      "# for some reason I had to specify the array, and then add in the float argument...\n",
      "#print('vector shape:',vc.shape)\n",
      "#print('')\n",
      "\n",
      "# this creates a 784-item vector, but we need to make a 28x28 matrix. Use reshape...\n",
      "\n",
      "im = vc.reshape(28,28)\n",
      "im\n",
      "plt.imshow(im)\n",
      "plt.title('image: ' + str(image_number) + ', label: '+ str(title_value))\n",
      "# make it white-on-black:\n",
      "plt.imshow(im, cmap = 'gray')\n",
      "# make it black-on-white:\n",
      "plt.imshow(255-im, cmap = 'gray')\n",
      "plt.show()\n",
      "22/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import scipy as sc\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "22/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "22/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',norm.pdf(0))\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0)),4)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0)),2)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0)),4)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0)),8)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0),4)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('pdf of 0 for standard normal:',np.round(norm.pdf(0),4))\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6),4)) \n",
      "\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "np.random.random(30)\n",
      "\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/25:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "joel = np.random.random(30)\n",
      "norm.pdf(joel)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "22/26:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "r = np.random.randn(30)\n",
      "norm.pdf(joel)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "#plt.scatter(x,y)\n",
      "#plt.show()\n",
      "22/27:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "r = np.random.randn(30)\n",
      "y = norm.pdf(r)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "plt.scatter(r,y)\n",
      "plt.show()\n",
      "22/28:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "r = np.random.randn(1000)\n",
      "y = norm.pdf(r)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "# df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "plt.scatter(r,y)\n",
      "plt.show()\n",
      "22/29:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution...\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "norm.logpdf(x)\n",
      "22/30:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution...\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x, loc = 6, scale = 11)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x, loc = 1, scale = 1)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x, loc = 3, scale = 1)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "y = norm.pdf(x)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "# If you set the location (mean) to be something else it's going to be skewed, since all your numbers are between 0 and 1\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "\n",
      "# with 1000 random numbers, should look like a normal distribution\n",
      "# Not perfect though, it should all be within 3 SD's (+/- 3 since it's a standard normal)\n",
      "# If you set the location (mean) to be something else it's going to be skewed, since all your numbers are between 0 and 1\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# from a processing standpoint, it's easier to take the log of individual probabilities and add them\n",
      "# instead of taking the actual probabilities and multiplying them...\n",
      "# easy function for log probability\n",
      "\n",
      "n = norm.logpdf(x)\n",
      "\n",
      "# find the cdf\n",
      "norm.cdf(0)\n",
      "25/7:\n",
      "# sample from a random distribution (rand n is the argument)\n",
      "\n",
      "x = np.random.random(1000)\n",
      "\n",
      "# histogram, just need those values\n",
      "\n",
      "plt.hist(x, bins = 20)\n",
      "plt.show()\n",
      "25/8:\n",
      "# sample from a random distribution (rand n is the argument)\n",
      "\n",
      "x = np.random.randn(1000)\n",
      "\n",
      "# histogram, just need those values\n",
      "\n",
      "plt.hist(x, bins = 20)\n",
      "plt.show()\n",
      "25/9:\n",
      "# to really plot it, make a scatter plot\n",
      "y = norm.pdf(x)\n",
      "\n",
      "plot.scatter(x,y)\n",
      "plt.show()\n",
      "25/10:\n",
      "# to really plot it, make a scatter plot\n",
      "y = norm.pdf(x)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "25/11:\n",
      "# sample from a random distribution (rand n is the argument)\n",
      "\n",
      "x = np.random.randn(10000)\n",
      "\n",
      "# histogram, just need those values\n",
      "\n",
      "plt.hist(x, bins = 100)\n",
      "plt.show()\n",
      "25/12:\n",
      "# now how about sampling from a distribution with a set mean and distrubtion?\n",
      "\n",
      "r = 10 * np.random.rand(10000) + 5\n",
      "\n",
      "# this implies a mean of 5, SD of 10\n",
      "\n",
      "plt.hist(r, bins = 100)\n",
      "plt.show()\n",
      "25/13:\n",
      "# now how about sampling from a distribution with a set mean and distrubtion?\n",
      "\n",
      "r = 10 * np.random.randn(10000) + 5\n",
      "\n",
      "# this implies a mean of 5, SD of 10\n",
      "\n",
      "plt.hist(r, bins = 100)\n",
      "plt.show()\n",
      "25/14:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "25/15:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s.head()\n",
      "25/16:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[,:5]\n",
      "25/17:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[,0:5]\n",
      "25/18:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[0:2,0:5]\n",
      "25/19:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[0:5,0:2]\n",
      "25/20:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[0:100,0:2]\n",
      "25/21:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "s[0:10,0:2]\n",
      "25/22:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "#s[0:10,0:2]\n",
      "25/23:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[,0],s[,1])\n",
      "plt.show()\n",
      "\n",
      "#s[0:10,0:2]\n",
      "25/24:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[:,0],s[,1])\n",
      "plt.show()\n",
      "\n",
      "#s[0:10,0:2]\n",
      "25/25:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.rand(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "\n",
      "#s[0:10,0:2]\n",
      "25/26:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.randn(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "\n",
      "#s[0:10,0:2]\n",
      "25/27:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.randn(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "\n",
      "s[0:10,0:2]\n",
      "25/28:\n",
      "# now eliptical - 2 different variances\n",
      "# make second have mean 2 sd 5\n",
      "\n",
      "s[:,1] = 5*s[:,1]+2\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "25/29:\n",
      "# now eliptical - 2 different variances\n",
      "# make second have mean 200 sd 5\n",
      "\n",
      "s[:,1] = 5*s[:,1]+200\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "25/30:\n",
      "# now eliptical - 2 different variances\n",
      "# make second have mean 200 sd 5\n",
      "\n",
      "s[:,1] = 5*s[:,1]+200\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "\n",
      "# set axes to be equal\n",
      "plt.axes('equal')\n",
      "plt.show()\n",
      "25/31:\n",
      "# now eliptical - 2 different variances\n",
      "# make second have mean 200 sd 5\n",
      "\n",
      "s[:,1] = 5*s[:,1]+200\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "\n",
      "# set axes to be equal\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "25/32:\n",
      "# now sample from a 2-dimensional Gaussian, both normal distribution\n",
      "# this is s spherical Gaussian - independent, uncorrelated, same variance\n",
      "\n",
      "s = np.random.randn(10000,2)\n",
      "\n",
      "x = s[0]\n",
      "y = s[1]\n",
      "\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "plt.show()\n",
      "\n",
      "# will be circular if we do the axes correctly\n",
      "\n",
      "# to look at a data sample\n",
      "# s[0:10,0:2]\n",
      "25/33:\n",
      "# now eliptical - 2 different variances\n",
      "# make second have mean 200 sd 5\n",
      "\n",
      "s[:,1] = 5*s[:,1]+200\n",
      "plt.scatter(s[:,0],s[:,1])\n",
      "\n",
      "# set axes to be equal\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "29/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "print('probability density of 0 for standard normal:',np.round(norm.pdf(0),4)) \n",
      "\n",
      "print('probability density of 0 for normal mean of 6, sd of 11:',np.round(norm.pdf(0, loc = 6, scale = 11),4)) \n",
      "\n",
      "# make a random array\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "30/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import norm\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([1,.8],[.8,3])\n",
      "\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "30/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([1,.8],[.8,3])\n",
      "\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "30/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "30/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "x = np.random.random(1000)\n",
      "y = norm.pdf(x)\n",
      "30/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "# to sample, use rvs (mean = None, cov = 1, size = 1) where size is the number of samples\n",
      "\n",
      "r = rvs (mean, cov, 100)\n",
      "30/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "# to sample, use rvs (mean = None, cov = 1, size = 1) where size is the number of samples\n",
      "\n",
      "r = mvn.rvs(mean, cov, 100)\n",
      "30/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "# to sample, use rvs (mean = None, cov = 1, size = 1) where size is the number of samples\n",
      "\n",
      "r = mvn.rvs(mean, cov, 100)\n",
      "\n",
      "r[5,:]\n",
      "30/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "# to sample, use rvs (mean = None, cov = 1, size = 1) where size is the number of samples\n",
      "\n",
      "r = mvn.rvs(mean, cov, 100)\n",
      "\n",
      "r[:,:5]\n",
      "30/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# sampling when a general multivariate distribution where covariance exists\n",
      "# var of 1 for first dimension, 3 for second, with 0.8 covariance\n",
      "\n",
      "cov = np.array([[1,.8],[.8,3]])\n",
      "mean = np.array([0,2])\n",
      "\n",
      "# to sample, use rvs (mean = None, cov = 1, size = 1) where size is the number of samples\n",
      "\n",
      "r = mvn.rvs(mean, cov, 100)\n",
      "\n",
      "r[:5,:]\n",
      "30/10:\n",
      "# check scatter plot\n",
      "\n",
      "plt.scatter(r[:,0],r[:,1])\n",
      "plt.show()\n",
      "30/11:\n",
      "# check scatter plot\n",
      "\n",
      "plt.scatter(r[:,0],r[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "30/12:\n",
      "# we can also do this in numpy\n",
      "\n",
      "r_numpy = np.random.multivariate_normal(mean, cov, 100)\n",
      "30/13:\n",
      "# we can also do this in numpy\n",
      "\n",
      "r_numpy = np.random.multivariate_normal(mean, cov, 100)\n",
      "plt.scatter(r_numpy[:,0],rr_numpy[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "30/14:\n",
      "# we can also do this in numpy\n",
      "\n",
      "r_numpy = np.random.multivariate_normal(mean, cov, 100)\n",
      "plt.scatter(r_numpy[:,0],r_numpy[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "V = np.array([1/3,1/3,1/3])\n",
      "31/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new V on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "np.round(v.dot(A),2))\n",
      "31/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new V on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "np.round(v.dot(A),2)\n",
      "31/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new v on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "v = np.round(v.dot(A),2)\n",
      "31/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new v on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "v = np.round(v.dot(A),2)\n",
      "31/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new v on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "v = np.round(v.dot(A),2)\n",
      "31/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new v on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "v = np.round(v.dot(A),2)\n",
      "print(v)\n",
      "31/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from scipy.stats import multivariate_normal as mvn\n",
      "\n",
      "# exercise 1\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to get a new v on each step:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "\n",
      "v = np.round(v.dot(A),2)\n",
      "print(v)\n",
      "31/9:\n",
      "import numpy as np\n",
      "\n",
      "A = np.array[[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]]\n",
      "v = np.array()\n",
      "print('aXb:', np.round(a.dot(b),2))\n",
      "31/10:\n",
      "import numpy as np\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "31/11:\n",
      "v = np.round(v.dot(A),2)\n",
      "\n",
      "print(v)\n",
      "31/12:\n",
      "v = np.round(v.dot(A),2)\n",
      "\n",
      "print(v)\n",
      "31/13:\n",
      "v = np.round(v.dot(A),2)\n",
      "\n",
      "print(v)\n",
      "31/14:\n",
      "for i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/15:\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/16:\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "    return v\n",
      "31/17:\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "    return(v)\n",
      "31/18:\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/19:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "print(i)\n",
      "31/20:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i <= 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/21:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/22:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "    return v\n",
      "31/23:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    v = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    print(i, v)\n",
      "31/24:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "31/25:\n",
      "import numpy as np\n",
      "import matploylib as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "31/26:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    # print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/27:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "31/28:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes\n",
      "    \n",
      "#for num_iters < 25:\n",
      "31/29:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes\n",
      "    \n",
      "#for num_iters < 25:\n",
      "distances\n",
      "31/30:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "31/31:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    print(i,vprime)\n",
      "31/32:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/33:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    #v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/34:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/35:\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "31/36:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/37:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    v = vprime\n",
      "    print(i,vprime)\n",
      "31/38:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "\n",
      "    print(i,vprime)\n",
      "31/39:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    print(i,vprime,d)\n",
      "31/40:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/41:\n",
      "import numpy as np\n",
      "import matplotlib as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "31/42:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/43:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/44:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = np.round(v.dot(A),2)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "#    print(i,vprime,d)\n",
      "\n",
      "print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/45:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "    \n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    print(i,vprime,d)\n",
      "\n",
      "print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/46:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A),2\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/47:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    print(i, v)\n",
      "\n",
      "# but I can't get v to iterate...then I could try to plot it\n",
      "31/48:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "31/49:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters) # make a vector of zeroes to hold the distances\n",
      "print(distances)    \n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/50:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros(num_iters,2) # make a vector of zeroes to hold the distances\n",
      "print(distances)    \n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/51:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "print(distances)    \n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/52:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i,0] = i\n",
      "    distances[i,1] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances)\n",
      "plt.show()\n",
      "31/53:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i,0] = i\n",
      "    distances[i,1] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/54:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "31/55:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "31/56:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "    distances[i,0] = i\n",
      "    distances[i,1] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "# print(distances)\n",
      "plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/57:\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v)\n",
      "#    distances[i,0] = i\n",
      "#    distances[i,1] = d\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "#plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/58:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "i = 0\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v) # I don't get this function, I guess it somehow calculates the difference\n",
      "#    distances[i,0] = i\n",
      "#    distances[i,1] = d\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "#plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/59:\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v) # I don't get this function, I guess it somehow calculates the difference\n",
      "#    distances[i,0] = i\n",
      "#    distances[i,1] = d\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "#plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/60:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v) # I don't get this function, I guess it somehow calculates the difference\n",
      "#    distances[i,0] = i\n",
      "#    distances[i,1] = d\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "#plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/61:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "\n",
      "sample = np.random.random()\n",
      "31/62:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "\n",
      "sample = np.random.random()\n",
      "sample\n",
      "31/63:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "\n",
      "sample = np.random.random(10)\n",
      "sample\n",
      "31/64:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "# Now take the mean of the sample\n",
      "31/65:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "# Now take the mean of the sample\n",
      "31/66:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "sample.type()\n",
      "# Now take the mean of the sample\n",
      "31/67:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "type(sample)\n",
      "# Now take the mean of the sample\n",
      "31/68:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "mean(sample)\n",
      "# Now take the mean of the sample\n",
      "31/69:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "31/70:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 100\n",
      "clt = np.zeros(num_iters)\n",
      "31/71:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 100\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "31/72:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 100\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.histogram(clt)\n",
      "plt.show()\n",
      "31/73:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 100\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt)\n",
      "plt.show()\n",
      "31/74:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 100\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt)\n",
      "plt.show()\n",
      "31/75:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt)\n",
      "plt.show()\n",
      "31/76:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 10)\n",
      "plt.show()\n",
      "31/77:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "31/78:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "31/79:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "31/80:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/81:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(2)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/82:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/83:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10000)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/84:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(1000000)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/85:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100000)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/86:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/87:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\\\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/88:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "# do 25 times to produce a new v each time:\n",
      "# v' = vA\n",
      "# v = v'\n",
      "# After 25 times, we've calculated v*A^25\n",
      "# Plot Euclidian distance |v'-v|, it should converge to 0.\n",
      "# When it is 0, we've found \"the eigenvector for A for which the corresponding eigenvalue is 1\"\n",
      "\n",
      "while i < 25:\n",
      "    vprime = v.dot(A)\n",
      "    i+=1\n",
      "    v = vprime\n",
      "    #print(i, v)\n",
      "\n",
      "# will not iterate if results are rounded!\n",
      "\n",
      "# answer from the repo\n",
      "\n",
      "A = np.array([[.3,.6,.1],[.5,.2,.3],[.4,.1,.5]])\n",
      "v = np.array([1/3,1/3,1/3])\n",
      "\n",
      "num_iters = 25\n",
      "distances = np.zeros([num_iters,2]) # make a vector of zeroes to hold the distances\n",
      "\n",
      "for i in range(num_iters):\n",
      "    vprime = v.dot(A)\n",
      "    d = np.linalg.norm(vprime-v) # I don't get this function, I guess it somehow calculates the difference\n",
      "#    distances[i,0] = i\n",
      "#    distances[i,1] = d\n",
      "    distances[i] = d\n",
      "    v = vprime\n",
      "    #print(i,vprime,d)\n",
      "\n",
      "plt.plot(distances)\n",
      "#plt.plot(distances[:,0], distances[:,1])\n",
      "plt.show()\n",
      "31/89:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/90:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/91:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/92:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(100)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/93:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(1000)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/94:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10000)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/95:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.mean(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/96:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "31/97:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m[[:5,:5]]\n",
      "31/98:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m[[0:5,0:5]]\n",
      "31/99:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m([[0:5,0:5]])\n",
      "31/100:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m[0:5,0:5]\n",
      "31/101:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "joel = m.groupby('label').mean()\n",
      "\n",
      "m[0:5,0:5]\n",
      "31/102:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "joel = df.groupby('label').mean()\n",
      "\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/103:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#joel = df.groupby('label').mean()\n",
      "df(columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/104:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#joel = df.groupby('label').mean()\n",
      "df.columns\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/105:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv', header = None)\n",
      "#joel = df.groupby('label').mean()\n",
      "list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/106:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "#joel = df.groupby('label').mean()\n",
      "list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/107:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "joel = df.groupby('label').mean()\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/108:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "joel = df.groupby('label').mean()\n",
      "joel\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/109:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean().values\n",
      "\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/110:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean().values\n",
      "labelsum\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/111:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "labelsum = labelsum.values\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/112:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "labelsum = labelsum.values\n",
      "labelsum\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/113:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "labelsum.type\n",
      "\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/114:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "type(labelsum)\n",
      "\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/115:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "type(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/116:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "type(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "type(m)\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/117:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "type(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "type(m)\n",
      "#list(df.columns)\n",
      "#rows = len(df.index)\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/118:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "#list(df.columns)\n",
      "rows = len(labelsum)\n",
      "rows\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/119:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    m[i,0] = label\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/120:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/121:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.image(plot_data)\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/122:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(plot_data)\n",
      "    plt.title(label)\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/123:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    print('label: ',label)\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(plot_data)\n",
      "    plt.title(label)\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/124:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    print('label: ',label)\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title(label)\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/125:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + label)\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/126:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/127:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m)\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/128:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m.info())\n",
      "print(m)\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/129:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(info(m))\n",
      "print(m)\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/130:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m.type)\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/131:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(type(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/132:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/133:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(size(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/134:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m)\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/135:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m.head())\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/136:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(head(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/137:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(m[0,:])\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/138:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "\n",
      "m = labelsum.values\n",
      "print(len(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/139:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "print(labelsum)\n",
      "m = labelsum.values\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/140:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "print(labelsum)\n",
      "m = labelsum.values\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = labelsum[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/141:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/142:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/143:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "print(np.info(m))\n",
      "print(m[:,0])\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/144:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "labels = labelsum[0]\n",
      "print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/145:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "#labels = labelsum[0]\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/146:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "labels = labelsum[0]\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/147:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "labels = labelsum[0]\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/148:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "labels = labelsum.as_matrix(columns=labelsum.columns[0])\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/149:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#labels = labelsum.as_matrix(columns=labelsum.columns[0])\n",
      "labels = labelsum.value_column.values\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/150:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "labelsum.reset_index(inplace = 'true')\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#labels = labelsum.as_matrix(columns=labelsum.columns[0])\n",
      "labels = labelsum.value_column.values\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/151:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label').mean()\n",
      "#labelsum.reset_index(inplace = 'true')\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/152:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "print(df.info())\n",
      "labelsum = df.groupby('label').mean()\n",
      "#labelsum.reset_index(inplace = 'true')\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/153:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "#labelsum.reset_index(inplace = 'true')\n",
      "\n",
      "print(labelsum.info())\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,0:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/154:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/155:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(np.round(label,0)))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/156:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/157:\n",
      "# Exercise 2, Central Limit Theorem\n",
      "\n",
      "# Take a sample of 10 random numbers\n",
      "sample = np.random.random(10)\n",
      "np.mean(sample)\n",
      "# Now take the mean of the sample\n",
      "\n",
      "# Repeat 100 times and see if the results look like a normal distribution\n",
      "\n",
      "num_iters = 10000\n",
      "clt = np.zeros(num_iters)\n",
      "\n",
      "for i in range(num_iters):\n",
      "    sample = np.random.random(10)\n",
      "    clt[i] = np.sum(sample)\n",
      "    \n",
      "plt.hist(clt, bins = 100)\n",
      "plt.show()\n",
      "\n",
      "print ('mean of clt:', np.mean(clt))\n",
      "print ('var of clt:', np.var(clt))\n",
      "31/158:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray', transform=tr + ax.transData)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/159:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray', transform=tr + plot_data.transData)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/160:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "#print(labels)\n",
      "\n",
      "#print(np.info(m))\n",
      "# loop through and plot each\n",
      "\n",
      "tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray', transform=tr)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#joel = m.groupby('label').mean()\n",
      "#m[0:5,0:5]\n",
      "31/161:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "#tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "#    plt.imshow(255-plot_data,cmap = 'gray', transform=tr + plot_data.transData)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "31/162:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "#tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plot_data = np.rot90(plot_data)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "#    plt.imshow(255-plot_data,cmap = 'gray', transform=tr + plot_data.transData)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "31/163:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "#tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "    plot_data = np.rot90(plot_data)\n",
      "    plot_data = np.rot90(plot_data)\n",
      "    plot_data = np.rot90(plot_data)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "#    plt.imshow(255-plot_data,cmap = 'gray', transform=tr + plot_data.transData)\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "31/164:\n",
      "# Exercise 3: Load in the MMIST dataset and plot the mean image for each digit class (0-9)\n",
      "import pandas as pd\n",
      "from matplotlib import transforms\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/1_Numpy_Stack_Python/machine_learning_examples/large_files/'\n",
      "\n",
      "df = pd.read_csv(path + 'train.csv')\n",
      "labelsum = df.groupby('label', as_index = False).mean()\n",
      "# Set the label to not be an index, otherwise it won't be included in the array when it gets converted.\n",
      "# And I want it for the title of the chart.\n",
      "\n",
      "rows = len(labelsum)\n",
      "m = labelsum.values\n",
      "\n",
      "# loop through and plot each\n",
      "\n",
      "#tr = transforms.Affine2D().rotate_deg(90)\n",
      "\n",
      "for i in range(rows):\n",
      "    label = m[i,0]\n",
      "    plot_data = m[i,1:].reshape(28,28)\n",
      "# Exercise 4 is to continue with the dataset, but flip the images 90 degrees\n",
      "# All that is needed is the np.rot90() function\n",
      "    plot_data = np.rot90(plot_data,2)\n",
      "    plt.imshow(255-plot_data,cmap = 'gray')\n",
      "    plt.title('label: ' + str(label))\n",
      "    plt.show()\n",
      "31/165:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "M1 = np.array([1,2],[3,4])\n",
      "31/166:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "31/167:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print M1\n",
      "print M1T\n",
      "31/168:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(M1)\n",
      "print(M1T)\n",
      "31/169:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test1:\n",
      "    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(rows(M1))\n",
      "print(M1T)\n",
      "31/170:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "#def test1:\n",
      "#    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(rows(M1))\n",
      "print(M1T)\n",
      "31/171:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "#def test1:\n",
      "#    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2],[3,4]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(len(M1))\n",
      "print(M1T)\n",
      "31/172:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "#def test1:\n",
      "#    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(len(M1))\n",
      "print(width(M1))\n",
      "\n",
      "print(M1T)\n",
      "31/173:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "#def test1:\n",
      "#    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(size(M1,0))\n",
      "print(size(M1,1))\n",
      "\n",
      "print(M1T)\n",
      "31/174:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "#def test1:\n",
      "#    rows \n",
      "\n",
      "\n",
      "M1 = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "\n",
      "\n",
      "M1T = M1.transpose()\n",
      "\n",
      "print(np.size(M1,0))\n",
      "print(np.size(M1,1))\n",
      "\n",
      "print(M1T)\n",
      "31/175:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test1(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows:\n",
      "        return False\n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "test1(m)\n",
      "31/176:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    return True\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "test1(m)\n",
      "31/177:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    return True\n",
      "    print(message)\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "test1(m)\n",
      "31/178:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "\n",
      "test1(m)\n",
      "31/179:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/180:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/181:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(m)\n",
      "31/182:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    \n",
      "    for i in range(rows):\n",
      "        for j in range(columns):\n",
      "            if m[i,j] != m[j,i]:\n",
      "                message = \"Matrix values are not symmetric\"\n",
      "                print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/183:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    elif:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/184:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    elif columns == rows:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/185:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "#    message = \"Matrix is symmetric\"\n",
      "#    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test1(n)\n",
      "31/186:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/187:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(o)\n",
      "31/188:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(m)\n",
      "31/189:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows):\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True\n",
      "\n",
      "        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(o)\n",
      "31/190:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "np.random.random(10000,2)\n",
      "31/191:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "31/192:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "head(x)\n",
      "31/193:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "np.head(x)\n",
      "31/194:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "x[:5]\n",
      "31/195:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "np.max(x)\n",
      "31/196:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "np.min(x)\n",
      "31/197:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "x = 2*np.random.random(10000)-1\n",
      "y = 2*np.random.random(10000)-1\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/198:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/199:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.zeros(points)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/200:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.zeros(points)\n",
      "for i in range(colors):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/201:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.zeros(points)\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/202:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.zeros(points)\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "#    else:\n",
      "#        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/203:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = []\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/204:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty[1,points]\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/205:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points])\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/206:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/207:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(colors)\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/208:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(colors.shape())\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/209:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(np.shape(colors))\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "colors[:10]\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/210:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(colors[:10])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/211:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(colors[:,:10])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/212:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([1,points],dtype = 'str')\n",
      "print(colors[:10,:])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/213:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'str')\n",
      "print(colors[:10,:])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/214:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'str')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/215:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'str')\n",
      "\n",
      "colors[:1,:1]\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/216:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'str')\n",
      "\n",
      "print(colors[:10,:])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/217:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'str')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/218:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 's10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/219:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'S10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/220:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = 'S10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/221:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = '<U10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/222:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = '<U10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "plt.scatter(x,y, color = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/223:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = '<U10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 'red'\n",
      "        \n",
      "plt.scatter(x,y, c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/224:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "colors = np.empty([points,1],dtype = '<U10')\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "        \n",
      "plt.scatter(x,y, c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/225:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "        \n",
      "plt.scatter(x,y, c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/226:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(points[:10,:])\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/227:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(colors[:10,:])\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/228:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(np.type(colors))\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/229:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(np.shape(colors))\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/230:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(type(colors))\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/231:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "print(columns(colors))\n",
      "        \n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/232:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "        \n",
      "plt.scatter(x,y, c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/233:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(x[:,0] < 0) & (y[:,0] > 0)] = 1\n",
      "repo[(x[:,0] > 0) & (y[:,0] < 0)] = 1\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/234:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((N, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/235:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/236:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "print(colors[:5])\n",
      "print(repo[:5])\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/237:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "print(colors[:5])\n",
      "print(repo[:5])\n",
      "\n",
      "print(np.shape(colors))\n",
      "print(np.shape(repo))\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/238:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "print(colors[:5])\n",
      "print(repo[:5])\n",
      "\n",
      "print(np.type(colors))\n",
      "print(np.type(repo))\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/239:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points,1])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "print(colors[:5])\n",
      "print(repo[:5])\n",
      "\n",
      "print(np.info(colors))\n",
      "print(np.info(repo))\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/240:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "repo = np.zeros(points)\n",
      "repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "print(colors[:5])\n",
      "print(repo[:5])\n",
      "\n",
      "print(np.info(colors))\n",
      "print(np.info(repo))\n",
      "\n",
      "#plt.scatter(x,y, c = colors)\n",
      "#plt.axis('equal')\n",
      "#plt.show()\n",
      "31/241:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points)-1\n",
      "y = 2*np.random.random(points)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i] * y[i] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "#X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#print(colors[:5])\n",
      "#print(repo[:5])\n",
      "\n",
      "#print(np.info(colors))\n",
      "#print(np.info(repo))\n",
      "\n",
      "plt.scatter(x,y, c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/242:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random(points,2)-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "#X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#print(colors[:5])\n",
      "#print(repo[:5])\n",
      "\n",
      "#print(np.info(colors))\n",
      "#print(np.info(repo))\n",
      "\n",
      "plt.scatter(x[:0],x[:1], c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/243:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = np.random.random(points,2)*2-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "#X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#print(colors[:5])\n",
      "#print(repo[:5])\n",
      "\n",
      "#print(np.info(colors))\n",
      "#print(np.info(repo))\n",
      "\n",
      "plt.scatter(x[:0],x[:1], c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/244:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "#X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#print(colors[:5])\n",
      "#print(repo[:5])\n",
      "\n",
      "#print(np.info(colors))\n",
      "#print(np.info(repo))\n",
      "\n",
      "plt.scatter(x[:0],x[:1], c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/245:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "#X = np.random.random((points, 2))*2 - 1\n",
      "\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "#print(colors[:5])\n",
      "#print(repo[:5])\n",
      "\n",
      "#print(np.info(colors))\n",
      "#print(np.info(repo))\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/246:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'gray')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/247:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/248:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 10\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/249:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 100\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/250:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 10000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/251:\n",
      "# Exercise 6 - make a plot with NW and SE squares red, SW and NE squares blue\n",
      "\n",
      "points = 1000\n",
      "\n",
      "x = 2*np.random.random((points,2))-1\n",
      "#colors = np.empty([points,1],dtype = '<U10')\n",
      "colors = np.zeros([points])\n",
      "\n",
      "for i in range(points):\n",
      "    if x[i,0] * x[i,1] > 0:\n",
      "        colors[i] = 1\n",
      "#        colors[i] = 'blue'\n",
      "    else:\n",
      "        colors[i] = 0\n",
      "\n",
      "# could also do it this way without a loop...\n",
      "#repo = np.zeros(points)\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "#repo[(X[:,0] > 0) & (X[:,1] < 0)] = 1\n",
      "\n",
      "plt.scatter(x[:,0],x[:,1], c = colors, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/252:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "radius = np.random.random(points)\n",
      "x = np.zeroes((points, 2))\n",
      "31/253:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "radius = np.random.random(points)\n",
      "x = np.zeros((points, 2))\n",
      "31/254:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "radius = round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "x = np.zeros((points, 2))\n",
      "31/255:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "radius = np.round(np.random.random(points),0)\n",
      "\n",
      "x = np.zeros((points, 2))\n",
      "31/256:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "x = np.round(np.random.random(points),0)\n",
      "31/257:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "m(:,5)\n",
      "31/258:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "m(:,:5)\n",
      "31/259:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 20\n",
      "small_radius = 10\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "m(:5,:)\n",
      "31/260:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "m(0:5,:)\n",
      "31/261:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "\n",
      "m\n",
      "31/262:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:]=0] = large_radius + 2*np.random.random-1\n",
      "r[m[:]=1] = small_radius + 2*np.random.random-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/263:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:]=0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:]=1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/264:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m=0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:]=1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/265:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:,0]=0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0]=1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/266:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0]=1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/267:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/268:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "r = np.zeros(points)\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/269:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "\n",
      "r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/270:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "\n",
      "r[(m[:,0] == 0)] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/271:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "np.shape(r)\n",
      "\n",
      "#r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "#r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/272:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "np.shape(m)\n",
      "\n",
      "#r[m[:,0] == 0] = large_radius + 2*np.random.random()-1\n",
      "#r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/273:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:,0] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/274:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "m = np.round(np.random.random(points),0)\n",
      "r = np.zeros(points)\n",
      "\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/275:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*pi\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/276:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/277:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "x[r[:] * np.cos(a[:])]\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/278:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "\n",
      "\n",
      "#repo[(X[:,0] < 0) & (X[:,1] > 0)] = 1\n",
      "31/279:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.show()\n",
      "31/280:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plot.axis('equal')\n",
      "plt.show()\n",
      "31/281:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/282:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius# + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius# + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/283:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "m(:5)\n",
      "31/284:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "m(:5,1)\n",
      "31/285:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "m[:5]\n",
      "31/286:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "31/287:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "31/288:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "print(x[:5])\n",
      "print(y[:5])\n",
      "31/289:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "#x[r[:] * np.cos(a[:])]\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.plot(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "print(a[:5])\n",
      "print(x[:5])\n",
      "print(y[:5])\n",
      "31/290:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "print(a[:5])\n",
      "print(x[:5])\n",
      "print(y[:5])\n",
      "31/291:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.scatter(x,y, c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "31/292:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i])\n",
      "    y[i] = r[i] * np.sin(a[i])\n",
      "\n",
      "plt.scatter(x,y, c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "31/293:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/294:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False    \n",
      "                message = \"Matrix is symmetric\"\n",
      "                print(message)\n",
      "                return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/295:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/296:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/297:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/298:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/299:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(n)\n",
      "31/300:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    else:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    \n",
      "    message = \"Matrix is symmetric\"\n",
      "    print(message)\n",
      "    return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(o)\n",
      "31/301:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    elif columns == rows:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    else:\n",
      "        message = \"Matrix is symmetric\"\n",
      "        print(message)\n",
      "        return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(o)\n",
      "31/302:\n",
      "# Exercise 5: define a function to determine whether a matrix is symmetric\n",
      "# Symmetric: Matrix is equal to its transpose\n",
      "\n",
      "# Kind of tough, going to need to refer to the repo a bit\n",
      "\n",
      "def test(m):\n",
      "    rows = np.size(m,0)\n",
      "    columns = np.size(m,1)\n",
      "    if columns != rows: # first test that number of rows = number of columns\n",
      "        message = \"Matrix is not square\"\n",
      "        print(message)\n",
      "        return False\n",
      "    elif 1>0:\n",
      "        for i in range(rows): # if it's square, compare individual values\n",
      "            for j in range(columns):\n",
      "                if m[i,j] != m[j,i]:\n",
      "                    message = \"Matrix values are not symmetric\"\n",
      "                    print(message)\n",
      "                    return False\n",
      "    else:\n",
      "        message = \"Matrix is symmetric\"\n",
      "        print(message)\n",
      "        return True        \n",
      "\n",
      "m = np.array([[1,2,3,4],[3,4,5,6]])\n",
      "n = np.array([[1,4],[1,1]])\n",
      "o = np.array([[1,1],[1,1]])\n",
      "\n",
      "test(o)\n",
      "31/303:\n",
      "# Exercise 7 - similar but with concentric donuts...\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.round(np.random.random(points),0)\n",
      "\n",
      "# Now assign the large radius to the 0,s small radius to the 1, add in some variation to give thickness to the donut\n",
      "r = np.zeros(points)\n",
      "r[m[:] == 0] = large_radius# + 2*np.random.random()-1\n",
      "r[m[:] == 1] = small_radius# + 2*np.random.random()-1\n",
      "\n",
      "# Now make the angle, randomized to 2pi radians\n",
      "a = np.random.random(points)*2*np.pi\n",
      "\n",
      "# Now the actual coordinates, cosine for the x, sine for the y\n",
      "x = np.zeros(points)\n",
      "y = np.zeros(points)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    y[i] = r[i] * np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "\n",
      "plt.scatter(x,y, c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "print(m[:5])\n",
      "print(r[:5])\n",
      "31/304:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(5, size = points)\n",
      "31/305:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(5, size = points)\n",
      "m[[:],[:5]]\n",
      "31/306:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(5, size = points)\n",
      "m[:,:5]\n",
      "31/307:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(5, size = points)\n",
      "m[:5]\n",
      "31/308:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(5, size = points)\n",
      "m[:50]\n",
      "31/309:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0/1 values\n",
      "m = np.random.randint(6, size = points)\n",
      "m[:50]\n",
      "31/310:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "31/311:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "large_radius = 10\n",
      "small_radius = 5\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    y[i] = r[i] * np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/312:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = r[i] * np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    y[i] = r[i] * np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/313:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    y[i] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/314:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros(points,2)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    y[i] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/315:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros(points,2)\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    x[i,1] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/316:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    x[i,1] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x,y)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/317:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    x[i,1] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x[:0],x[:1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/318:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + 1.5*np.random.random()-.75\n",
      "    x[i,1] = np.sin(a[i]) + 1.5*np.random.random()-.75\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/319:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/320:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)*30\n",
      "\n",
      "print(m[:10])\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/321:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)*60\n",
      "\n",
      "print(m[:50])\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/322:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "print(m[:50])\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]*m[i]*np.pi/6\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/323:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "print(m[:50])\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]*m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/324:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "print(m[:50])\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/325:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi/4\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/326:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/327:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    #a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/328:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/329:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = .5 + np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = .5 + np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/330:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = .5 + np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = .5 + np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/331:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(.5 + a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(.5 + a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/332:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = .5 + np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = .5 + np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1])\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/333:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/334:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(2, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/335:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/336:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi/6\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/337:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi/4\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/338:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi/3\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/339:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .1*np.random.random()-.05\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/340:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/341:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi radians so we get half circles\n",
      "a = np.random.random(points)*np.pi/3\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/342:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(6, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi/3\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/343:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/344:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 3\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi/3) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/345:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 3\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/346:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 4\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/347:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 5\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m, cmap = 'bwr_r')\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/348:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 5\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/349:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 3\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/350:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 2\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/351:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/352:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = m[i] + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/353:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = m[i] + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = m[i] + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/354:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi) + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = m[i] + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/355:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi) + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi) + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/356:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.sin(m[i]*np.pi) + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.cos(m[i]*np.pi) + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/357:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi) + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi) + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/358:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi)/2 + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi)/2 + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/359:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.sin(m[i]*np.pi)/2 + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.cos(m[i]*np.pi)/2 + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/360:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi)/2 + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi)/2 + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/361:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi)/4 + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi)/4 + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/362:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(m[i]*np.pi)/2 + np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    x[i,1] = np.sin(m[i]*np.pi)/2 + np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/363:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/364:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 #+ np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 #+ np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/365:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/366:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/367:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/4\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/4\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/368:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/369:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/370:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)*2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)*2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/371:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/372:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/373:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/3\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/3\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/374:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/10\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/10\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/375:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - 2*np.cos(m[i]*np.pi)\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - 2*np.sin(m[i]*np.pi)\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/376:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/377:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/378:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/379:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/380:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/381:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/382:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/383:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/384:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "axis_adj = np.zeros(points,2)\n",
      "\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/385:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/386:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 1,4] = 1\n",
      "\n",
      "print(m[:5])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/387:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "print(m[:5])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/388:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "print(m[:5])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/389:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "print(m[:5])\n",
      "print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025# - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/390:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/391:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -1\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 1\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/392:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = 0\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 0\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/393:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -1\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 1\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/394:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = 2\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 2\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/395:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -2\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 2\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/396:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -1\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 2\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/397:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.75\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = 2\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/398:\n",
      "# Exercise 8 - spirals\n",
      "\n",
      "points = 1000\n",
      "arcs = 6\n",
      "\n",
      "# Make an array of 0-5 values\n",
      "m = np.random.randint(arcs, size = points)\n",
      "\n",
      "# Not sure this is great, but adjust the x/y coordinates based on the value of m\n",
      "axis_adj = np.zeros((points,2))\n",
      "axis_adj[m[:] == 0,0] = -1\n",
      "axis_adj[m[:] == 1,0] = -.5\n",
      "axis_adj[m[:] == 2,0] = 1\n",
      "axis_adj[m[:] == 3,0] = 1\n",
      "axis_adj[m[:] == 4,0] = .5\n",
      "axis_adj[m[:] == 5,0] = -1\n",
      "axis_adj[m[:] == 1,1] = -1\n",
      "axis_adj[m[:] == 4,1] = 1\n",
      "\n",
      "#print(m[:5])\n",
      "#print(axis_adj[:5,:])\n",
      "\n",
      "# Make an angle, only to pi/3 radians so we get 1/6 of a circle\n",
      "a = np.random.random(points)*np.pi*2/arcs\n",
      "# Now adjust by m*pi/6 radians\n",
      "\n",
      "x = np.zeros((points,2))\n",
      "\n",
      "for i in range(points):\n",
      "#    a[i] = a[i]+m[i]*np.pi/3\n",
      "    x[i,0] = np.cos(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,0] # - np.cos(m[i]*np.pi)/2\n",
      "    x[i,1] = np.sin(a[i]+m[i]*np.pi*2/arcs) + .05*np.random.random()-.025 + axis_adj[i,1] # - np.sin(m[i]*np.pi)/2\n",
      "    \n",
      "plt.scatter(x[:,0],x[:,1], c = m)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "31/399: x[:,:5]\n",
      "31/400:\n",
      "x[:,:5]\n",
      "np.shape(x)\n",
      "31/401:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "x.to_csv('x.csv', index = 'false')\n",
      "31/402:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x)\n",
      "#x.to_csv('x.csv', index = 'false')\n",
      "31/403:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x)\n",
      "\n",
      "x\n",
      "#x.to_csv('x.csv', index = 'false')\n",
      "31/404:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x)\n",
      "xdf.to_csv('xdf.csv', index = 'false')\n",
      "31/405:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x, columns = ['x_coord', 'y_coord'])\n",
      "xdf.to_csv('xdf.csv', index = 'false')\n",
      "31/406:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x, index = none, columns = ['x_coord', 'y_coord'])\n",
      "xdf.to_csv('xdf.csv', index = 'false')\n",
      "31/407:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x, columns = ['x_coord', 'y_coord'])\n",
      "xdf.to_csv('xdf.csv', index = 'false')\n",
      "31/408:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "xdf = pd.DataFrame(data = x, index = None, columns = ['x_coord', 'y_coord'])\n",
      "xdf.to_csv('xdf.csv', index = 'false')\n",
      "31/409:\n",
      "# Exercise 9, output dataframe to .csv using Pandas\n",
      "\n",
      "# first turn the array into a data frame (but I'm still getting index values)\n",
      "xdf = pd.DataFrame(data = x, index = None, columns = ['x_coord', 'y_coord'])\n",
      "xdf.to_csv('xdf.csv', index = False)\n",
      "44/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# images are still just a matrix of numbers.\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/7: m[:,:5]\n",
      "44/8: m[:5,:]\n",
      "44/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/10: m[:5,:]\n",
      "44/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/12: m[:5,:]\n",
      "44/13: m[:10,:]\n",
      "44/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/18: m[:10,:]\n",
      "44/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/24: m[90:,:]\n",
      "44/25: shape(m)\n",
      "44/26: m.shape\n",
      "44/27: plt.scatter(m[:,1],m[:,2])\n",
      "44/28:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/29:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "44/30:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "44/31:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m.info\n",
      "44/32:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m.info()\n",
      "44/33:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "info(m)\n",
      "44/34:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "np.info(m)\n",
      "44/35:\n",
      "plt.scatter(m[:,'year'],m[:,1])\n",
      "plt.show()\n",
      "44/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m.dtype.names\n",
      "44/37:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "m.dtype.names\n",
      "44/38:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/39:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/40: log_transistor = np.log2(m[:,1])\n",
      "44/41: np.log2(64)\n",
      "44/42: np.log2(62)\n",
      "44/43: m[:1]\n",
      "44/44: m[:,:1]\n",
      "44/45: m[:,:2]\n",
      "44/46: m[:,2]\n",
      "44/47: log_transistor = np.log2(m[:,2])\n",
      "44/48: m[:,3]\n",
      "44/49: m[:,1]\n",
      "44/50: m[:,0]\n",
      "44/51: m[:,2]\n",
      "44/52:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/53: m.shape\n",
      "44/54:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/55: log_transistor = np.log2(m[:,2])\n",
      "44/56: m[:,2]\n",
      "44/57: m[:,1]\n",
      "44/58:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/59:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "#print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/60:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "\n",
      "print(df.info())\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/61:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df.['year'].astype(int)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/62:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(int)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/63:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/64:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/65:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/66:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/67:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/68:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(str).astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/69: m[:,2]\n",
      "44/70:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(int)\n",
      "\n",
      "print(df.info())\n",
      "print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/71:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/72: log_transistor = np.log2(m[:,2])\n",
      "44/73: m[:,2]\n",
      "44/74:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(int)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/75:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/76: log_transistor = np.log2(m[:,2])\n",
      "44/77: m[:,2]\n",
      "44/78:\n",
      "m[:,2] = m[:,2].astype('int64')\n",
      "\n",
      "m[:,2]\n",
      "44/79:\n",
      "m[:,2] = m[:,2].astype('float64')\n",
      "\n",
      "m[:,2]\n",
      "44/80:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/81:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float64)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/82:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "44/83:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "44/84: log_transistor = np.log2(m[:,2])\n",
      "45/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "45/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "45/3:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "45/4:\n",
      "#log_transistor = np.log2(m[:,2]) \n",
      "\n",
      "log_transistor = np.log2(m[:,2].astype('float64'))\n",
      "45/5:\n",
      "#log_transistor = np.log2(m[:,2]) \n",
      "\n",
      "log_transistor = np.log2(m[:,2].astype('float64'))\n",
      "\n",
      "log_transistor\n",
      "45/6:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "m[:,2]\n",
      "45/7:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "m[:,2]\n",
      "\n",
      "joel = np.log2(m[:,2]) \n",
      "joel\n",
      "45/8:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "joel = np.log2(m[:,2]) \n",
      "joel\n",
      "\n",
      "type(m)\n",
      "45/9:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "#joel = np.log2(m[:,2]) \n",
      "#joel\n",
      "\n",
      "type(m)\n",
      "45/10:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "#joel = np.log2(m[:,2]) \n",
      "#joel\n",
      "\n",
      "type(m[:,2])\n",
      "45/11:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "#joel = np.log2(m[:,2]) \n",
      "#joel\n",
      "\n",
      "type(m[:,2][0])\n",
      "45/12:\n",
      "m[:,2] = m[:,2].astype('int')\n",
      "\n",
      "joel = np.log2(m[:,2]) \n",
      "joel\n",
      "45/13:\n",
      "m[:,2] = m[:,2].astype('float64')\n",
      "\n",
      "joel = np.log2(m[:,2]) \n",
      "joel\n",
      "45/14: log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "45/15:\n",
      "log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "\n",
      "log_transistor\n",
      "45/16:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log2(m[:,'count'].astype('float64'))\n",
      "45/17:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "45/18:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "\n",
      "plt.scatter(m[:,2],log_transistor)\n",
      "plt.show()\n",
      "46/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "46/2:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "\n",
      "# and just label the x's too, because we'll do the calcs ourselves\n",
      "\n",
      "year = m[:,2]\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.show()\n",
      "46/3: head(year)\n",
      "46/4: year.head()\n",
      "46/5: year.info()\n",
      "46/6: year\n",
      "46/7: year[:5]\n",
      "46/8:\n",
      "# need average of x*y\n",
      "\n",
      "xy = year*log_transistor\n",
      "46/9:\n",
      "# need average of x*y\n",
      "\n",
      "xy = year*log_transistor\n",
      "xy\n",
      "46/10:\n",
      "# need average of x*y\n",
      "\n",
      "xy = np.mean(year*log_transistor)\n",
      "46/11:\n",
      "# need average of x*y\n",
      "\n",
      "xy = np.mean(year*log_transistor)\n",
      "xy\n",
      "46/12:\n",
      "# need average of x*y\n",
      "\n",
      "meanxy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "\n",
      "meanx_meany = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "print('meanxy: ',meanxy)\n",
      "print('meanx_meany: ',meanx_meany)\n",
      "46/13:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "46/14:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_xsquare_d)\n",
      "46/15:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "46/16:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = (mean_xy - mean_x_mean_y)/(mean_xsquared - mean_x_squared)\n",
      "print('m: ',m)\n",
      "46/17:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = (mean_xy - mean_x_mean_y)/(mean_xsquared - mean_x_squared)\n",
      "print('m: ',m)\n",
      "\n",
      "b = (np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/(mean_xsquared - mean_x_squared)\n",
      "print('b: ',b)\n",
      "46/18:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np,round((mean_xy - mean_x_mean_y)/(mean_xsquared - mean_x_squared),4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/(mean_xsquared - mean_x_squared))\n",
      "print('b: ',b)\n",
      "46/19:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/(mean_xsquared - mean_x_squared),4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/(mean_xsquared - mean_x_squared))\n",
      "print('b: ',b)\n",
      "47/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "47/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "x = df[:,0]\n",
      "y = df[:,1]\n",
      "\n",
      "x\n",
      "47/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "x\n",
      "47/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "x.type()\n",
      "47/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "type(x)\n",
      "47/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "length(x)\n",
      "47/7:\n",
      "# plot data to see what it looks like\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "46/20:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "# print('mean_xy: ',mean_xy)\n",
      "# print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "# print('mean_xsquared: ',mean_xsquared)\n",
      "# print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "47/8:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(x))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "47/9:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(x))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "denominator\n",
      "47/10:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method: mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "denominator\n",
      "47/11:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "print(joel_mean_xsquared)\n",
      "print(mean_xsquared)\n",
      "\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "47/12:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "joel_mean_xsquared = np.square(x)\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "print(joel_mean_xsquared)\n",
      "print(mean_xsquared)\n",
      "\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "47/13:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "print(joel_mean_xsquared)\n",
      "print(mean_xsquared)\n",
      "\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "47/14:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.mean(np.dot(x,x))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "denominator\n",
      "47/15:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.mean(np.dot(x,x))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/16:\n",
      "#Now let's calculate m and b\n",
      "\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.mean(np.dot(x,x))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(np.dot(x,x))\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/17:\n",
      "#Now let's calculate m and b\n",
      "np.rows(x)\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/18:\n",
      "#Now let's calculate m and b\n",
      "x.rows()\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/19:\n",
      "#Now let's calculate m and b\n",
      "x.rows\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/20:\n",
      "#Now let's calculate m and b\n",
      "len(x)\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.sum(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/21:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print(mean_xsquared)\n",
      "print(mean_x_squared)\n",
      "print(denominator)\n",
      "47/22:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself, divided by number of rows\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "a = (np.dot(x/y)/len(x) - np.mean(x)*np.mean(y))/denominator\n",
      "47/23:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself, divided by number of rows\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "a = (np.dot(x,y)/len(x) - np.mean(x)*np.mean(y))/denominator\n",
      "47/24:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself, divided by number of rows\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "a = (np.dot(x,y)/len(x) - np.mean(x)*np.mean(y))/denominator\n",
      "b = (np.mean(y) * np.dot(x,x)/len(x) - np.mean(x)*np.dot(x,y)/len(x))/denominator\n",
      "47/25:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself, divided by number of rows\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "a = (np.dot(x,y)/len(x) - np.mean(x)*np.mean(y))/denominator\n",
      "b = (np.mean(y) * np.dot(x,x)/len(x) - np.mean(x)*np.dot(x,y)/len(x))/denominator\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "47/26:\n",
      "# now plot it all\n",
      "\n",
      "yhat = a*x + b\n",
      "46/21:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,yhat)\n",
      "plt.show()\n",
      "47/27:\n",
      "# now plot it all\n",
      "\n",
      "yhat = a*x + b\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,yhat)\n",
      "plt.show()\n",
      "46/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "46/23:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/24:\n",
      "# need average of x*y\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "# print('mean_xy: ',mean_xy)\n",
      "# print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "# print('mean_xsquared: ',mean_xsquared)\n",
      "# print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/25:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/26:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      " print('mean_xy: ',mean_xy)\n",
      " print('mean_x_mean_y: ',mean_x_mean_y)\n",
      " print('mean_xsquared: ',mean_xsquared)\n",
      " print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/27:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/28:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/29:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/30:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('dp_mean_xsquared: ',dp_mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/31:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/32:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/33:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/34:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year^2+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/35:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*np.square(year)+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "46/36:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "47/28:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "47/29:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sse\n",
      "47/30:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "47/31:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "sstotal\n",
      "47/32:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "\n",
      "rsquared = 1 - sse/sstotal\n",
      "rsquared\n",
      "47/33:\n",
      "# calculate r-squared, which is 1 - SSE/SSTotal\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/37:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "print(m.dtype.names)\n",
      "46/38:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "46/39:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log2(m[:,1].astype('float64'))\n",
      "\n",
      "# and just label the x's too, because we'll do the calcs ourselves\n",
      "\n",
      "year = m[:,2]\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.show()\n",
      "46/40:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/41:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "#dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "#dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/42:\n",
      "# now add in predictions\n",
      "year\n",
      "#yhat = m*year+b\n",
      "\n",
      "#plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "#plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/43:\n",
      "# now add in predictions\n",
      "m\n",
      "#yhat = m*year+b\n",
      "\n",
      "#plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "#plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/44:\n",
      "# now add in predictions\n",
      "b\n",
      "#yhat = m*year+b\n",
      "\n",
      "#plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "#plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/45:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "#plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "#plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/46:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "#plt.scatter(year,log_transistor)\n",
      "#plt.plot(year,yhat)\n",
      "#plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/47:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "#sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/48:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "#sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/49:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "#rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/50:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "#rsquared\n",
      "46/51:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/52:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/53:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "#print(m.dtype.names)\n",
      "46/54:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "#print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "#print(m.dtype.names)\n",
      "46/55:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "46/56:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log(m[:,1].astype('float64'))\n",
      "\n",
      "# and just label the x's too, because we'll do the calcs ourselves\n",
      "\n",
      "year = m[:,2]\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.show()\n",
      "46/57:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "#dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "#dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/58:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/59:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "#dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "#dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_x_mean_y)/denominator,4)\n",
      "print('np.mean(log_transistor)*mean_xsquared: ',np.mean(log_transistor)*mean_xsquared)\n",
      "print('np.mean(year)*mean_x_mean_y: ',np.mean(year)*mean_x_mean_y)\n",
      "print('b: ',b)\n",
      "46/60:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "#dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "#dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "#print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_xy)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/61:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/62:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "46/63:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "print('rsquared: ',rsquared'\n",
      "46/64:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "print('rsquared: ',rsquared')\n",
      "46/65:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "print('rsquared: ',rsquared)\n",
      "46/66:\n",
      "# big question is how long does it take transistor count to double?\n",
      "\n",
      "# ln(tc) = m*year + b, take exponentiate both sides\n",
      "# tc = exp(m * year) * exp(b), now for some reason multiply it by 2\n",
      "# 2*tc = 2 * exp(m*year) * exp(b), now rewrite 2\n",
      "# 2*tc = exp(ln(2)) * exp(m*year) * exp(b), now combine terms\n",
      "# 2*tc = exp(m*year + ln(2)) * exp(b)\n",
      "\n",
      "# now substitute in year2 and year1 (meaning the tc's at those years)\n",
      "# exp(b) * exp(m*year2) = exp(m*year1 + ln(2)) * exp(b), simplifies to\n",
      "# exp(m*year2) = exp(m*year1 + ln(2)), simplifies to\n",
      "# m*year2 = m*year1+ln(2), simplifies to\n",
      "# year2 = year1 + ln(2)/m\n",
      "# so, time to double is given by\n",
      "print('time to double: ', np.log(2)/m)\n",
      "46/67:\n",
      "# big question is how long does it take transistor count to double?\n",
      "\n",
      "# ln(tc) = m*year + b, take exponentiate both sides\n",
      "# tc = exp(m * year) * exp(b), now for some reason multiply it by 2\n",
      "# 2*tc = 2 * exp(m*year) * exp(b), now rewrite 2\n",
      "# 2*tc = exp(ln(2)) * exp(m*year) * exp(b), now combine terms\n",
      "# 2*tc = exp(m*year + ln(2)) * exp(b)\n",
      "\n",
      "# now substitute in year2 and year1 (meaning the tc's at those years)\n",
      "# exp(b) * exp(m*year2) = exp(m*year1 + ln(2)) * exp(b), simplifies to\n",
      "# exp(m*year2) = exp(m*year1 + ln(2)), simplifies to\n",
      "# m*year2 = m*year1+ln(2), simplifies to\n",
      "# year2 = year1 + ln(2)/m\n",
      "# so, time to double is given by\n",
      "print('time to double: ', np.round(np.log(2)/m,2), ' years')\n",
      "46/68:\n",
      "# big question is how long does it take transistor count to double?\n",
      "\n",
      "# ln(tc) = m*year + b, take exponentiate both sides\n",
      "# tc = exp(m * year) * exp(b), now for some reason multiply it by 2\n",
      "# 2*tc = 2 * exp(m*year) * exp(b), now rewrite 2\n",
      "# 2*tc = exp(ln(2)) * exp(m*year) * exp(b), now combine terms\n",
      "# 2*tc = exp(m*year + ln(2)) * exp(b)\n",
      "\n",
      "# now substitute in year2 and year1 (meaning the tc's at those years)\n",
      "# exp(b) * exp(m*year2) = exp(m*year1 + ln(2)) * exp(b), simplifies to\n",
      "# exp(m*year2) = exp(m*year1 + ln(2)), simplifies to\n",
      "# m*year2 = m*year1+ln(2), simplifies to\n",
      "# year2 = year1 + ln(2)/m\n",
      "# so, time to double is given by\n",
      "print('time to double: ', np.round(np.log(2)/m,2), 'years')\n",
      "46/69:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'moore.csv', header = None)\n",
      "df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "df['year'].astype(float)\n",
      "\n",
      "#print(df.info())\n",
      "#print(df.head())\n",
      "\n",
      "#print('')\n",
      "\n",
      "# Moore's law says that \n",
      "# tn = t0*2^n - that's exponential but if we take logs all of a sudden we have a linear equation\n",
      "# log(tn) = log(t0) + n*(log(2))\n",
      "\n",
      "# transistor count on an integrated circuit doubles every two years\n",
      "\n",
      "# transistor counts are in second column, year is in third\n",
      "\n",
      "rows = len(df.index)\n",
      "m = df.values\n",
      "\n",
      "#print(m.dtype.names)\n",
      "46/70:\n",
      "plt.scatter(m[:,2],m[:,1])\n",
      "plt.show()\n",
      "46/71:\n",
      "# take the log of the transitor just to verify\n",
      "log_transistor = np.log(m[:,1].astype('float64'))\n",
      "\n",
      "# and just label the x's too, because we'll do the calcs ourselves\n",
      "\n",
      "year = m[:,2]\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.show()\n",
      "46/72:\n",
      "# need average of x*y (could also just do the dot product divided by the number of rows)\n",
      "mean_xy = np.mean(year*log_transistor)\n",
      "#dp_mean_xy = np.dot(year,log_transistor)/len(year)\n",
      "\n",
      "# need average of x * average of y\n",
      "mean_x_mean_y = np.mean(year) * np.mean(log_transistor)\n",
      "\n",
      "# need average of x^2\n",
      "mean_xsquared = np.mean(np.square(year))\n",
      "#dp_mean_xsquared = np.dot(year,year)/len(year)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(year))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "print('mean_xy: ',mean_xy)\n",
      "#print('dp_mean_xy: ',dp_mean_xy)\n",
      "print('mean_x_mean_y: ',mean_x_mean_y)\n",
      "print('mean_xsquared: ',mean_xsquared)\n",
      "print('mean_x_squared: ',mean_x_squared)\n",
      "print('denominator: ', denominator)\n",
      "\n",
      "m = np.round((mean_xy - mean_x_mean_y)/denominator,4)\n",
      "print('m: ',m)\n",
      "\n",
      "b = np.round((np.mean(log_transistor)*mean_xsquared - np.mean(year)*mean_xy)/denominator,4)\n",
      "print('b: ',b)\n",
      "46/73:\n",
      "# now add in predictions\n",
      "\n",
      "yhat = m*year+b\n",
      "\n",
      "plt.scatter(year,log_transistor)\n",
      "plt.plot(year,yhat)\n",
      "plt.show()\n",
      "\n",
      "sse = np.sum(np.square(log_transistor-yhat))\n",
      "sstotal = np.sum(np.square(log_transistor-np.mean(log_transistor)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "print('rsquared: ',rsquared)\n",
      "46/74:\n",
      "# big question is how long does it take transistor count to double?\n",
      "\n",
      "# ln(tc) = m*year + b, take exponentiate both sides\n",
      "# tc = exp(m * year) * exp(b), now for some reason multiply it by 2\n",
      "# 2*tc = 2 * exp(m*year) * exp(b), now rewrite 2\n",
      "# 2*tc = exp(ln(2)) * exp(m*year) * exp(b), now combine terms\n",
      "# 2*tc = exp(m*year + ln(2)) * exp(b)\n",
      "\n",
      "# now substitute in year2 and year1 (meaning the tc's at those years?)\n",
      "# exp(b) * exp(m*year2) = exp(m*year1 + ln(2)) * exp(b), simplifies to\n",
      "# exp(m*year2) = exp(m*year1 + ln(2)), simplifies to\n",
      "# m*year2 = m*year1+ln(2), simplifies to\n",
      "# year2 = year1 + ln(2)/m\n",
      "# so, time to double is given by\n",
      "print('time to double: ', np.round(np.log(2)/m,2), 'years')\n",
      "\n",
      "print('time to increase ten-fold: ', np.round(np.log(10)/m,2), 'years')\n",
      "49/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "49/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,[0:1]]\n",
      "y = m[:,2]\n",
      "49/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:1]\n",
      "y = m[:,2]\n",
      "49/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "x\n",
      "49/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "x\n",
      "49/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "x\n",
      "y\n",
      "49/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      " print(df.info[:,:5])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "print(df.info[:,:5])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "print(df.info[:,0])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "print(df[:,0])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/14:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "49/15:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show()\n",
      "49/16:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "49/17:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "print(np.corrcoef(m[:,0],m[:,1]))\n",
      "49/18:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "print(np.correlate(m[:,0],m[:,1]))\n",
      "49/19:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "print(np.correlate(m[:,0],m[:,1])[0,1])\n",
      "49/20:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.correlate(m[:,0],m[:,1])\n",
      "print(x1x2correlation[0,1])\n",
      "49/21:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.correlate(m[:,0],m[:,1])\n",
      "print(x1x2correlation)\n",
      "49/22:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])\n",
      "print(x1x2correlation)\n",
      "49/23:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print(x1x2correlation)\n",
      "49/24:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print('x1x2correlation: ',x1x2correlation)\n",
      "49/25:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',x1ycorrelation)\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show()\n",
      "49/26:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.round(np.corrcoef(m[:,0],m[:,2])[0,1],4)\n",
      "print('x1ycorrelation: ',x1ycorrelation)\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.round(np.corrcoef(m[:1],m[:,2])[0,1],4)\n",
      "print('x2ycorrelation: ',x2ycorrelation)\n",
      "49/27:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "#x1ycorrelation = np.round(np.corrcoef(m[:,0],m[:,2])[0,1],4)\n",
      "#print('x1ycorrelation: ',x1ycorrelation)\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.round(np.corrcoef(m[:1],m[:,2])[0,1],4)\n",
      "print('x2ycorrelation: ',x2ycorrelation)\n",
      "49/28:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',x1ycorrelation)\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.corrcoef(m[:1],m[:,2])[0,1]\n",
      "print('x2ycorrelation: ',x2ycorrelation)\n",
      "49/29:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print('x1x2correlation: ',x1x2correlation)\n",
      "49/30:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print('x1x2correlation: ',np.round(x1x2correlation,4))\n",
      "49/31:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',np.round(x1ycorrelation,4))\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.corrcoef(m[:,1],m[:,2])[0,1]\n",
      "print('x2ycorrelation: ',np.round(x2ycorrelation,4))\n",
      "49/32:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',np.round(x1ycorrelation,4))\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.corrcoef(m[:,1],m[:,2])[0,1]\n",
      "print('x2ycorrelation: ',np.round(x2ycorrelation,4))\n",
      "49/33:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print('x1x2correlation: ',np.round(x1x2correlation,4))\n",
      "49/34:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "49/35: w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "49/36:\n",
      "w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "w\n",
      "51/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "joel = np.ones[100]\n",
      "joel\n",
      "51/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "joel = np.ones[100]\n",
      "joel\n",
      "\n",
      "len(x)\n",
      "51/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "#joel = np.ones[100]\n",
      "#joel\n",
      "\n",
      "len(x)\n",
      "51/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "x = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "joel = np.ones(len(x))\n",
      "joel\n",
      "51/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "x = np.apppend(ones,xdata,axis = 1)\n",
      "x\n",
      "51/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "x\n",
      "51/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "x = np.append(ones,xdata,axis = 0)\n",
      "x\n",
      "51/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "x\n",
      "51/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "x = np.append(ones,xdata)\n",
      "x\n",
      "51/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "#x = np.append(ones,xdata)\n",
      "ones.info()\n",
      "51/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "#print(df[:,:])\n",
      "\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "#x = np.append(ones,xdata)\n",
      "info(ones)\n",
      "51/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "#x = np.append(ones,xdata)\n",
      "ones.size\n",
      "51/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(x))\n",
      "#x = np.append(ones,xdata)\n",
      "print(len(x))\n",
      "51/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "#x = np.append(ones,xdata)\n",
      "print(len(y))\n",
      "51/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "x\n",
      "51/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata, axis = 0)\n",
      "x\n",
      "51/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata, axis = 1)\n",
      "x\n",
      "51/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "print(x[:5,:])\n",
      "51/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "print(x[:,:5])\n",
      "51/20:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',np.round(x1ycorrelation,4))\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.corrcoef(m[:,1],m[:,2])[0,1]\n",
      "print('x2ycorrelation: ',np.round(x2ycorrelation,4))\n",
      "51/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "51/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "51/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y))\n",
      "x = np.append(ones,xdata)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones(len(y),1)\n",
      "x = np.append(ones,xdata)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/25:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/26:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 0)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/27:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "#print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/28:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(x[:5,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/29:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(x[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/30:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/31:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/32:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/33:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:5])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "51/34:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "#print(x[:1,:5])\n",
      "51/35:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "print(x[:1,:5])\n",
      "51/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "ones = np.ones((len(y),1))\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(xdata[:1,:])\n",
      "print(x.shape)\n",
      "print(xdata.shape)\n",
      "print(ones.shape)\n",
      "print(x[:2,:])\n",
      "51/37:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "# need to add a column of ones at the beginning for an intercept\n",
      "ones = np.ones((len(y),1)) # have to add the 1 argument so xdata can be appended to it.\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(x.shape)\n",
      "print(x[:2,:])\n",
      "51/38:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_2d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "# print(df.info())\n",
      "\n",
      "# x is not just a one-dimensional array. It is an n-dimensional vector which we'll call the feature vector\n",
      "# in this example, \n",
      "\n",
      "m = df.values\n",
      "#print(m[:5,:])\n",
      "xdata = m[:,0:2]\n",
      "y = m[:,2]\n",
      "\n",
      "# need to add a column of ones at the beginning for an intercept\n",
      "ones = np.ones((len(y),1)) # have to add the 1 argument so xdata can be appended to it.\n",
      "x = np.append(ones,xdata,axis = 1)\n",
      "print(x.shape)\n",
      "print(x[:2,:])\n",
      "51/39:\n",
      "# let's just take a look at plots of each individual feature variable\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "x1ycorrelation = np.corrcoef(m[:,0],m[:,2])[0,1]\n",
      "print('x1ycorrelation: ',np.round(x1ycorrelation,4))\n",
      "\n",
      "plt.scatter(m[:,1],m[:,2])\n",
      "plt.show\n",
      "x2ycorrelation = np.corrcoef(m[:,1],m[:,2])[0,1]\n",
      "print('x2ycorrelation: ',np.round(x2ycorrelation,4))\n",
      "51/40:\n",
      "# just curious, does there appear to be a relationship between the two feature variables?\n",
      "\n",
      "plt.scatter(m[:,0],m[:,1])\n",
      "plt.show()\n",
      "\n",
      "# not really, they look independent. Could calculate correlation or an r-squared to be more certain\n",
      "\n",
      "x1x2correlation = np.corrcoef(m[:,0],m[:,1])[0,1]\n",
      "print('x1x2correlation: ',np.round(x1x2correlation,4))\n",
      "51/41:\n",
      "w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "w\n",
      "51/42:\n",
      "# 3-d plot just for the heck of it\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111,projection = '3d')\n",
      "ax.scatter(x[:,1],x[:,2],y)\n",
      "plt.show()\n",
      "51/43:\n",
      "# 3-d plot just for the heck of it\n",
      "\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111,projection = '3d')\n",
      "ax.scatter(x[:,1],x[:,2],y)\n",
      "plt.show()\n",
      "51/44:\n",
      "# 3-d plot just for the heck of it\n",
      "\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111,projection = '3d')\n",
      "ax.scatter(x[:,0],x[:,1],y)\n",
      "plt.show()\n",
      "51/45:\n",
      "# 3-d plot just for the heck of it\n",
      "\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "fig = plt.figure()\n",
      "ax = fig.add_subplot(111,projection = '3d')\n",
      "ax.scatter(x[:,1],x[:,2],y) # But I would think that x[:0] is just ones?\n",
      "plt.show()\n",
      "51/46:\n",
      "# remember that an * just does element by element multiplication\n",
      "# np.dot does actual matrix multiplication\n",
      "\n",
      "#w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "#or\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "w\n",
      "51/47:\n",
      "# remember that an * just does element by element multiplication\n",
      "# np.dot does actual matrix multiplication\n",
      "\n",
      "#w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "#or\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "print('w: ', w)\n",
      "51/48:\n",
      "# remember that an * just does element by element multiplication\n",
      "# np.dot does actual matrix multiplication\n",
      "\n",
      "#w = np.linalg.solve(np.dot(x.transpose(),x),np.dot(x.transpose(),y))\n",
      "#or\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "print('w: ', w)\n",
      "\n",
      "yhat = np.dot(x,w)\n",
      "52/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "52/2:\n",
      "# plot data to see what it looks like\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "51/49:\n",
      "yhat = np.dot(x,w)\n",
      "\n",
      "sse = np.sum(np.square(y-yhat))\n",
      "sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "51/50:\n",
      "yhat = np.dot(x,w)\n",
      "\n",
      "#sse = np.sum(np.square(y-yhat))\n",
      "#sstotal = np.sum(np.square(y-np.mean(y)))\n",
      "\n",
      "# or do it with dots\n",
      "\n",
      "sse = np.dot(y-yhat,y-yhat)\n",
      "sstotal = np.dot(y-np.mean(y),(y-np.mean(y)))\n",
      "\n",
      "rsquared = np.round(1 - sse/sstotal,4)\n",
      "rsquared\n",
      "51/51:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn.linear_model import Linear_Regression\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "x = xdata\n",
      "51/52:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import Linear_Regression\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "x = xdata\n",
      "51/53:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "x = xdata\n",
      "51/54:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "x = xdata\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "51/55:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "regr.fit(xdata,y)\n",
      "51/56:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_)\n",
      "# The mean squared error\n",
      "print('Mean squared error: ' mean_squared_error(y, yhat))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "print('Coefficient of determination: ' r2_score(y, yhat))\n",
      "51/57:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_)\n",
      "# The mean squared error\n",
      "print('Mean squared error: ', mean_squared_error(y, yhat))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "print('Coefficient of determination: ', r2_score(y, yhat))\n",
      "51/58:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_)\n",
      "# The mean squared error\n",
      "#print('Mean squared error: ', mean_squared_error(y, yhat))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "#print('Coefficient of determination: ', r2_score(y, yhat))\n",
      "51/59:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_) # matches what I have above but they're not showing the intercept?\n",
      "# The mean squared error\n",
      "print('Mean squared error: ', mean_squared_error(y, yhat))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "print('Coefficient of determination: ', r2_score(y, yhat))\n",
      "51/60:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_) # matches what I have above but they're not showing the intercept?\n",
      "# The mean squared error\n",
      "print('Mean squared error: ', mean_squared_error(y, yhat))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "print('Coefficient of determination: ', np.round(r2_score(y, yhat),4))\n",
      "51/61:\n",
      "# Can we replicate in scikit?\n",
      "\n",
      "from sklearn import linear_model as lr\n",
      "from sklearn.metrics import mean_squared_error, r2_score\n",
      "\n",
      "# use xdata, I shouldn't need to include the intercept vector\n",
      "\n",
      "regr = lr.LinearRegression()\n",
      "# train the model (don't worry about test/train split)\n",
      "regr.fit(xdata,y)\n",
      "# now make predictions\n",
      "yhat = regr.predict(xdata)\n",
      "\n",
      "# The coefficients\n",
      "print('Coefficients: ', regr.coef_) # matches what I have above but they're not showing the intercept?\n",
      "# The mean squared error\n",
      "print('Mean squared error: ', np.round(mean_squared_error(y, yhat),2))\n",
      "# The coefficient of determination: 1 is perfect prediction\n",
      "print('Coefficient of determination: ', np.round(r2_score(y, yhat),4))\n",
      "53/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'poly.csv', header = None)\n",
      "df.info()\n",
      "53/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "53/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "df[:1,:1]\n",
      "53/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "df([:1,:1])\n",
      "53/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values()\n",
      "m[:5,:]\n",
      "53/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "m[:5,:]\n",
      "53/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "53/8:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(x)\n",
      "53/9:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(x)\n",
      "xsquared.shape()\n",
      "53/10:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(x)\n",
      "xsquared.shape\n",
      "53/11:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(x)\n",
      "xsquared.shape\n",
      "x.shape\n",
      "53/12:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "x = np.append(xsquared,xdata,axis = 1)\n",
      "53/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "53/14:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "x = np.append(xsquared,xdata,axis = 1)\n",
      "53/15:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 1)\n",
      "print(xsquared.shape)\n",
      "\n",
      "#x = np.append(xsquared,xdata,axis = 1)\n",
      "53/16:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "\n",
      "#x = np.append(xsquared,xdata,axis = 1)\n",
      "53/17:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "np.expand_dims(xdata,axis = 0)\n",
      "print(xdata.shape)\n",
      "\n",
      "#x = np.append(xsquared,xdata,axis = 1)\n",
      "53/18:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "np.expand_dims(xdata,axis = 0)\n",
      "print(xdata.shape)\n",
      "\n",
      "x = np.append(xsquared,xdata,axis = 1)\n",
      "53/19:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "\n",
      "xdata.reshape(-1,1).shape\n",
      "print(xdata.shape)\n",
      "\n",
      "#x = np.append(xsquared,xdata,axis = 1)\n",
      "53/20:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "\n",
      "xdata.reshape(-1,1).shape\n",
      "print(xdata.shape)\n",
      "\n",
      "x = np.column_stack(xsquared,xdata)\n",
      "53/21:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "\n",
      "xdata.reshape(-1,1).shape\n",
      "print(xdata.shape)\n",
      "\n",
      "x = np.column_stack((xsquared,xdata))\n",
      "53/22:\n",
      "# let's add a column\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "print(xsquared.shape)\n",
      "print(xdata.shape)\n",
      "\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "print(xsquared.shape)\n",
      "\n",
      "xdata.reshape(-1,1).shape\n",
      "print(xdata.shape)\n",
      "\n",
      "x = np.column_stack((xsquared,xdata))\n",
      "print(x.shape)\n",
      "53/23:\n",
      "# let's add a column, and the intercept\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "xones = np.ones(length(y))\n",
      "\n",
      "# tried to add dimension then append, didn't work\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "xdata.reshape(-1,1).shape\n",
      "\n",
      "# so I went with this instead, which worked\n",
      "x = np.column_stack((xones,xsquared,xdata))\n",
      "x.shape\n",
      "53/24:\n",
      "# let's add a column, and the intercept\n",
      "\n",
      "xsquared = np.square(xdata)\n",
      "xones = np.ones(len(y))\n",
      "\n",
      "# tried to add dimension then append, didn't work\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "xdata.reshape(-1,1).shape\n",
      "\n",
      "# so I went with this instead, which worked\n",
      "x = np.column_stack((xones,xsquared,xdata))\n",
      "x.shape\n",
      "53/25:\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "print('w: ', w)\n",
      "53/26: yhat = np.dot(w,x)\n",
      "53/27: yhat = np.dot(x,w)\n",
      "53/28:\n",
      "yhat = np.dot(x,w)\n",
      "\n",
      "sse = np.dot(y-yhat,y-yhat)\n",
      "sstotal = np.dot(y-np.mean(y),y-np.mean(y))\n",
      "\n",
      "rsquared = 1 - sse/sstotal\n",
      "rsquared\n",
      "53/29:\n",
      "plt.scatter(x,y)\n",
      "plt.show(x,yhat)\n",
      "plt.show()\n",
      "53/30:\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,yhat)\n",
      "plt.show()\n",
      "53/31:\n",
      "plt.scatter(xdata,y)\n",
      "plt.plot(xdata,yhat)\n",
      "plt.show()\n",
      "53/32:\n",
      "plt.scatter(xdata,y)\n",
      "#plt.plot(xdata,yhat)\n",
      "plt.show()\n",
      "53/33:\n",
      "plt.scatter(xdata,y)\n",
      "plt.plot(xdata,yhat)\n",
      "plt.show()\n",
      "53/34:\n",
      "plt.scatter(xdata,y)\n",
      "#plt.plot(xdata,yhat)\n",
      "# why isn't yhat plotting correctly?\n",
      "plt.show()\n",
      "53/35:\n",
      "plt.scatter(xdata,y)\n",
      "#plt.plot(xdata,yhat)\n",
      "# why isn't yhat plotting correctly? If x isn't sorted, sometimes it gets plotted as a series of points.\n",
      "# we can sort both xdata and yhat individually because a quadratic function is monotonically increasing\n",
      "plt.plot(sorted(xdata),sorted(yhat))\n",
      "plt.show()\n",
      "54/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "#plt.scatter(x,y)\n",
      "#plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "df.info()\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "#plt.scatter(m[:,0],y)\n",
      "#plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "53/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0]\n",
      "y = m[:,1]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "#plt.scatter(m[:,0],y)\n",
      "#plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "54/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "df.astype('float64').dtypes\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "54/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "df.astype('float').dtypes\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "54/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = None)\n",
      "print(df)\n",
      "#df.astype('float').dtypes\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "54/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "print(df)\n",
      "#df.astype('float').dtypes\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "54/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,0],m[:,2])\n",
      "plt.show()\n",
      "54/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,1],m[:,0])\n",
      "plt.show()\n",
      "54/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,1],m[:,'x1'])\n",
      "plt.show()\n",
      "54/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,0:1]\n",
      "y = m[:,2]\n",
      "\n",
      "plt.scatter(m[:,1],y)\n",
      "plt.show()\n",
      "54/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "54/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "54/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:3]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "54/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "54/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,1],xdata[:,1])\n",
      "plt.show()\n",
      "54/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "54/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print(np.corrcoef(xdata[:,0],y))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "54/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print(np.corrcoef(xdata[:,0],y)[0,1])\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "54/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print(np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "54/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print('y vs x1 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "print('x1 vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "54/25:\n",
      "# add a column of ones\n",
      "\n",
      "xdata.shape\n",
      "54/26:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones(length(y),1)\n",
      "\n",
      "xones.shape\n",
      "54/27:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones(len(y),1)\n",
      "\n",
      "xones.shape\n",
      "54/28:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones((len(y),1))\n",
      "\n",
      "xones.shape\n",
      "54/29:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones((len(y),1))\n",
      "\n",
      "x = np.append(xones,xdata)\n",
      "x.shape\n",
      "54/30:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones((len(y),1))\n",
      "\n",
      "x = np.append(xones,xdata,axis = 0)\n",
      "x.shape\n",
      "54/31:\n",
      "# add a column of ones\n",
      "\n",
      "xones = np.ones((len(y),1))\n",
      "\n",
      "x = np.append(xones,xdata,axis = 1)\n",
      "x.shape\n",
      "54/32:\n",
      "# now let's run the multinomial regression\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "print('w: ', w)\n",
      "54/33:\n",
      "yhat = np.dot(x,w)\n",
      "\n",
      "sse = np.dot(y-yhat,y-yhat)\n",
      "sstotal = np.dot(y-np.mean(y),y-np.mean(y))\n",
      "\n",
      "rsquared = 1 - sse/sstotal\n",
      "rsquared\n",
      "54/34:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "x\n",
      "54/35: print(df)\n",
      "54/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "m = df.as_matrix()\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print('y vs x1 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "print('x1 vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "54/37:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "m = df.as_matrix\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print('y vs x1 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "print('x1 vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "54/38:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'mlr02.xls', header = 0)\n",
      "#print(df.info())\n",
      "\n",
      "m = df.values\n",
      "mm = df.to_records(index=False)\n",
      "print(mm)\n",
      "\n",
      "xdata = m[:,1:]\n",
      "y = m[:,0]\n",
      "\n",
      "# plot y against x1\n",
      "plt.scatter(xdata[:,0],y)\n",
      "plt.show()\n",
      "print('y vs x1 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x2\n",
      "plt.scatter(xdata[:,1],y)\n",
      "plt.show()\n",
      "print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x1 against x2\n",
      "plt.scatter(xdata[:,0],xdata[:,1])\n",
      "plt.show()\n",
      "print('x1 vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "54/39:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,2]]\n",
      "54/40:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,2]]\n",
      "xjust2\n",
      "54/41:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "print(xjust2[:,:5])\n",
      "54/42:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "print(xjust2[:5,:])\n",
      "54/43:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "print(xjust2[:5,:])\n",
      "print(xjust3[:5,:])\n",
      "54/44:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(),Y-np.mean)\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared for ',X,': ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "54/45:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(),Y-np.mean())\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared for ',X,': ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "54/46:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(),Y-np.mean())\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared for ',X,': ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "54/47:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(Y),Y-np.mean(Y))\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared for ',X,': ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "54/48:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(Y),Y-np.mean(Y))\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared for: ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "54/49:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(Y),Y-np.mean(Y))\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared: ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "\n",
      "regress(xjust2)\n",
      "54/50:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(Y),Y-np.mean(Y))\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared: ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "\n",
      "regress(xjust2,y)\n",
      "54/51:\n",
      "# Because there is so much correlation between the two variables, let's try just doing it with one at a time.\n",
      "\n",
      "xjust2 = x[:,[0,1]]\n",
      "xjust3 = x[:,[0,2]]\n",
      "\n",
      "def regress(X,Y):\n",
      "    w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "    yhat = np.dot(X,w)\n",
      "    sse = np.dot(Y-yhat,Y-yhat)\n",
      "    sstotal = np.dot(Y-np.mean(Y),Y-np.mean(Y))\n",
      "    rsquared = np.round(1 - sse/sstotal,4)\n",
      "    print('rsquared: ',rsquared)\n",
      "    \n",
      "regress(x,y)\n",
      "\n",
      "regress(xjust2,y)\n",
      "\n",
      "regress(xjust3,y)\n",
      "55/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_excel(path + 'overfitting.csv', header = 0)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "# mm = df.to_records(index=False)\n",
      "\n",
      "# xdata = m[:,1:]\n",
      "# y = m[:,0]\n",
      "\n",
      "# plot y against x2\n",
      "# plt.scatter(xdata[:,0],y)\n",
      "# plt.show()\n",
      "# print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x3\n",
      "# plt.scatter(xdata[:,1],y)\n",
      "# plt.show()\n",
      "# print('y vs x3 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x2 against x3\n",
      "# plt.scatter(xdata[:,0],xdata[:,1])\n",
      "# plt.show()\n",
      "# print('x2 vs x3 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "55/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'overfitting.csv', header = 0)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "# mm = df.to_records(index=False)\n",
      "\n",
      "# xdata = m[:,1:]\n",
      "# y = m[:,0]\n",
      "\n",
      "# plot y against x2\n",
      "# plt.scatter(xdata[:,0],y)\n",
      "# plt.show()\n",
      "# print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x3\n",
      "# plt.scatter(xdata[:,1],y)\n",
      "# plt.show()\n",
      "# print('y vs x3 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x2 against x3\n",
      "# plt.scatter(xdata[:,0],xdata[:,1])\n",
      "# plt.show()\n",
      "# print('x2 vs x3 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "55/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "# mm = df.to_records(index=False)\n",
      "\n",
      "# xdata = m[:,1:]\n",
      "# y = m[:,0]\n",
      "\n",
      "# plot y against x2\n",
      "# plt.scatter(xdata[:,0],y)\n",
      "# plt.show()\n",
      "# print('y vs x2 correlation: ',np.round(np.corrcoef(xdata[:,0],y)[0,1],4))\n",
      "\n",
      "# plot y against x3\n",
      "# plt.scatter(xdata[:,1],y)\n",
      "# plt.show()\n",
      "# print('y vs x3 correlation: ',np.round(np.corrcoef(xdata[:,1],y)[0,1],4))\n",
      "\n",
      "# also plot x2 against x3\n",
      "# plt.scatter(xdata[:,0],xdata[:,1])\n",
      "# plt.show()\n",
      "# print('x2 vs x3 correlation: ',np.round(np.corrcoef(xdata[:,0],xdata[:,1])[0,1],4))\n",
      "55/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[:0]\n",
      "Y = df[:1]\n",
      "\n",
      "m = df.values\n",
      "55/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[:0]\n",
      "Y = df[:1]\n",
      "\n",
      "m = df.values\n",
      "\n",
      "X\n",
      "55/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[:0,:]\n",
      "Y = df[:1,:]\n",
      "\n",
      "m = df.values\n",
      "\n",
      "X\n",
      "55/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "\n",
      "X\n",
      "55/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "\n",
      "Y\n",
      "55/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "55/10:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(x))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "makedata(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/11:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(x))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "55/13:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(x))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/14:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(x))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "print(X)\n",
      "\n",
      "# make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/15:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(x))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/16:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in xrange(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/17:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in degree:\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/18:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append[X==(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/19:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append[X**(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/20:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "#    xdata=[np.ones(len(X))]\n",
      "#    for d in range(degree):\n",
      "#        xdata.append[X**(d+1)]\n",
      "#    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/21:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "#    for d in range(degree):\n",
      "#        xdata.append[X**(d+1)]\n",
      "#    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/22:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append[X**(d+1)]\n",
      "#    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/23:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "#    for d in range(degree):\n",
      "#        xdata.append[X**(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/24:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append[X**(d+1)]\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/25:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/26:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print(xdata.shape)\n",
      "    return xdata\n",
      "\n",
      "make_data(X,4)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/27:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,0)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/28:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,1)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/29:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/30:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,10)\n",
      "\n",
      "# create and run regression models using a train/test split\n",
      "\n",
      "# function to run them all at once\n",
      "55/31:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "#make_data(X,1)\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    Xtest = X[test_idx]\n",
      "    Ytest = Y[test_idx]\n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/32:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/33:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "        yhat_train = np.dot(xtraindata,ytrain)\n",
      "        yhat_test = np.dot(xtestdata,ytest)\n",
      "    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/34:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        \n",
      "    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/35:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        \n",
      "    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/36:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/37:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(x)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/38:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random_choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/39:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,.8)\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/40:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,np.round(.8*n,0))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/41:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = makedata(xtrain,d)\n",
      "        xtestdata = makedata(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/42:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/43:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "55/44:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/45:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(1:deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/46:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/47:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        xtraindata = xtraindata.values\n",
      "        xtestdata = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "55/48:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "56/2:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/3:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(1,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/4:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/5:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/6:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/7:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        return(xtraindata.info)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/8:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        return(xtraindata.info())\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/9:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        return(info(xtraindata))\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/10:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/11:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=(np.ones(len(X)))\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/12:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=(np.ones(len(X)))\n",
      "    for d in range(degree):\n",
      "        xdata.concatenate(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/13:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X(train_idx)\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X(test_idx)\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/14:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/15:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = np.array(make_data(xtrain,d))\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/16:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = np.array(make_data(xtrain,d))\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/17:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(2,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/18:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(1,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/19:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/20:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/21:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/22:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/23:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print(xdata)\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/24:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print(len(xdata))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/25:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print('length: ',len(xdata))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/26:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print('length: ',size(xdata))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/27:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print('length: ',type(xdata))\n",
      "    return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/28:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print('length: ',type(xdata))\n",
      "    np.savetxt(\"joel.csv\", xdata, delimiter=\",\", fmt='%s', header=header)\n",
      "    #return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/29:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    print('length: ',type(xdata))\n",
      "    np.savetxt(\"joel.csv\", xdata, delimiter=\",\", fmt='%s')\n",
      "    #return xdata\n",
      "\n",
      "make_data(X,2)\n",
      "56/30:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/31:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/32:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/33:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        xtrain_data = xtraindata.values\n",
      "        xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtrain_data.T,xtrain_data),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/34:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        #xtrain_data = xtraindata.values\n",
      "        #xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtrain_data,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtest_data,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/35:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        #xtrain_data = xtraindata.values\n",
      "        #xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/36:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        #print(xtraindata.info())\n",
      "        #return(xtraindata)\n",
      "        #xtrain_data = xtraindata.values\n",
      "        #xtest_data = xtestdata.values\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('d, r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,20)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/37:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        \n",
      "run_model(X,Y,10)    \n",
      "\n",
      "\n",
      "# function to run them all at once\n",
      "56/38:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "56/39:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,10)\n",
      "56/40:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,3)\n",
      "56/41:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/42:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return xdata.T\n",
      "\n",
      "make_data(X,2)\n",
      "56/43:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/44:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    return np.vstack(xdata)\n",
      "\n",
      "make_data(X,2)\n",
      "56/45:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    np.savetxt(\"noT.csv\", data1, delimiter=\",\", fmt='%s')\n",
      "    return np.vstack(xdata)\n",
      "\n",
      "make_data(X,2)\n",
      "56/46:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    joel = np.vstack(xdata)\n",
      "    np.savetxt(\"noT.csv\", joel, delimiter=\",\", fmt='%s')\n",
      "    return np.vstack(xdata)\n",
      "\n",
      "make_data(X,2)\n",
      "56/47:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    joel = np.vstack(xdata).T\n",
      "    np.savetxt(\"T.csv\", joel, delimiter=\",\", fmt='%s')\n",
      "    return np.vstack(xdata).T\n",
      "\n",
      "make_data(X,2)\n",
      "56/48:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it?\n",
      "    joel = np.vstack(xdata).T\n",
      "    np.savetxt(\"T.csv\", joel, delimiter=\",\", fmt='%s')\n",
      "    return np.vstack(xdata).T\n",
      "59/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# predict blood pressure as function of age and weight\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# x1 = systolic blood pressure\n",
      "# x2 = age in years\n",
      "# x3 = weight in pounds\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "59/2:\n",
      "# let's add a column, and the intercept\n",
      "xsquared = np.square(xdata)\n",
      "xones = np.ones(len(y))\n",
      "\n",
      "# tried to add dimension then append, didn't work\n",
      "np.expand_dims(xsquared,axis = 0)\n",
      "xdata.reshape(-1,1).shape\n",
      "\n",
      "# so I went with this instead, which worked\n",
      "x = np.column_stack((xones,xsquared,xdata))\n",
      "x.shape\n",
      "59/3:\n",
      "# now let's run the multinomial regression\n",
      "w = np.linalg.solve(np.dot(x.T,x),np.dot(x.T,y))\n",
      "print('w: ', w)\n",
      "59/4:\n",
      "yhat = np.dot(x,w)\n",
      "\n",
      "sse = np.dot(y-yhat,y-yhat)\n",
      "sstotal = np.dot(y-np.mean(y),y-np.mean(y))\n",
      "\n",
      "rsquared = 1 - sse/sstotal\n",
      "rsquared\n",
      "59/5:\n",
      "plt.scatter(xdata,y)\n",
      "#plt.plot(xdata,yhat)\n",
      "# why isn't yhat plotting correctly? If x isn't sorted, sometimes it gets plotted as a series of points.\n",
      "# we can sort both xdata and yhat individually because a quadratic function is monotonically increasing\n",
      "plt.plot(sorted(xdata),sorted(yhat))\n",
      "plt.show()\n",
      "59/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# predict blood pressure as function of age and weight\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_poly.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# x1 = systolic blood pressure\n",
      "# x2 = age in years\n",
      "# x3 = weight in pounds\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "plt.scatter(xdata,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "56/49:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        plt.scatter(xtestdata,ytest)\n",
      "        plt.plot(xtestdata,yhat_test)\n",
      "        plot.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/50:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        plt.scatter(xtestdata,ytest)\n",
      "        plt.plot(xtestdata,yhat_test)\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/51:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtestdata,ytest)\n",
      "        #plt.plot(xtestdata,yhat_test)\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/52:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        plt.scatter(xtestdata,ytest)\n",
      "        #plt.plot(xtestdata,yhat_test)\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/53:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        print('xtestdata: ', length(xtestdata), 'yhat_test: length(yhat_test)')\n",
      "        plt.scatter(xtestdata,ytest)\n",
      "        plt.plot(xtestdata,yhat_test)\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/54:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        print('xtestdata: ', len(xtestdata), 'yhat_test: ',len(yhat_test))\n",
      "        plt.scatter(xtestdata,ytest)\n",
      "        plt.plot(xtestdata,yhat_test)\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/55:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        print('xtestdata: ', len(xtestdata), 'yhat_test: ',len(yhat_test))\n",
      "        plt.scatter(xtest,ytest)\n",
      "        plt.plot(xtest,yhat_test)\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/56:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        plt.scatter(xtest,ytest)\n",
      "        plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/57:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print(d, 'r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/58:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree ',d, '- r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/59:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train: ', np.round(r_square_train,4), 'r-square_test: ', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/60:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "56/61:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,20)\n",
      "56/62:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "56/63:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "56/64:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "60/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 50 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "X = np.linspace(0,10,N)\n",
      "X\n",
      "\n",
      "#path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#print(df.info())\n",
      "\n",
      "#X = df[0]\n",
      "#Y = df[1]\n",
      "\n",
      "#m = df.values\n",
      "60/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 50 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "X\n",
      "\n",
      "#path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#print(df.info())\n",
      "\n",
      "#X = df[0]\n",
      "#Y = df[1]\n",
      "\n",
      "#m = df.values\n",
      "61/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "#path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#print(df.info())\n",
      "\n",
      "#X = df[0]\n",
      "#Y = df[1]\n",
      "\n",
      "#m = df.values\n",
      "61/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "#path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#print(df.info())\n",
      "\n",
      "#X = df[0]\n",
      "#Y = df[1]\n",
      "\n",
      "#m = df.values\n",
      "61/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "#path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#print(df.info())\n",
      "\n",
      "#X = df[0]\n",
      "#Y = df[1]\n",
      "\n",
      "#m = df.values\n",
      "61/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "61/5:\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "61/6:\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "X\n",
      "61/7:\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "X\n",
      "61/8:\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "X\n",
      "61/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "61/10:\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "X\n",
      "61/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.append(np.ones(N),X)\n",
      "X\n",
      "61/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack(np.append(np.ones(N),X)).T\n",
      "X\n",
      "61/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X)]).T\n",
      "X\n",
      "61/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "X\n",
      "61/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "61/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "yhat = np.dot(X,w)\n",
      "61/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.plot(X,yhat)\n",
      "plt.show()\n",
      "61/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "x = np.linspace(0,10,N)\n",
      "y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "y[-1] += 30\n",
      "y[-2] += 20\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),x]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,y))\n",
      "yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,yhat)\n",
      "plt.show()\n",
      "61/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "x = np.linspace(0,10,N)\n",
      "y = .5 * x + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "y[-1] += 30\n",
      "y[-2] += 20\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),x]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,y))\n",
      "yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.plot(x,yhat)\n",
      "plt.show()\n",
      "61/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "Yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat)\n",
      "plt.show()\n",
      "61/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# understand that linear regression is the MLE solution to the line of best fit\n",
      "# minimizing the squared error is the same as maximizing the log likelihood function to produce the best mu, which is MLE\n",
      "\n",
      "# L2 (ridge) regression helps with outliers\n",
      "# It adds a penalty to large weights (coefficients) - flatten the slope so that predictions are less sensitive\n",
      "\n",
      "# Make data - 51 evenly spaced points between 0 and 10\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "Yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat)\n",
      "plt.show()\n",
      "61/22:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "\n",
      "l2 = 1000\n",
      "\n",
      "w_map = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "61/23:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "\n",
      "l2 = 1000\n",
      "\n",
      "#w_map = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "\n",
      "l2*(np.eye(2))\n",
      "61/24:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function...\n",
      "w_map = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y)+l2*np.eye(2))\n",
      "61/25:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function...\n",
      "w_map = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y)+l2*np.eye(2))\n",
      "\n",
      "Ymap = np.dot(X,w_map)\n",
      "61/26:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function...\n",
      "w_map = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y)+l2*np.eye(2))\n",
      "\n",
      "Ymap = np.dot(X,w_map)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat)\n",
      "plt.plot(X[:,1],Ymap)\n",
      "plt.show()\n",
      "61/27:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function...\n",
      "w_map = np.linalg.solve(l2*np.eye(2)+np.dot(X.T,X),np.dot(X.T,Y))\n",
      "\n",
      "Ymap = np.dot(X,w_map)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat)\n",
      "plt.plot(X[:,1],Ymap)\n",
      "plt.show()\n",
      "61/28:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function.\n",
      "# Have to put the l2*np.eye(2) first, if you put it second you get something different...\n",
      "w_map = np.linalg.solve(l2*np.eye(2)+np.dot(X.T,X),np.dot(X.T,Y))\n",
      "\n",
      "Ymap = np.dot(X,w_map)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat, label = \"mle\")\n",
      "plt.plot(X[:,1],Ymap, label = \"l2/ridge\")\n",
      "plt.show()\n",
      "61/29:\n",
      "# Now add in the penalty. Set L2 penalty as 1000\n",
      "l2 = 1000\n",
      "\n",
      "# Would need to go revisit algebra for this function.\n",
      "# Have to put the l2*np.eye(2) first, if you put it second you get something different...\n",
      "w_map = np.linalg.solve(l2*np.eye(2)+np.dot(X.T,X),np.dot(X.T,Y))\n",
      "\n",
      "Ymap = np.dot(X,w_map)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat, label = \"mle\")\n",
      "plt.plot(X[:,1],Ymap, label = \"l2/ridge\")\n",
      "plt.legend()\n",
      "plt.show()\n",
      "64/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function\n",
      "\n",
      "N = 51\n",
      "X = np.linspace(0,10,N)\n",
      "Y = .5 * X + np.random.randn(N)\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "Y[-1] += 30\n",
      "Y[-2] += 20\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Start with MLE method (regular old linear regression)\n",
      "\n",
      "X = np.vstack([np.ones(N),X]).T\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "Yhat = np.dot(X,w)\n",
      "\n",
      "plt.scatter(X[:,1],Y)\n",
      "plt.plot(X[:,1],Yhat)\n",
      "plt.show()\n",
      "64/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(0,10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(n, w)\n",
      "64/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(0,10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration: \",n,\":\", w)\n",
      "64/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(0,10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n,\":\", w)\n",
      "64/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(0,10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n,\":\", np.round(w,4))\n",
      "64/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(0,100):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n,\":\", np.round(w,4))\n",
      "64/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n,\":\", np.round(w,4))\n",
      "64/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "64/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    if w < .01:\n",
      "        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "64/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    if w < .01:\n",
      "        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0001\n",
      "test = 1        \n",
      "\n",
      "while w > precision:\n",
      "    w = w - learning_rate * 2*w\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(w,4))\n",
      "64/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0001\n",
      "test = 1        \n",
      "\n",
      "while w > precision:\n",
      "    w = w - learning_rate * 2*w\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(w,4))\n",
      "64/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(50):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0001\n",
      "test = 1        \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(60):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0001\n",
      "test = 1        \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(60):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0001\n",
      "test = 1        \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(60):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .001\n",
      "test = 1        \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(60):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .001\n",
      "test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .0000001\n",
      "test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .000000000001\n",
      "test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .00000000000000000000000000000000000000000000000000000000001\n",
      "test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .01\n",
      "#test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .01\n",
      "#test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# function = x^2\n",
      "# minimum occurs by setting derivative (2x) to be 0, which occurs at zero\n",
      "# let's do this in code though\n",
      "\n",
      "# gradient descent function:\n",
      "# wn = wn-1 - learning_rate * df/dx\n",
      "\n",
      "learning_rate = .1\n",
      "w = 20 #initial w\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - learning_rate * 2*w\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "precision = .01\n",
      "test = 0    \n",
      "\n",
      "joel = 20\n",
      "while joel > precision:\n",
      "    joel = joel - learning_rate * 2*joel\n",
      "    test +=1\n",
      "\n",
      "print(\"test\",test,\":\", np.round(joel,4))\n",
      "64/23:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2^3\n",
      "    print('test',n,': w1:'np.round(w1,4)', w2:',np.round(w2,4))\n",
      "64/24:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2^3\n",
      "    print('test',n,': w1:',np.round(w1,4)', w2:',np.round(w2,4))\n",
      "64/25:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2^3\n",
      "    print('test',n,': w1:',np.round(w1,4),', w2:',np.round(w2,4))\n",
      "64/26:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2^3\n",
      "    print('test',n,': w1:',np.round(w1,4),', w2:',np.round(w2,4))\n",
      "64/27:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    print('test',n,': w1:',np.round(w1,4),', w2:',np.round(w2,4))\n",
      "64/28:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1+w2\n",
      "    print('test',n,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,4))\n",
      "64/29:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .01\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1+w2\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,4))\n",
      "64/30:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .1\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1+w2\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,4))\n",
      "64/31:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .1\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,4))\n",
      "64/32:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "\n",
      "learning_rate = .1\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,5))\n",
      "64/33:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 10\n",
      "w2 = 10\n",
      "\n",
      "learning_rate = .1\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,5))\n",
      "64/34:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 10\n",
      "w2 = 10\n",
      "learning_rate = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,5))\n",
      "64/35:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 5\n",
      "w2 = 5\n",
      "learning_rate = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,5))\n",
      "64/36:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 15\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "64/37:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 15\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "64/38:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 1\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "64/39:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = 1\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/40:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 10\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "\n",
      "# resolves to 1.28416\n",
      "64/41:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = 1\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/42:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = 10\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/43:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 10\n",
      "lr = .1\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "\n",
      "# resolves to 1.28416\n",
      "64/44:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 5\n",
      "lr = .01\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "\n",
      "# resolves to 1.28416\n",
      "64/45:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = 10\n",
      "lr = .1\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/46:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = 10\n",
      "lr = .1\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/47:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = -2\n",
      "lr = .1\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/48:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "#w = -2\n",
      "#lr = .1\n",
      "\n",
      "#for n in range(100):\n",
      "#    w = w - lr*(6*w -5)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/49:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "\n",
      "df[:,:5]\n",
      "\n",
      "#w = -2\n",
      "#lr = .1\n",
      "\n",
      "#for n in range(100):\n",
      "#    w = w - lr*(6*w -5)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/50:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "m[:,:5]\n",
      "\n",
      "#w = -2\n",
      "#lr = .1\n",
      "\n",
      "#for n in range(100):\n",
      "#    w = w - lr*(6*w -5)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/51:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "m[:5,:]\n",
      "\n",
      "#w = -2\n",
      "#lr = .1\n",
      "\n",
      "#for n in range(100):\n",
      "#    w = w - lr*(6*w -5)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/52:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "\n",
      "#w = -2\n",
      "#lr = .1\n",
      "\n",
      "#for n in range(100):\n",
      "#    w = w - lr*(6*w -5)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "65/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "x = m[:,0]\n",
      "y = m[:,1]\n",
      "65/2:\n",
      "# plot data to see what it looks like\n",
      "\n",
      "plt.scatter(x,y)\n",
      "plt.show()\n",
      "65/3:\n",
      "#Now let's calculate m and b\n",
      "# need average of x^2\n",
      "# Joel's method:\n",
      "# joel_mean_xsquared = np.mean(np.square(x))\n",
      "# but it's also just a dot product with itself, divided by number of rows\n",
      "mean_xsquared = np.dot(x,x)/len(x)\n",
      "\n",
      "# need (average of x)^2\n",
      "mean_x_squared = np.square(np.mean(x))\n",
      "\n",
      "denominator = mean_xsquared - mean_x_squared\n",
      "\n",
      "a = (np.dot(x,y)/len(x) - np.mean(x)*np.mean(y))/denominator\n",
      "b = (np.mean(y) * np.dot(x,x)/len(x) - np.mean(x)*np.dot(x,y)/len(x))/denominator\n",
      "\n",
      "print(a)\n",
      "print(b)\n",
      "64/53:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .1\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/length(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/length(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/54:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .1\n",
      "length(X)\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "#for n in range(100):\n",
      "#    yhat = a*X + b\n",
      "#    dxda = -2/length(X) * np.sum(X*(Y-yhat))\n",
      "#    dxdb = -2/length(X) * np.sum(Y-yhat)\n",
      "#    a = a - lr*dxda\n",
      "#    b = b - lr*dxdb\n",
      "#    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/55:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .1\n",
      "len(X)\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "#for n in range(100):\n",
      "#    yhat = a*X + b\n",
      "#    dxda = -2/length(X) * np.sum(X*(Y-yhat))\n",
      "#    dxdb = -2/length(X) * np.sum(Y-yhat)\n",
      "#    a = a - lr*dxda\n",
      "#    b = b - lr*dxdb\n",
      "#    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/56:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .1\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/57:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/58:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 0\n",
      "b = 0\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "    print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/59:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 0\n",
      "b = 0\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/60:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 2\n",
      "b = 0\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/61:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "64/62:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "\n",
      "# now with this instructor's code\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*np.transpose(X)*(w-Y)\n",
      "64/63:\n",
      "# try to use gradient descent on the following:\n",
      "# J(w1,w2) = w1^2 + w2^4\n",
      "\n",
      "# derivative wrt w1: 2w1\n",
      "# derivative wrt w2: 4w2^3\n",
      "w1 = 5\n",
      "w2 = 5\n",
      "learning_rate = .01\n",
      "\n",
      "for n in range(10):\n",
      "    w1 = w1 - learning_rate * 2 * w1\n",
      "    w2 = w2 - learning_rate * 4 * w2**3\n",
      "    j = w1**2+w2**4\n",
      "    print('test',n+1,': w1:',np.round(w1,4),', w2:',np.round(w2,4),', j:',np.round(j,5))\n",
      "64/64:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^3 - 5x^2 - 2x\n",
      "# derivative = 9x^2 - 10x - 2\n",
      "\n",
      "w = 5\n",
      "lr = .01\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - lr*(9*w**2 - 10*w - 2)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "\n",
      "# resolves to 1.28416\n",
      "64/65:\n",
      "# how about an equation that should not resolve to 0?\n",
      "# function = 3x^2 - 5x - 2\n",
      "# derivative = 6x - 5\n",
      "# should resolve to .83333\n",
      "\n",
      "w = -2\n",
      "lr = .1\n",
      "\n",
      "for n in range(10):\n",
      "    w = w - lr*(6*w -5)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "64/66:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "\n",
      "# now with this instructor's code\n",
      "\n",
      "for n in range(100):\n",
      "    w = w - lr*np.transpose(X)*(w-Y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/67:\n",
      "# gradient descent for linear regression\n",
      "\n",
      "# after taking derivatives:\n",
      "# wn = wn-1 - learning_rate*XT(yhat-Y)\n",
      "# draw initial w from a Gaussian distribution with mean 0, variance 1/D (D is dimensionality)\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "m = df.values\n",
      "\n",
      "X = m[:,0]\n",
      "Y = m[:,1]\n",
      "\n",
      "# intial values for y = ax+b\n",
      "a = 1\n",
      "b = 1\n",
      "\n",
      "# from 001_1d_code, a = 1.9726, b = 2.8644\n",
      "\n",
      "w1 = 1\n",
      "w2 = 1\n",
      "lr = .0001\n",
      "\n",
      "# from https://towardsdatascience.com/linear-regression-using-gradient-descent-97a6c8700931\n",
      "\n",
      "for n in range(100):\n",
      "    yhat = a*X + b\n",
      "    dxda = -2/len(X) * np.sum(X*(Y-yhat))\n",
      "    dxdb = -2/len(X) * np.sum(Y-yhat)\n",
      "    a = a - lr*dxda\n",
      "    b = b - lr*dxdb\n",
      "print(\"iteration\",n+1,\": a =\", np.round(a,4), \",b =\",np.round(b,4))\n",
      "\n",
      "# now with this instructor's code\n",
      "\n",
      "for n in range(100):\n",
      "    w1 = w1 - lr*np.transpose(X)*(w-Y)\n",
      "    print(\"iteration\",n+1,\": w1 =\", np.round(w1,4))\n",
      "67/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeroes(N,D)\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeroes((N,D))\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros(N,D)\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "X\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "x[:,0] = 1\n",
      "\n",
      "X\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "\n",
      "X\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "X\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array(0*N/2,1*N/2)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*N/2,[1]*N/2)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*N/2 + [1]*N/2)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*N/2 + [1]*5)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5)\n",
      "Y\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "\n",
      "#learning_rate = .1\n",
      "#w = 20 #initial w\n",
      "\n",
      "#for n in range(10):\n",
      "#    w = w - learning_rate * 2*w\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "#    if w < .01:\n",
      "#        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    if w < .01:\n",
      "        print(\"iteration\",n+1,\":\", np.round(w,4))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "        \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    mse = np.dot(delta.T,delta)    \n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    mse = np.dot(delta.T,delta)/N\n",
      "    costs.append(mse)\n",
      "#precision = .01\n",
      "#test = 0\n",
      "67/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    mse = np.dot(delta.T,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show\n",
      "67/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    mse = np.dot(delta.T,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show\n",
      "67/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# gradient descent example\n",
      "\n",
      "# make an NxD matrix\n",
      "N = 10\n",
      "D = 3\n",
      "X = np.zeros((N,D))\n",
      "\n",
      "# first column (bias column?) all ones\n",
      "X[:,0] = 1\n",
      "# set first five elements of second column, second five elements of third column to 1\n",
      "X[:5,1] = 1\n",
      "X[5:,2] = 1\n",
      "\n",
      "# now make our Y to be 0 for the first half of the data, 1 for the second\n",
      "Y = np.array([0]*5 + [1]*5) # why won't N/2 work instead of 5?\n",
      "\n",
      "# try the usual way to find the coefficients\n",
      "# w = np.linalg.solve(np.dot(X.T,X),np.dot(X.T,Y))\n",
      "# oops, that's a singular matrix (cannot be inverted)\n",
      "# columns of all 1s, with the one-hot encoding\n",
      "\n",
      "# so we'll try gradient descent instead\n",
      "\n",
      "# store costs so that we can plot them\n",
      "costs = []\n",
      "\n",
      "# initialize random weights so that it has variance 1/D\n",
      "w = np.random.randn(D)/np.sqrt(D)\n",
      "\n",
      "learning_rate = .001\n",
      "\n",
      "#w = 20 #initial w\n",
      "\n",
      "for n in range(1000):\n",
      "    yhat = np.dot(X,w)\n",
      "    delta = yhat - Y\n",
      "    w = w - learning_rate * X.T.dot(delta)\n",
      "#    print(\"iteration\",n+1,\":\", np.round(w,5))\n",
      "    mse = np.dot(delta.T,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show\n",
      "print(w)\n",
      "71/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "71/2:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/3:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "m = df.values\n",
      "m\n",
      "71/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "m = df.values\n",
      "m\n",
      "71/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = (np.random.random(50))*100\n",
      "Yadd = (np.random.random(50))*200\n",
      "X = np.array(X + Xadd)\n",
      "Y = np.array(Y + Yadd)\n",
      "#m = df.values\n",
      "#m\n",
      "71/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = (np.random.random(50))*100\n",
      "Yadd = (np.random.random(50))*200\n",
      "#X = np.array(X + Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "\n",
      "X.type\n",
      "71/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = (np.random.random(50))*100\n",
      "Yadd = (np.random.random(50))*200\n",
      "#X = np.array(X + Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "\n",
      "X.shape\n",
      "71/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = (np.random.random(50))*100\n",
      "Yadd = (np.random.random(50))*200\n",
      "X.append(Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "71/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = DataFrame((np.random.random(50))*100))\n",
      "Yadd = (np.random.random(50))*200\n",
      "X.append(Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "71/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = DataFrame((np.random.random(50))*100)\n",
      "Yadd = (np.random.random(50))*200\n",
      "X.append(Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "71/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = (np.random.random(50))*200\n",
      "X.append(Xadd)\n",
      "#Y = np.array(Y + Yadd)\n",
      "71/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "X.shape\n",
      "71/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "Xadd.shape\n",
      "71/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "X.shape\n",
      "71/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "X.size\n",
      "71/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "X.ndim\n",
      "71/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "Xadd.ndim\n",
      "71/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "type(X)\n",
      "71/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.DataFrame((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "print(type(X))\n",
      "print(type(Xadd))\n",
      "71/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.Series((np.random.random(50))*100)\n",
      "Yadd = pd.DataFrame((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "#plt.scatter(X,Y)\n",
      "#plt.show()\n",
      "print(type(X))\n",
      "print(type(Xadd))\n",
      "71/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.Series((np.random.random(50))*100)\n",
      "Yadd = pd.Series((np.random.random(50))*200)\n",
      "X.append(Xadd)\n",
      "Y.append(Yadd)\n",
      "\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.Series((np.random.random(50))*100)\n",
      "Yadd = pd.Series((np.random.random(50))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/25:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/26:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/27:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "#Xadd = pd.Series((np.random.random(50))*100)\n",
      "#Yadd = pd.Series((np.random.random(50))*200)\n",
      "#X = X.append(Xadd)\n",
      "#Y = Y.append(Yadd)\n",
      "\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/28:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/29:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/30:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in 50 outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "\n",
      "Xadd = pd.Series((np.random.random(50))*100)\n",
      "Yadd = pd.Series((np.random.random(50))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/31:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/32:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/33:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/34:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    xtrain = X.iloc[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X.iloc[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "70/1:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+0*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,truew) + np.random.randn(N) + .5\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "70/2:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+[0]*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,truew) + np.random.randn(N) + .5\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "70/3:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+[0]*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,true_w) + np.random.randn(N) + .5\n",
      "\n",
      "# Now create a few outliers - add 30 to the last point and 20 to the second-to-last\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "70/4:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+[0]*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,true_w) + np.random.randn(N) * .5\n",
      "\n",
      "# now do gradient descent\n",
      "costs = []\n",
      "w = np.random.randn(D) / np.sqrt(D)\n",
      "learning_rate = .001\n",
      "l1 = 10\n",
      "\n",
      "for t in range(500):\n",
      "    Yhat = np.dot(X,w)\n",
      "    delta = Yhat - Y\n",
      "    w = w - learning_rate*np.dot(X.T,delta) + l1*np.sign(w)\n",
      "    mse = np.dot(delta,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show()\n",
      "71/35:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "# Maybe this will show the overfitting...\n",
      "\n",
      "outliers = 20\n",
      "\n",
      "Xadd = pd.Series((np.random.random(outliers))*100)\n",
      "Yadd = pd.Series((np.random.random(outliers))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/36:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/37:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    # Because X and Y are series, I have to use iloc?\n",
      "    xtrain = X.iloc[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X.iloc[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/38:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "# Maybe this will show the overfitting...\n",
      "\n",
      "outliers = 25\n",
      "\n",
      "Xadd = pd.Series((np.random.random(outliers))*100)\n",
      "Yadd = pd.Series((np.random.random(outliers))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/39:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/40:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    # Because X and Y are series, I have to use iloc?\n",
      "    xtrain = X.iloc[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X.iloc[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/41:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "# Maybe this will show the overfitting...\n",
      "\n",
      "outliers = 50\n",
      "\n",
      "Xadd = pd.Series((np.random.random(outliers))*100)\n",
      "Yadd = pd.Series((np.random.random(outliers))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/42:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/43:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    # Because X and Y are series, I have to use iloc?\n",
      "    xtrain = X.iloc[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X.iloc[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "71/44:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "X = df[0]\n",
      "Y = df[1]\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "\n",
      "# Add in outliers from 0 to 100 on X, 0 to 200 on Y\n",
      "# Maybe this will show the overfitting...\n",
      "\n",
      "outliers = 50\n",
      "\n",
      "Xadd = pd.Series((np.random.random(outliers))*100)\n",
      "Yadd = pd.Series((np.random.random(outliers))*200)\n",
      "X = X.append(Xadd)\n",
      "Y = Y.append(Yadd)\n",
      "\n",
      "plt.scatter(X,Y)\n",
      "plt.show()\n",
      "71/45:\n",
      "# make a function that appends polynomials to the nth degree\n",
      "def make_data(X,degree):\n",
      "    xdata=[np.ones(len(X))]\n",
      "    for d in range(degree):\n",
      "        xdata.append(X**(d+1))\n",
      "    #have to transpose it? Need to look into vstack...\n",
      "    return np.vstack(xdata).T\n",
      "71/46:\n",
      "# create and run regression models using a train/test split, use 80/20\n",
      "def run_model(X,Y,deg):\n",
      "    n = len(Y)\n",
      "    train_idx=np.random.choice(n,int(.8*n))\n",
      "\n",
      "    # Because X and Y are series, I have to use iloc?\n",
      "    xtrain = X.iloc[train_idx]\n",
      "    ytrain = Y.iloc[train_idx]\n",
      "    \n",
      "    test_idx = [idx for idx in range(n) if idx not in train_idx]\n",
      "    xtest = X.iloc[test_idx]\n",
      "    ytest = Y.iloc[test_idx]\n",
      "    \n",
      "    for d in range(0,deg):\n",
      "        xtraindata = make_data(xtrain,d)\n",
      "        xtestdata = make_data(xtest,d)\n",
      "        w = np.linalg.solve(np.dot(xtraindata.T,xtraindata),np.dot(xtraindata.T,ytrain))\n",
      "\n",
      "        yhat_train = np.dot(xtraindata,w)\n",
      "        sse_train = np.dot(ytrain-yhat_train,ytrain-yhat_train)\n",
      "        error_train = np.dot(ytrain-np.mean(ytrain),ytrain-np.mean(ytrain))\n",
      "        r_square_train = 1 - sse_train/error_train\n",
      "        \n",
      "        yhat_test = np.dot(xtestdata,w)\n",
      "        sse_test = np.dot(ytest-yhat_test,ytest-yhat_test)\n",
      "        error_test = np.dot(ytest-np.mean(ytest),ytest-np.mean(ytest))\n",
      "        r_square_test = 1 - sse_test/error_test\n",
      "        print('degree',d, '- r-square_train:', np.round(r_square_train,4), 'r-square_test:', np.round(r_square_test,4))\n",
      "        #plt.scatter(xtest,ytest)\n",
      "        #plt.plot(sorted(xtest),sorted(yhat_test))\n",
      "        #plt.show()\n",
      "        \n",
      "run_model(X,Y,10)\n",
      "70/5:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+[0]*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,true_w) + np.random.randn(N) * .5\n",
      "\n",
      "# now do gradient descent\n",
      "costs = []\n",
      "w = np.random.randn(D) / np.sqrt(D)\n",
      "learning_rate = .001\n",
      "l1 = 10\n",
      "\n",
      "for t in range(500):\n",
      "    Yhat = np.dot(X,w)\n",
      "    delta = Yhat - Y\n",
      "    w = w - learning_rate*(np.dot(X.T,delta) + l1*np.sign(w))\n",
      "    mse = np.dot(delta,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show()\n",
      "70/6:\n",
      "import matplotlib.pyplot as plt\n",
      "#import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Make a fat matrix data - 51 evenly spaced points between 0 and 10\n",
      "N = 50\n",
      "D = 50\n",
      "\n",
      "# centered around 0 from -5 to 5\n",
      "X = (np.random.random((N,D)) - .5) * 10\n",
      "\n",
      "# last D-3 terms will not influence the output\n",
      "true_w = np.array([1,0.5,-0.5]+[0]*(D-3))\n",
      "\n",
      "# add in some Gaussian random noise\n",
      "Y = np.dot(X,true_w) + np.random.randn(N) * .5\n",
      "\n",
      "# now do gradient descent\n",
      "costs = []\n",
      "w = np.random.randn(D) / np.sqrt(D)\n",
      "learning_rate = .001\n",
      "l1 = 10\n",
      "\n",
      "for t in range(10):\n",
      "    Yhat = np.dot(X,w)\n",
      "    delta = Yhat - Y\n",
      "    w = w - learning_rate*(np.dot(X.T,delta) + l1*np.sign(w))\n",
      "    mse = np.dot(delta,delta)/N\n",
      "    costs.append(mse)\n",
      "\n",
      "plt.plot(costs)\n",
      "plt.show()\n",
      "72/1:\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split \n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn import metrics\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "# File paths\n",
      "input_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\data'\n",
      "image_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\images'\n",
      "\n",
      "\n",
      "# Expand columns for readability\n",
      "pd.set_option('display.max_columns', 50)\n",
      "\n",
      "%matplotlib inline\n",
      "72/2:\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split \n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn import metrics\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "# File paths\n",
      "#input_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\data'\n",
      "#image_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\images'\n",
      "\n",
      "input_dir= 'C:\\Users\\jlahrman\\OneDrive - LMI\\Documents\\FAI\\Regression\\data'\n",
      "image_dir= 'C:\\Users\\jlahrman\\OneDrive - LMI\\Documents\\FAI\\Regression\\images'\n",
      "\n",
      "# Expand columns for readability\n",
      "pd.set_option('display.max_columns', 50)\n",
      "\n",
      "%matplotlib inline\n",
      "72/3:\n",
      "# Return correlation matrix in more useful table\n",
      "def corr_table(df):\n",
      "    # Calculate the correlation matrix and reshape\n",
      "    df_corr = df.corr().stack().reset_index()\n",
      "    \n",
      "    # Rename columns\n",
      "    df_corr.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
      "\n",
      "    # Create a mask to identify rows with duplicate features \n",
      "    mask_dups = (df_corr[['Feature_1', 'Feature_2']].apply(frozenset, axis=1).duplicated()) | (\n",
      "                    df_corr['Feature_1']==df_corr['Feature_2']) \n",
      "\n",
      "    # Filter out duplicates\n",
      "    df_corr = df_corr[~mask_dups]\n",
      "    \n",
      "    return df_corr\n",
      "72/4:\n",
      "# Reference\n",
      "prof_col=['Competency 1','Competency 2','Competency 3','Competency 4','Competency 5',\n",
      "         'Competency 6','Competency 7','Competency 8', 'Competency 9', 'Competency 10',\n",
      "         'Competency 11','Competency 12','Competency 13','Competency 14']\n",
      "time_col=['Time in Competency 1','Time in Competency 2','Time in Competency 3','Time in Competency 4',\n",
      "         'Time in Competency 5','Time in Competency 6','Time in Competency 7','Time in Competency 8',\n",
      "         'Time in Competency 9', 'Time in Competency 10','Time in Competency 11','Time in Competency 12',\n",
      "         'Time in Competency 13','Time in Competency 14']\n",
      "72/5:\n",
      "# Read in pickled filtered data set\n",
      "filt_df=pd.read_pickle(os.path.join(input_dir, 'filtered_data.pkl')).reset_index().drop('id', axis=1)\n",
      "# filt_df.head()\n",
      "72/6:\n",
      "# Read in pickled filtered data set\n",
      "filt_df=pd.read_pickle(os.path.join(input_dir, 'filtered_data.pkl')).reset_index().drop('id', axis=1)\n",
      "# filt_df.head()\n",
      "72/7:\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split \n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn import metrics\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "# File paths\n",
      "#input_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\data'\n",
      "#image_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\images'\n",
      "\n",
      "input_dir= 'C:\\Users\\jlahrman\\OneDrive - LMI\\Documents\\FAI\\Regression\\data'\n",
      "image_dir= 'C:\\Users\\jlahrman\\OneDrive - LMI\\Documents\\FAI\\Regression\\images'\n",
      "\n",
      "# Expand columns for readability\n",
      "pd.set_option('display.max_columns', 50)\n",
      "\n",
      "%matplotlib inline\n",
      "72/8:\n",
      "import os\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.model_selection import train_test_split \n",
      "from sklearn import datasets, linear_model\n",
      "from sklearn import metrics\n",
      "import statsmodels.api as sm\n",
      "\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "\n",
      "# File paths\n",
      "#input_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\data'\n",
      "#image_dir= 'c:\\\\Users\\\\tlewis\\\\OneDrive - LMI\\\\work\\\\jupyter stuff\\\\fai_ed_comp\\\\images'\n",
      "\n",
      "input_dir= 'C:/Users/jlahrman/OneDrive - LMI/Documents/FAI/Regression/data'\n",
      "image_dir= 'C:/Users/jlahrman/OneDrive - LMI/Documents/FAI/Regression/images'\n",
      "\n",
      "# Expand columns for readability\n",
      "pd.set_option('display.max_columns', 50)\n",
      "\n",
      "%matplotlib inline\n",
      "72/9:\n",
      "# Return correlation matrix in more useful table\n",
      "def corr_table(df):\n",
      "    # Calculate the correlation matrix and reshape\n",
      "    df_corr = df.corr().stack().reset_index()\n",
      "    \n",
      "    # Rename columns\n",
      "    df_corr.columns = ['Feature_1', 'Feature_2', 'Correlation']\n",
      "\n",
      "    # Create a mask to identify rows with duplicate features \n",
      "    mask_dups = (df_corr[['Feature_1', 'Feature_2']].apply(frozenset, axis=1).duplicated()) | (\n",
      "                    df_corr['Feature_1']==df_corr['Feature_2']) \n",
      "\n",
      "    # Filter out duplicates\n",
      "    df_corr = df_corr[~mask_dups]\n",
      "    \n",
      "    return df_corr\n",
      "72/10:\n",
      "# Reference\n",
      "prof_col=['Competency 1','Competency 2','Competency 3','Competency 4','Competency 5',\n",
      "         'Competency 6','Competency 7','Competency 8', 'Competency 9', 'Competency 10',\n",
      "         'Competency 11','Competency 12','Competency 13','Competency 14']\n",
      "time_col=['Time in Competency 1','Time in Competency 2','Time in Competency 3','Time in Competency 4',\n",
      "         'Time in Competency 5','Time in Competency 6','Time in Competency 7','Time in Competency 8',\n",
      "         'Time in Competency 9', 'Time in Competency 10','Time in Competency 11','Time in Competency 12',\n",
      "         'Time in Competency 13','Time in Competency 14']\n",
      "72/11:\n",
      "# Read in pickled filtered data set\n",
      "filt_df=pd.read_pickle(os.path.join(input_dir, 'filtered_data.pkl')).reset_index().drop('id', axis=1)\n",
      "# filt_df.head()\n",
      "72/12: prof_df=filt_df[prof_col]\n",
      "72/13:\n",
      "# Add column with mean competencies/proficencies\n",
      "prof_df['Competency Avg']=prof_df.mean(axis=1)\n",
      "72/14: prof_df.head()\n",
      "72/15:\n",
      "# Primary features for now\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)',]\n",
      "72/16:\n",
      "features=filt_df[feature_col]\n",
      "\n",
      "features.head()\n",
      "72/17:\n",
      "# Ages\n",
      "agemap={'25 Years Old and Under': 0,'26-30': 1,'31-35': 2,'36-40': 3,'41-45': 4,'46-50': 5,\n",
      "     '51-55': 6,'56-60': 7,  'Over 60 Years Old':8}\n",
      "\n",
      "# Education Level \n",
      "# Set 'other' to null\n",
      "edmap= {\"Associate's Degree\": 1,\"Bachelor's Degree\": 2,'Doctoral Degree': 4,'High School/GED': 0,\n",
      " \"Master's Degree\": 3,'Other': np.nan}\n",
      "\n",
      "# FAC-C Level\n",
      "facmap= {'In Progress Level 1':0, 'Level 1':1, 'Level 2':2, 'Level 3':3}\n",
      "\n",
      "# GS Level \n",
      "gsmap={'Intermediate Level (GS 8 - 12)':1, 'Expert Level (GS 13 - SES)':2,\n",
      "       'Entry Level (GS 1 - 7)':0, 'FS-1':2,'FS-2':2, 'FS-3':1, 'FS-4':1,\n",
      "       'FS-5':1, 'Other':np.nan }\n",
      "\n",
      "# Experience in government\n",
      "expmap={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "expmap_gov={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "72/18:\n",
      "# Use replace to manually map integers to ordinal values\n",
      "X=features.copy(deep=True)\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap},inplace=True)\n",
      "72/19: X.head()\n",
      "72/20: y = prof_df.loc[:,'Competency Avg']\n",
      "72/21: y.head()\n",
      "72/22:\n",
      "# to impute replacement values \n",
      "from sklearn.impute import SimpleImputer\n",
      "72/23:\n",
      "# split with defaults\n",
      "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1)\n",
      "72/24: X_train.shape\n",
      "72/25: y_train.shape\n",
      "72/26:\n",
      "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
      "imp.fit(X_train)\n",
      "72/27: X_train=imp.transform(X_train)\n",
      "72/28: from sklearn.ensemble import RandomForestClassifier\n",
      "72/29: clf=RandomForestClassifier(n_jobs=2, random_state=0)\n",
      "72/30: X_train=X_train.astype(int)\n",
      "72/31: X_train.dtype\n",
      "72/32: y_train=y_train.astype(int)\n",
      "72/33: clf.fit(X_train,y_train)\n",
      "72/34:\n",
      "# Impute nulls in test data \n",
      "imp.fit(X_test)\n",
      "X_test=imp.transform(X_test)\n",
      "72/35: pred=clf.predict(X_test)\n",
      "72/36: pred[0:5]\n",
      "72/37: y_test[0:5]\n",
      "72/38:\n",
      "# May need to fill nulls by correlated columns- side task\n",
      "corr_table(X).sort_values(by='Correlation', ascending=False)\n",
      "72/39: X['FAC-C Level'].value_counts(dropna=False).tail(1)\n",
      "72/40:\n",
      "for (column, data) in X.iteritems():\n",
      "    print(column)\n",
      "    print(data.value_counts(dropna=False).tail(1))\n",
      "72/41: X[X['Education Level'].isnull()]\n",
      "72/42: X['Education Level'].fillna((X['Education Level'].mode()), inplace=True)\n",
      "72/43: from sklearn.linear_model import LinearRegression\n",
      "72/44: X.head()\n",
      "72/45:\n",
      "# few nulls relative to data, so drop rather than impute\n",
      "\n",
      "is_NaN = X.isnull()\n",
      "row_has_NaN = is_NaN.any(axis=1)\n",
      "rows_with_NaN = X[row_has_NaN]\n",
      "# print(rows_with_NaN)\n",
      "\n",
      "X_nonN = X.dropna()\n",
      "y_nonN = y.loc[X_nonN.index]\n",
      "72/46:\n",
      "print(X.shape)\n",
      "print(X_nonN.shape)\n",
      "72/47: (2845-2728)\n",
      "72/48:\n",
      "# start with using all competency AVG\n",
      "y.head()\n",
      "72/49:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "72/50:\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)\n",
      "72/51:\n",
      "plt.scatter(y_test, predictions)\n",
      "plt.xlabel('True Values')\n",
      "plt.ylabel('Predictions')\n",
      "72/52:\n",
      "# initial results are BAD\n",
      "\n",
      "print('Score:',model.score(X_test, y_test))\n",
      "72/53:\n",
      "import statsmodels.api as sm\n",
      "lin_model=sm.OLS(y_train,X_train)\n",
      "result=lin_model.fit()\n",
      "print(result.summary())\n",
      "72/54:\n",
      "# Primary features for now\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)','Years of experience (industry)']\n",
      "72/55:\n",
      "features=filt_df[feature_col]\n",
      "\n",
      "features.head()\n",
      "72/56:\n",
      "# Experience in industry\n",
      "expmap_ind={'10-20 Years':4, '20 Years or More':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "72/57:\n",
      "X=features.copy(deep=True)\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap_gov, 'Years of experience (industry)':expmap_ind},inplace=True)\n",
      "72/58: X.head()\n",
      "72/59:\n",
      "# few nulls relative to data, so drop rather than impute\n",
      "\n",
      "is_NaN = X.isnull()\n",
      "row_has_NaN = is_NaN.any(axis=1)\n",
      "rows_with_NaN = X[row_has_NaN]\n",
      "# print(rows_with_NaN)\n",
      "\n",
      "X_nonN = X.dropna()\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_nonN = y.loc[X_nonN.index]\n",
      "72/60:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "72/61:\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)\n",
      "72/62:\n",
      "plt.scatter(y_test, predictions)\n",
      "plt.xlabel('True Values')\n",
      "plt.ylabel('Predictions')\n",
      "72/63: print('Score:',model.score(X_test, y_test))\n",
      "72/64:\n",
      "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "72/65:\n",
      "n_bins = 20\n",
      "plt.hist(y_nonN,bins=n_bins)\n",
      "72/66:\n",
      "n_bins = 20\n",
      "plt.hist(predictions,bins=n_bins)\n",
      "72/67: model.coef_\n",
      "72/68: X_nonN.columns\n",
      "72/69:\n",
      "import statsmodels.api as sm\n",
      "lin_model=sm.OLS(y_train,X_train)\n",
      "result=lin_model.fit()\n",
      "print(result.summary())\n",
      "72/70: filt_df.head(1)\n",
      "72/71:\n",
      "# Ages\n",
      "agemap={'25 Years Old and Under': 0,'26-30': 1,'31-35': 2,'36-40': 3,'41-45': 4,'46-50': 5,\n",
      "     '51-55': 6,'56-60': 7,  'Over 60 Years Old':8}\n",
      "\n",
      "# Education Level \n",
      "# Set 'other' to null\n",
      "edmap= {\"Associate's Degree\": 1,\"Bachelor's Degree\": 2,'Doctoral Degree': 4,'High School/GED': 0,\n",
      " \"Master's Degree\": 3,'Other': np.nan}\n",
      "\n",
      "# FAC-C Level\n",
      "facmap= {'In Progress Level 1':0, 'Level 1':1, 'Level 2':2, 'Level 3':3}\n",
      "\n",
      "# GS Level \n",
      "gsmap={'Intermediate Level (GS 8 - 12)':1, 'Expert Level (GS 13 - SES)':2,\n",
      "       'Entry Level (GS 1 - 7)':0, 'FS-1':2,'FS-2':2, 'FS-3':1, 'FS-4':1,\n",
      "       'FS-5':1, 'Other':np.nan }\n",
      "\n",
      "# Experience in government\n",
      "expmap_gov={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# Experience in industry\n",
      "expmap_ind={'10-20 Years':4, '20 Years or More':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# FAC-C %\n",
      "facpct = {'0%-25%':0, '26%-50%':1, '51%-75%':2, '76%-100%':3}\n",
      "\n",
      "# Commodities\n",
      "IT_comm = {'No':0, 'Yes':1}\n",
      "serv_comm = {'No':0, 'Yes':1}\n",
      "MP_comm = {'No':0, 'Yes':1}\n",
      "con_comm = {'No':0, 'Yes':1}\n",
      "AE_comm = {'No':0, 'Yes':1}\n",
      "fac_comm = {'No':0, 'Yes':1}\n",
      "sch_comm = {'No':0, 'Yes':1}\n",
      "72/72:\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)','Years of experience (industry)', \n",
      "             'IT commodities', 'Services commodities', 'Major Programs commodities', 'Construction commodities', \n",
      "             'A&E commodities', 'Facilities commodities', 'Schedules commodities',\n",
      "             'Overall, what percentage of your time is dedicated to FAC-related activities?']\n",
      "72/73:\n",
      "features=filt_df[feature_col]\n",
      "\n",
      "features.head()\n",
      "72/74:\n",
      "X=features.copy(deep=True)\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap_gov, 'Years of experience (industry)':expmap_ind,\n",
      "          'IT commodities':IT_comm, 'Services commodities':serv_comm, 'Major Programs commodities':MP_comm, \n",
      "           'Construction commodities': con_comm, 'A&E commodities':AE_comm, 'Facilities commodities':fac_comm, \n",
      "           'Schedules commodities': sch_comm,\n",
      "           'Overall, what percentage of your time is dedicated to FAC-related activities?': facpct},inplace=True)\n",
      "72/75: X.head()\n",
      "72/76:\n",
      "# few nulls relative to data, so drop rather than impute\n",
      "\n",
      "is_NaN = X.isnull()\n",
      "row_has_NaN = is_NaN.any(axis=1)\n",
      "rows_with_NaN = X[row_has_NaN]\n",
      "# print(rows_with_NaN)\n",
      "\n",
      "X_nonN = X.dropna()\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_nonN = y.loc[X_nonN.index]\n",
      "72/77:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "72/78:\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)\n",
      "72/79:\n",
      "plt.scatter(y_test, predictions)\n",
      "plt.xlabel('True Values')\n",
      "plt.ylabel('Predictions')\n",
      "72/80: print('Score:',model.score(X_test, y_test))\n",
      "72/81:\n",
      "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/82: metrics.mean_absolute_error(y_test, predictions) / y_test.mean()\n",
      "72/83:\n",
      "n_bins = 20\n",
      "plt.hist(y_nonN,bins=n_bins)\n",
      "72/84:\n",
      "n_bins = 20\n",
      "plt.hist(predictions,bins=n_bins)\n",
      "72/85: model.coef_\n",
      "72/86: X_nonN.columns\n",
      "72/87:\n",
      "import statsmodels.api as sm\n",
      "lin_model=sm.OLS(y_train,X_train)\n",
      "result=lin_model.fit()\n",
      "print(result.summary())\n",
      "72/88: prof_df\n",
      "72/89:\n",
      "X_N = X.dropna()\n",
      "for i in range(1,15):\n",
      "    print('')\n",
      "    print('Competency {}'.format(i))\n",
      "    y = prof_df.loc[:,'Competency {}'.format(i)]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "#     print(X_train.shape, y_train.shape)\n",
      "#     print(X_test.shape, y_test.shape)\n",
      "    lin_model=sm.OLS(y_train,X_train)\n",
      "    result=lin_model.fit()\n",
      "    print(result.summary())\n",
      "72/90:\n",
      "X_N = X.dropna()\n",
      "for i in range(1,15):\n",
      "    print('')\n",
      "    print('')\n",
      "    print('Competency {}'.format(i))\n",
      "    y = prof_df.loc[:,'Competency {}'.format(i)]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "    lm = linear_model.LinearRegression()\n",
      "    model = lm.fit(X_train, y_train)\n",
      "    predictions = lm.predict(X_test)    \n",
      "\n",
      "    print('Score:',model.score(X_test, y_test))\n",
      "\n",
      "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "    print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "    print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/91:\n",
      "# Primary features for now\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)','Years of experience (industry)']\n",
      "72/92:\n",
      "features=filt_df[feature_col]\n",
      "\n",
      "features.head()\n",
      "72/93:\n",
      "# Ages\n",
      "agemap={'25 Years Old and Under': 0,'26-30': 1,'31-35': 2,'36-40': 3,'41-45': 4,'46-50': 5,\n",
      "     '51-55': 6,'56-60': 7,  'Over 60 Years Old':8}\n",
      "\n",
      "# Education Level \n",
      "# Set 'other' to null\n",
      "edmap= {\"Associate's Degree\": 1,\"Bachelor's Degree\": 2,'Doctoral Degree': 4,'High School/GED': 0,\n",
      " \"Master's Degree\": 3,'Other': np.nan}\n",
      "\n",
      "# FAC-C Level\n",
      "facmap= {'In Progress Level 1':0, 'Level 1':1, 'Level 2':2, 'Level 3':3}\n",
      "\n",
      "# GS Level \n",
      "gsmap={'Intermediate Level (GS 8 - 12)':1, 'Expert Level (GS 13 - SES)':2,\n",
      "       'Entry Level (GS 1 - 7)':0, 'FS-1':2,'FS-2':2, 'FS-3':1, 'FS-4':1,\n",
      "       'FS-5':1, 'Other':np.nan }\n",
      "\n",
      "# Experience in government\n",
      "expmap_gov={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# Experience in industry\n",
      "expmap_ind={'10-20 Years':4, '20 Years or More':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "72/94:\n",
      "X=features.copy(deep=True)\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap_gov, 'Years of experience (industry)':expmap_ind},inplace=True)\n",
      "72/95: X.head()\n",
      "72/96:\n",
      "is_NaN = X.isnull()\n",
      "row_has_NaN = is_NaN.any(axis=1)\n",
      "rows_with_NaN = X[row_has_NaN]\n",
      "# print(rows_with_NaN)\n",
      "\n",
      "X_nonN = X.dropna()\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_nonN = round(y.loc[X_nonN.index])\n",
      "72/97:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "72/98: X_nonN.head()\n",
      "72/99:\n",
      "lm = linear_model.LogisticRegression()\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)\n",
      "72/100:\n",
      "plt.scatter(y_test, predictions)\n",
      "plt.xlabel('True Values')\n",
      "plt.ylabel('Predictions')\n",
      "72/101: print('Score:',model.score(X_test, y_test))\n",
      "72/102:\n",
      "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/103:\n",
      "n_bins = 20\n",
      "plt.hist(y_nonN,bins=n_bins)\n",
      "72/104: plt.hist(predictions,bins=n_bins)\n",
      "72/105: model.coef_\n",
      "72/106: X_nonN.columns\n",
      "72/107:\n",
      "import statsmodels as sm\n",
      "logit_model=sm.discrete.discrete_model.MNLogit(y_train,X_train)\n",
      "result=logit_model.fit()\n",
      "print(result.summary())\n",
      "72/108: filt_df.head(1)\n",
      "72/109:\n",
      "# Ages\n",
      "agemap={'25 Years Old and Under': 0,'26-30': 1,'31-35': 2,'36-40': 3,'41-45': 4,'46-50': 5,\n",
      "     '51-55': 6,'56-60': 7,  'Over 60 Years Old':8}\n",
      "\n",
      "# Education Level \n",
      "# Set 'other' to null\n",
      "edmap= {\"Associate's Degree\": 1,\"Bachelor's Degree\": 2,'Doctoral Degree': 4,'High School/GED': 0,\n",
      " \"Master's Degree\": 3,'Other': np.nan}\n",
      "\n",
      "# FAC-C Level\n",
      "facmap= {'In Progress Level 1':0, 'Level 1':1, 'Level 2':2, 'Level 3':3}\n",
      "\n",
      "# GS Level \n",
      "gsmap={'Intermediate Level (GS 8 - 12)':1, 'Expert Level (GS 13 - SES)':2,\n",
      "       'Entry Level (GS 1 - 7)':0, 'FS-1':2,'FS-2':2, 'FS-3':1, 'FS-4':1,\n",
      "       'FS-5':1, 'Other':np.nan }\n",
      "\n",
      "# Experience in government\n",
      "expmap_gov={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# Experience in industry\n",
      "expmap_ind={'10-20 Years':4, '20 Years or More':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# FAC-C %\n",
      "facpct = {'0%-25%':0, '26%-50%':1, '51%-75%':2, '76%-100%':3}\n",
      "\n",
      "# Commodities\n",
      "IT_comm = {'No':0, 'Yes':1}\n",
      "serv_comm = {'No':0, 'Yes':1}\n",
      "MP_comm = {'No':0, 'Yes':1}\n",
      "con_comm = {'No':0, 'Yes':1}\n",
      "AE_comm = {'No':0, 'Yes':1}\n",
      "fac_comm = {'No':0, 'Yes':1}\n",
      "sch_comm = {'No':0, 'Yes':1}\n",
      "72/110:\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)','Years of experience (industry)', \n",
      "             'IT commodities', 'Services commodities', 'Major Programs commodities', 'Construction commodities', \n",
      "             'A&E commodities', 'Facilities commodities', 'Schedules commodities',\n",
      "             'Overall, what percentage of your time is dedicated to FAC-related activities?']\n",
      "72/111:\n",
      "features=filt_df[feature_col]\n",
      "\n",
      "features.head()\n",
      "72/112:\n",
      "X=features.copy(deep=True)\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap_gov, 'Years of experience (industry)':expmap_ind,\n",
      "          'IT commodities':IT_comm, 'Services commodities':serv_comm, 'Major Programs commodities':MP_comm, \n",
      "           'Construction commodities': con_comm, 'A&E commodities':AE_comm, 'Facilities commodities':fac_comm, \n",
      "           'Schedules commodities': sch_comm,\n",
      "           'Overall, what percentage of your time is dedicated to FAC-related activities?': facpct},inplace=True)\n",
      "72/113: X.head()\n",
      "72/114:\n",
      "is_NaN = X.isnull()\n",
      "row_has_NaN = is_NaN.any(axis=1)\n",
      "rows_with_NaN = X[row_has_NaN]\n",
      "# print(rows_with_NaN)\n",
      "\n",
      "X_nonN = X.dropna()\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_nonN = round(y.loc[X_nonN.index])\n",
      "72/115:\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "72/116: X_train.head()\n",
      "72/117: y_train.head()\n",
      "72/118:\n",
      "lm = linear_model.LogisticRegression(max_iter=1000)\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)\n",
      "72/119:\n",
      "plt.scatter(y_test, predictions)\n",
      "plt.xlabel('True Values')\n",
      "plt.ylabel('Predictions')\n",
      "72/120: print('Score:',model.score(X_test, y_test))\n",
      "72/121:\n",
      "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/122:\n",
      "n_bins = 20\n",
      "plt.hist(y_nonN,bins=n_bins)\n",
      "72/123: plt.hist(predictions,bins=n_bins)\n",
      "72/124: model.coef_\n",
      "72/125: X_nonN.columns\n",
      "72/126:\n",
      "import statsmodels as sm\n",
      "logit_model=sm.discrete.discrete_model.MNLogit(y_train,X_train)\n",
      "result=logit_model.fit(maxiter=2000)\n",
      "print(result.summary())\n",
      "72/127: X = X.rename(columns={\"Overall, what percentage of your time is dedicated to FAC-related activities?\": \"Overall percentage of time to FAC activities\"})\n",
      "72/128:\n",
      "\n",
      "X_N = X.dropna()\n",
      "for i in range(1,15):\n",
      "    print('')\n",
      "    print('')\n",
      "    print('Competency {}'.format(i))\n",
      "    y = prof_df.loc[:,'Competency {}'.format(i)]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y.loc[X_N.index]\n",
      "    y_nonN = round(y_x.dropna())\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "    if i==3:\n",
      "        print(X_nonN.shape)\n",
      "        print(y_nonN.shape)\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "    logit_model=sm.discrete.discrete_model.MNLogit(y_train,X_train)\n",
      "    result=logit_model.fit()\n",
      "    print(result.summary())\n",
      "72/129:\n",
      "# Read in pickled filtered data set\n",
      "filt_df=pd.read_pickle(os.path.join(input_dir, 'filtered_data.pkl')).reset_index().drop('id', axis=1)\n",
      "filt_df.head()\n",
      "72/130: vs=pd.read_csv(os.path.join(input_dir,'encoded_all_data.csv'))\n",
      "72/131:\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of Exp (Gov)','Years of Exp (Ind)', \n",
      "             'Major Programs commodities', 'Construction commodities', \n",
      "             'A&E commodities', 'Facilities commodities', 'Schedules commodities',\n",
      "             'FAC Related % Time']\n",
      "\n",
      "features=vs[feature_col]\n",
      "X=features.copy(deep=True)\n",
      "\n",
      "# features.head()\n",
      "72/132: y.head()\n",
      "72/133:\n",
      "X_nonN = X.dropna()\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_nonN = y.loc[X_nonN.index]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "print(X_train.shape, y_train.shape)\n",
      "print(X_test.shape, y_test.shape)\n",
      "\n",
      "# X_train.head()\n",
      "\n",
      "# y_train.head()\n",
      "\n",
      "# lm = linear_model.LogisticRegression(max_iter=1000)\n",
      "# model = lm.fit(X_train, y_train)\n",
      "# predictions = lm.predict(X_test)\n",
      "72/134: X.dtypes\n",
      "72/135: y.head()\n",
      "72/136:\n",
      "X_N = X.dropna()\n",
      "y_n = y.loc[:,'Competency Avg']\n",
      "y_x = y_n.loc[X_N.index]\n",
      "y_nonN = y_x.dropna()\n",
      "X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "#     print(X_train.shape, y_train.shape)\n",
      "#     print(X_test.shape, y_test.shape)\n",
      "lin_model=sm.OLS(y_train,X_train)\n",
      "result=lin_model.fit()\n",
      "print(result.summary())\n",
      "72/137:\n",
      "X_N = X.dropna()\n",
      "for i in range(1,15):\n",
      "    print('')\n",
      "    print('Competency {}'.format(i))\n",
      "    y_n = y.loc[:,'Competency {}'.format(i)]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "#     print(X_train.shape, y_train.shape)\n",
      "#     print(X_test.shape, y_test.shape)\n",
      "    lin_model=sm.OLS(y_train,X_train)\n",
      "    result=lin_model.fit()\n",
      "    print(result.summary())\n",
      "72/138:\n",
      "X_N = X.dropna()\n",
      "for i in range(1,15):\n",
      "    print('')\n",
      "    print('Competency {}'.format(i))\n",
      "    y_n = y.loc[:,'Competency {}'.format(i)]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "    lm = linear_model.LinearRegression()\n",
      "    model = lm.fit(X_train, y_train)\n",
      "    predictions = lm.predict(X_test)    \n",
      "\n",
      "    print('Score:',model.score(X_test, y_test))\n",
      "\n",
      "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "    print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "    print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/139:\n",
      "y_n = y.loc[:,'Competency Avg']\n",
      "# few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "# round predictions for logistic regression\n",
      "y_x = y_n.loc[X_N.index]\n",
      "y_nonN = y_x.dropna()\n",
      "X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "lm = linear_model.LinearRegression()\n",
      "model = lm.fit(X_train, y_train)\n",
      "predictions = lm.predict(X_test)    \n",
      "\n",
      "print('Score:',model.score(X_test, y_test))\n",
      "\n",
      "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/140:\n",
      "comp_col=['Ability to Influence Prof', 'Critical Thinking Prof', 'Customer Service Prof', \n",
      "          'Oral Communication Prof', 'Problem Solving Prof', 'Written Communication Prof']\n",
      "\n",
      "comps = vs[comp_col]\n",
      "comps['Competency Avg']=comps.mean(axis=1)\n",
      "\n",
      "# comps.head()\n",
      "y = comps.copy(deep=True)\n",
      "\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of Exp (Gov)','Years of Exp (Ind)', \n",
      "             'Major Programs commodities', 'Construction commodities', \n",
      "             'A&E commodities', 'Facilities commodities', 'Schedules commodities',\n",
      "             'FAC Related % Time']\n",
      "\n",
      "features=vs[feature_col]\n",
      "X=features.copy(deep=True)\n",
      "\n",
      "# features.head()\n",
      "72/141:\n",
      "X_N = X.dropna()\n",
      "for i in comps.columns:\n",
      "    print('')\n",
      "    print(i)\n",
      "    y_n = y.loc[:,i]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "    lm = linear_model.LinearRegression()\n",
      "    model = lm.fit(X_train, y_train)\n",
      "    predictions = lm.predict(X_test)    \n",
      "\n",
      "    print('Score:',model.score(X_test, y_test))\n",
      "\n",
      "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "    print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "    print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "72/142:\n",
      "X_N = X.dropna()\n",
      "for i in comps.columns:\n",
      "    print('')\n",
      "    print(i)\n",
      "    y_n = y.loc[:,i]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "#     print(X_train.shape, y_train.shape)\n",
      "#     print(X_test.shape, y_test.shape)\n",
      "    lin_model=sm.OLS(y_train,X_train)\n",
      "    result=lin_model.fit()\n",
      "    print(result.summary())\n",
      "72/143:\n",
      "# Ages\n",
      "agemap={'25 Years Old and Under': 0,'26-30': 1,'31-35': 2,'36-40': 3,'41-45': 4,'46-50': 5,\n",
      "     '51-55': 6,'56-60': 7,  'Over 60 Years Old':8}\n",
      "\n",
      "# Education Level \n",
      "# Set 'other' to null\n",
      "edmap= {\"Associate's Degree\": 1,\"Bachelor's Degree\": 2,'Doctoral Degree': 4,'High School/GED': 0,\n",
      " \"Master's Degree\": 3,'Other': np.nan}\n",
      "\n",
      "# FAC-C Level\n",
      "facmap= {'In Progress Level 1':0, 'Level 1':1, 'Level 2':2, 'Level 3':3}\n",
      "\n",
      "# GS Level \n",
      "gsmap={'Intermediate Level (GS 8 - 12)':1, 'Expert Level (GS 13 - SES)':2,\n",
      "       'Entry Level (GS 1 - 7)':0, 'FS-1':2,'FS-2':2, 'FS-3':1, 'FS-4':1,\n",
      "       'FS-5':1, 'Other':np.nan }\n",
      "\n",
      "# Experience in government\n",
      "expmap_gov={'10-20 Years':4, '20 or More Years':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# Experience in industry\n",
      "expmap_ind={'10-20 Years':4, '20 Years or More':5, '5-10 Years':3, '3-5 Years':2,\n",
      "       'Less than 1 Year':0, '1-3 Years':1}\n",
      "\n",
      "# FAC-C %\n",
      "facpct = {'0%-25%':0, '26%-50%':1, '51%-75%':2, '76%-100%':3}\n",
      "\n",
      "# Commodities\n",
      "IT_comm = {'No':0, 'Yes':1}\n",
      "serv_comm = {'No':0, 'Yes':1}\n",
      "MP_comm = {'No':0, 'Yes':1}\n",
      "con_comm = {'No':0, 'Yes':1}\n",
      "AE_comm = {'No':0, 'Yes':1}\n",
      "fac_comm = {'No':0, 'Yes':1}\n",
      "sch_comm = {'No':0, 'Yes':1}\n",
      "72/144:\n",
      "comp_col=['Ability to Influence Prof', 'Critical Thinking Prof', 'Customer Service Prof', \n",
      "          'Oral Communication Prof', 'Problem Solving Prof', 'Written Communication Prof']\n",
      "\n",
      "comps = filt_df[comp_col]\n",
      "comps['Competency Avg']=comps.mean(axis=1)\n",
      "\n",
      "# comps.head()\n",
      "y = comps.copy(deep=True)\n",
      "\n",
      "feature_col=['Age',\n",
      " 'Education Level','FAC-C Level','GS Level','Years of experience (government)','Years of experience (industry)', \n",
      "             'IT commodities', 'Services commodities', 'Major Programs commodities', 'Construction commodities', \n",
      "             'A&E commodities', 'Facilities commodities', 'Schedules commodities',\n",
      "             'Overall, what percentage of your time is dedicated to FAC-related activities?']\n",
      "\n",
      "features=filt_df[feature_col]\n",
      "X=features.copy(deep=True)\n",
      "X.rename(columns={'Overall, what percentage of your time is dedicated to FAC-related activities?':'FAC Related % Time'},inplace=True)\n",
      "\n",
      "X.replace({'Age':agemap,'Education Level':edmap, 'FAC-C Level':facmap, 'GS Level':gsmap, \n",
      "                  'Years of experience (government)':expmap_gov, 'Years of experience (industry)':expmap_ind,\n",
      "          'IT commodities':IT_comm, 'Services commodities':serv_comm, 'Major Programs commodities':MP_comm, \n",
      "           'Construction commodities': con_comm, 'A&E commodities':AE_comm, 'Facilities commodities':fac_comm, \n",
      "           'Schedules commodities': sch_comm,\n",
      "           'FAC Related % Time': facpct},inplace=True)\n",
      "\n",
      "# features.head()\n",
      "72/145: X.head()\n",
      "72/146:\n",
      "X_N = X.dropna()\n",
      "for i in comps.columns:\n",
      "    print('')\n",
      "    print(i)\n",
      "    y_n = y.loc[:,i]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "#     print(X_train.shape, y_train.shape)\n",
      "#     print(X_test.shape, y_test.shape)\n",
      "    lin_model=sm.OLS(y_train,X_train)\n",
      "    result=lin_model.fit()\n",
      "    print(result.summary())\n",
      "72/147:\n",
      "X_N = X.dropna()\n",
      "for i in comps.columns:\n",
      "    print('')\n",
      "    print(i)\n",
      "    y_n = y.loc[:,i]\n",
      "    # few nulls relative to data, so drop rather than impute\n",
      "#     print(y.head(1))\n",
      "\n",
      "#     is_NaN = X.isnull()\n",
      "#     row_has_NaN = is_NaN.any(axis=1)\n",
      "#     rows_with_NaN = X[row_has_NaN]\n",
      "#     # print(rows_with_NaN)\n",
      "\n",
      "    # round predictions for logistic regression\n",
      "    y_x = y_n.loc[X_N.index]\n",
      "    y_nonN = y_x.dropna()\n",
      "    X_nonN = X_N.loc[y_nonN.index]\n",
      "\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X_nonN, y_nonN, test_size=0.2)\n",
      "\n",
      "    lm = linear_model.LinearRegression()\n",
      "    model = lm.fit(X_train, y_train)\n",
      "    predictions = lm.predict(X_test)    \n",
      "\n",
      "    print('Score:',model.score(X_test, y_test))\n",
      "\n",
      "    print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions))  \n",
      "    print('Mean Squared Error:', metrics.mean_squared_error(y_test, predictions))  \n",
      "    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))\n",
      "    print('Explained Variance Score:', metrics.explained_variance_score(y_test, predictions))\n",
      "    print('r squared:', metrics.r2_score(y_test,predictions))\n",
      "73/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/logistic_regression_class/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "#print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "73/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "#df = pd.read_csv(path + 'data_1d.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "#print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "73/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "#print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "73/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "73/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "73/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#df.columns = ['processor','count', 'year', 'OEM', 'field1', 'field2']\n",
      "#df['year'].astype(float)\n",
      "\n",
      "print(df.head())\n",
      "print(df.info())\n",
      "print(df.summary())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "X = np.array[100,2]\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "X = np.random.rand[n,d]\n",
      "print(X)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "X = np.random.rand(n,d)\n",
      "print(X)\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "#Y = \n",
      "print(X.type())\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "#Y = \n",
      "print(X.head())\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "#Y = \n",
      "print(head(X))\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "ones = np.array([[1]*n]*T)\n",
      "\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "ones = np.array([[1]*n].T)\n",
      "\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "print(ones)\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(X[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "77/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(n,d+1)\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "joel = np.dot(Xb,d+1)\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "joel = np.dot(Xb,d+1)\n",
      "joel\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "joel = np.dot(Xb,d+1)\n",
      "joel\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/19:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,d+1)\n",
      "z.shape()\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/20:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#Y = \n",
      "print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z)\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/21:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "\n",
      "def sigmoid(z):\n",
      "    return(1/1 + np.exp(-z))\n",
      "\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/22:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/1 + np.exp(-a))\n",
      "\n",
      "print(sigmoid(z))\n",
      "# print(df.head())\n",
      "# print(df.info())\n",
      "77/23:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/1 + np.exp(-a))\n",
      "\n",
      "print(sigmoid(z))\n",
      "\n",
      "plt.scatter(sigmoid(z))\n",
      "plt.show()\n",
      "77/24:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z)\n",
      "def sigmoid(a):\n",
      "    return(1/1 + np.exp(-a))\n",
      "\n",
      "print(sigmoid(z))\n",
      "77/25:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z[:5])\n",
      "def sigmoid(a):\n",
      "    return(1/1 + np.exp(-a))\n",
      "\n",
      "print(sigmoid(z))\n",
      "77/26:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z[:5])\n",
      "\n",
      "# now apply the sigmoid function to z\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from odds (p/(1-p)) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "print(sigmoid(z))\n",
      "77/27:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "# now create our bias term of 1s, this will look funny but it's because an array is one dimensional but we need it to be 2D.\n",
      "# this way it will have n rows and 1 column\n",
      "ones = np.array([[1]*n]).T\n",
      "\n",
      "# now concatenate ones with X\n",
      "Xb = np.concatenate((ones,X),axis = 1)\n",
      "\n",
      "#print(Xb[0:5,:])\n",
      "\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z[:5])\n",
      "\n",
      "# now apply the sigmoid function to z\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from odds (p/(1-p)) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "print(sigmoid(z))\n",
      "79/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "\n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#m = df.values\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    return m\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    return(m)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    return(m)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    return(m)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    #return(m)\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    #return(m)\n",
      "\n",
      "print(df.info())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    #return(m)\n",
      "\n",
      "joel = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "print(joel.info())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    #return(m)\n",
      "\n",
      "joel = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "print(joel.info())\n",
      "print(joel.head())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    #return(m)\n",
      "\n",
      "joel = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "print(joel.info())\n",
      "print(joel.head())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean)\n",
      "\n",
      "#joel = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#print(joel.info())\n",
      "#print(joel.head())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "#joel = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "#print(joel.info())\n",
      "#print(joel.head())\n",
      "    \n",
      "    \n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "79/17: from process import get_binary_data\n",
      "80/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "80/2: from process import get_binary_data\n",
      "80/3:\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/4:\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents\\Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/5:\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/6:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "80/8:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "80/10:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "#from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/11:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/12:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "80/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "80/14:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X, Y = get_binary_data()\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "80/15:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y = get_binary_data()\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "80/16:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X = get_binary_data()\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/17:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/18:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(X.shape())\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/19:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/20:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "D2 = X.shape[2]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/21:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "D0 = X.shape[0]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/22:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/23:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(X.type())\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/24:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(type(X))\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(D)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/25:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(type(X))\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print(X.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/26:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(type(X))\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2 shape:',X2.shape)\n",
      "print('Y2 shape:',Y2.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/27:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "print(type(X))\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/28:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"score:\", classification_rate(Y,predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "80/29:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "P_Y2test_given_X2test = forward(X2test,W,b)\n",
      "predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/30:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "P_Y2test_given_X2test = forward(X2test,W,b)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/31:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "#P_Y2test_given_X2test = forward(X2test,W,b)\n",
      "#test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "#print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/32:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "P_Y2test_given_X2test = forward(X2test,W,b)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/33:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/34:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# 316 observations in X/Y (train)\n",
      "# 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "print(X.shape[0])\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/35:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y,test_predictions))\n",
      "80/36:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "# bias term\n",
      "b = 0\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,2)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "80/37:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test,2)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "80/38:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "80/39:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "80/40:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "80/41:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "82/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n/2,:] = X[:n/2,:] - 2*np.ones((n/2,d))\n",
      "\n",
      "# this way it will have n rows and 1 column\n",
      "82/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:50,:] = X[:50,:] - 2*np.ones((50,d))\n",
      "\n",
      "# this way it will have n rows and 1 column\n",
      "82/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "half = n/2\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:half,:] = X[:50,:] - 2*np.ones((50,d))\n",
      "\n",
      "# this way it will have n rows and 1 column\n",
      "82/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "half = n/2\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:50,:] - 2*np.ones((50,d))\n",
      "\n",
      "# this way it will have n rows and 1 column\n",
      "82/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# this way it will have n rows and 1 column\n",
      "82/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*n//2 + [1]*n//2)\n",
      "T\n",
      "82/7:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*n//2)\n",
      "T\n",
      "82/8:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*50)\n",
      "T\n",
      "82/9:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*50 + [1]*50)\n",
      "T\n",
      "82/10:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*50)\n",
      "T\n",
      "82/11:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "T\n",
      "82/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [3]*(n//2))\n",
      "T\n",
      "82/13:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "X\n",
      "82/14:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "X\n",
      "82/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "X.head()\n",
      "82/16:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "head(X)\n",
      "82/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "X[:5,:]\n",
      "82/18:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(Xb,w)\n",
      "print(z[:5])\n",
      "82/19:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "print(z[:5])\n",
      "82/20:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "print(z[:5])\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "print(sigmoid(z))\n",
      "82/21:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "print(sigmoid(z))\n",
      "82/22:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/23:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    error = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(error))\n",
      "\n",
      "cross_entropy(T,Y)\n",
      "82/24:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e e += -(T[i,0]*np.log(Y[i,0])+ (1-T[i,0])*np.log(1-Y[i,0]))\n",
      "    \n",
      "\n",
      "cross_entropy(T,Y)\n",
      "82/25:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i,0]*np.log(Y[i,0])+ (1-T[i,0])*np.log(1-Y[i,0]))\n",
      "    \n",
      "\n",
      "cross_entropy(T,Y)\n",
      "82/26:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i,0])+ (1-T[i])*np.log(1-Y[i,0]))\n",
      "    \n",
      "\n",
      "cross_entropy(T,Y)\n",
      "82/27:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "\n",
      "cross_entropy(T,Y)\n",
      "82/28:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e2 = 0\n",
      "    for i in range(n):\n",
      "        e2 += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e, e2\n",
      "    \n",
      "cross_entropy(T,Y)\n",
      "82/29:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e2 = 0\n",
      "    for i in range(n):\n",
      "        e2 += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e, e2\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/30:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e2 = 0\n",
      "    for i in range(n):\n",
      "        e2 += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return (e, e2)\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/31:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e2 = 0\n",
      "    for i in range(n):\n",
      "        e2 += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/32:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e2 = 0\n",
      "    for i in range(n):\n",
      "        e2 += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e2\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/33:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print e\n",
      "82/34:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/35:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "print(e)\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/37:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/38:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/39:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/40:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/41:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/42:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/43:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/44:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/45:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/46:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/47:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/48:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/49:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/50:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/51:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/52:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/53:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/54:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/55:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/56:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/57:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/58:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "for i in nrange(100):\n",
      "    print(cross_entropy(T,Y))\n",
      "82/59:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "for i in nrange(100):\n",
      "    print(cross_entropy(T,Y))\n",
      "82/60:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "for i in range(100):\n",
      "    print(cross_entropy(T,Y))\n",
      "82/61:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "for i in range(100):\n",
      "    print(cross_entropy(T,Y))\n",
      "82/62:\n",
      "# this first part might just return the last value? It isn't summing?\n",
      "e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "print(np.sum(e))\n",
      "\n",
      "# instead, sum them all?\n",
      "e = 0\n",
      "for i in range(n):\n",
      "    e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "print(e)\n",
      "82/63:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "82/64:\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "for i in range(100):\n",
      "    print(cross_entropy(T,Y))\n",
      "82/65:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/66:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/67:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/68:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "    \n",
      "print(cross_entropy(T,Y))\n",
      "82/69:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors:\n",
      "    z = np.dot(X,w)\n",
      "82/70:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    z = np.dot(X,w)\n",
      "82/71:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        print(cross_entropy(T,Y))\n",
      "\n",
      "lots_of_errors()\n",
      "82/72:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        print(cross_entropy(T,Y))\n",
      "\n",
      "lots_of_errors()\n",
      "82/73:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "82/74:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/75:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/76:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(summary(results))\n",
      "82/77:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "results.type()\n",
      "82/78:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #print(cross_entropy(T,Y))\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "type(results)\n",
      "82/79:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        return(cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/80:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #return(cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/81:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #return(cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "results\n",
      "82/82:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #return(cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(results)\n",
      "82/83:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/84:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "82/85:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        #return(cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(results)\n",
      "82/86:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(results)\n",
      "82/87:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(e)\n",
      "82/88:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "print(e)\n",
      "82/89:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/90:\n",
      "total = np.array[]\n",
      "total = np.concatenate((total,[1]),axis = 1)\n",
      "total\n",
      "82/91:\n",
      "total = np.array([])\n",
      "total = np.concatenate((total,[1]),axis = 1)\n",
      "total\n",
      "82/92:\n",
      "total = np.array([])\n",
      "total = np.concatenate((total,[1]))\n",
      "total\n",
      "82/93:\n",
      "total = np.array([])\n",
      "total = np.concatenate((total,[1]))\n",
      "total = np.concatenate((total,[13]))\n",
      "total\n",
      "82/94:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/95:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        print(e)\n",
      "#        error_array = np.concatenate(error_array,e)\n",
      "#        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "#results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/96:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        print(e)\n",
      "#        error_array = np.concatenate(error_array,e)\n",
      "#        return error_array\n",
      "        #find out how to concatenate value?\n",
      "lots_of_errors()\n",
      "#results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/97:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "#        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "#results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/98:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "#        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "#results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/99:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/100:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "82/101:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "#results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/102:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/103:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array()\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/104:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.concatenate(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/105:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "#print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/106:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,e)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/107:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/108:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "        return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/109:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "print(results)\n",
      "#max_error = np.max(results)\n",
      "#min_error = np.min(results)\n",
      "#print(min_error,max_error)\n",
      "82/110:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array\n",
      "        #find out how to concatenate value?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/111:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,max_error)\n",
      "82/112:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,mean error, max_error)\n",
      "82/113:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(min_error,mean_error, max_error)\n",
      "82/114:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "82/115:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "82/116:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "82/117:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "82/118:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors(w):\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "results = lots_of_errors(w)\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "82/119:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors(X,w):\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "results = lots_of_errors(X,w)\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "82/120:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "82/121:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "82/122:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "plt.show()\n",
      "82/123:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1])\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/124:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/125:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/126:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/127:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 2)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/128:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[:5,:])\n",
      "82/129:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[95:,:])\n",
      "82/130:\n",
      "# Not sure why mine isn't visualizing right...\n",
      "\n",
      "N = 100\n",
      "D = 2\n",
      "\n",
      "\n",
      "X = np.random.randn(N,D)\n",
      "\n",
      "# center the first 50 points at (-2,-2)\n",
      "X[:50,:] = X[:50,:] - 2*np.ones((50,D))\n",
      "\n",
      "# center the last 50 points at (2, 2)\n",
      "X[50:,:] = X[50:,:] + 2*np.ones((50,D))\n",
      "\n",
      "# labels: first 50 are 0, last 50 are 1\n",
      "T = np.array([0]*50 + [1]*50)\n",
      "\n",
      "# add a column of ones\n",
      "# ones = np.array([[1]*N]).T\n",
      "ones = np.ones((N, 1))\n",
      "Xb = np.concatenate((ones, X), axis=1)\n",
      "\n",
      "def sigmoid(z):\n",
      "    return 1/(1 + np.exp(-z))\n",
      "\n",
      "# get the closed-form solution\n",
      "w = np.array([0, 4, 4])\n",
      "\n",
      "# calculate the model output\n",
      "z = Xb.dot(w)\n",
      "Y = sigmoid(z)\n",
      "\n",
      "plt.scatter(X[:,0], X[:,1], c=T, s=100, alpha=0.5)\n",
      "\n",
      "x_axis = np.linspace(-6, 6, 100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis, y_axis)\n",
      "plt.show()\n",
      "82/131:\n",
      "# Not sure why mine isn't visualizing right...\n",
      "\n",
      "N = 100\n",
      "D = 2\n",
      "\n",
      "\n",
      "X = np.random.randn(N,D)\n",
      "\n",
      "# center the first 50 points at (-2,-2)\n",
      "X[:50,:] = X[:50,:] - 2*np.ones((50,D))\n",
      "\n",
      "# center the last 50 points at (2, 2)\n",
      "X[50:,:] = X[50:,:] + 2*np.ones((50,D))\n",
      "print(X[:5,])\n",
      "# labels: first 50 are 0, last 50 are 1\n",
      "T = np.array([0]*50 + [1]*50)\n",
      "\n",
      "# add a column of ones\n",
      "# ones = np.array([[1]*N]).T\n",
      "ones = np.ones((N, 1))\n",
      "Xb = np.concatenate((ones, X), axis=1)\n",
      "\n",
      "def sigmoid(z):\n",
      "    return 1/(1 + np.exp(-z))\n",
      "\n",
      "# get the closed-form solution\n",
      "w = np.array([0, 4, 4])\n",
      "\n",
      "# calculate the model output\n",
      "z = Xb.dot(w)\n",
      "Y = sigmoid(z)\n",
      "\n",
      "plt.scatter(X[:,0], X[:,1], c=T, s=100, alpha=0.5)\n",
      "\n",
      "x_axis = np.linspace(-6, 6, 100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis, y_axis)\n",
      "plt.show()\n",
      "82/132:\n",
      "# Not sure why mine isn't visualizing right...\n",
      "\n",
      "N = 100\n",
      "D = 2\n",
      "\n",
      "\n",
      "X = np.random.randn(N,D)\n",
      "\n",
      "# center the first 50 points at (-2,-2)\n",
      "#X[:50,:] = X[:50,:] - 2*np.ones((50,D))\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "\n",
      "# center the last 50 points at (2, 2)\n",
      "#X[50:,:] = X[50:,:] + 2*np.ones((50,D))\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# labels: first 50 are 0, last 50 are 1\n",
      "T = np.array([0]*50 + [1]*50)\n",
      "\n",
      "# add a column of ones\n",
      "# ones = np.array([[1]*N]).T\n",
      "ones = np.ones((N, 1))\n",
      "Xb = np.concatenate((ones, X), axis=1)\n",
      "\n",
      "def sigmoid(z):\n",
      "    return 1/(1 + np.exp(-z))\n",
      "\n",
      "# get the closed-form solution\n",
      "w = np.array([0, 4, 4])\n",
      "\n",
      "# calculate the model output\n",
      "z = Xb.dot(w)\n",
      "Y = sigmoid(z)\n",
      "\n",
      "plt.scatter(X[:,0], X[:,1], c=T, s=100, alpha=0.5)\n",
      "\n",
      "x_axis = np.linspace(-6, 6, 100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis, y_axis)\n",
      "plt.show()\n",
      "82/133:\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/134:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "X[:5,:]\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/135:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/136:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "82/137:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[95:,:])\n",
      "83/2:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "83/3:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "83/4:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/5: # now let's do gradient descent (covered in logistic3)\n",
      "83/6:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/7:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[95:,:])\n",
      "83/8:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/9:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/10: # now let's do gradient descent (covered in logistic3)\n",
      "83/11:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "83/12:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "# plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/13:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/14:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.rand(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "83/15:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.randn(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "83/16:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "83/17:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "83/18:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/19:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/20:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/21:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "\n",
      "plt.scatter(X[:,0],X[:,1],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "84/1:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "84/2:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "84/3:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.randn(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "84/4:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "84/5:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "84/6:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "print(X[:5,:])\n",
      "print(X[95:,:])\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/22:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.randn(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "83/23:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "83/24:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "83/25:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/26:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    i % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w += learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/27:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if i % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w += learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/28:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w += learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/29:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/30:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/31:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w = np.sum(w, learning_rate * np.dot((T-Y).T,X))\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/32:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/33:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/34:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(j, cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/35:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "n = 100 # rows\n",
      "d = 2 # dimensions/columns\n",
      "\n",
      "# make a normally distributed data matrix\n",
      "X = np.random.randn(n,d)\n",
      "\n",
      "# set first half of the points to be centered at X = -2, Y = -2\n",
      "X[:n//2,:] = X[:n//2,:] - 2*np.ones((n//2,d))\n",
      "# need // to make sure it's an integer; / returns a float\n",
      "# center the second half at 2\n",
      "X[n//2:,:] = X[n//2:,:] + 2*np.ones((n//2,d))\n",
      "\n",
      "# now create targets, first 50 set to 0, second 50 set to 1. This should be easy to predict!\n",
      "T = np.array([0]*(n//2) + [1]*(n//2))\n",
      "\n",
      "# now concatenate ones with X to add in the bias term\n",
      "ones = ones = np.array([[1]*n]).T\n",
      "X = np.concatenate((ones,X),axis = 1)\n",
      "83/36:\n",
      "# now randomly initialize a weight vector, needs d+1 dimensions to include the bias term\n",
      "\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "# first thing - calculate dot product of each row of x with w\n",
      "\n",
      "z = np.dot(X,w)\n",
      "\n",
      "# Remember: this is just a linear equation: z = mx + b\n",
      "# But z = the log odds of a \"success\"\n",
      "# Applying the sigmoid function is a fancy way of going from log odds ln((p/(1-p))) to probability\n",
      "# Probability of success = 1 / (1 + e^-z)\n",
      "# That is how we go from a linear estimate of log odds to a probability\n",
      "\n",
      "def sigmoid(a):\n",
      "    return(1/(1 + np.exp(-a)))\n",
      "\n",
      "Y = sigmoid(z)\n",
      "\n",
      "# Now we need to calculate our cross entropy error (which we need to minimize)\n",
      "def cross_entropy(T,Y):\n",
      "    # this first part might just return the last value? It isn't summing?\n",
      "    # e = -(T*np.log(Y)+ (1-T)*np.log(1-Y))\n",
      "    # return(np.sum(e))\n",
      "    # instead, sum them all?\n",
      "    e = 0\n",
      "    for i in range(n):\n",
      "        e += -(T[i]*np.log(Y[i])+ (1-T[i])*np.log(1-Y[i]))\n",
      "    return e\n",
      "\n",
      "print(cross_entropy(T,Y))\n",
      "83/37:\n",
      "# I get some varied results because of these random weights, anywhere from 2 to 400? Let's run 100:\n",
      "\n",
      "def lots_of_errors():\n",
      "    error_array = np.array([])\n",
      "    for i in range(100):\n",
      "        w = np.random.randn(d+1)\n",
      "        z = np.dot(X,w)\n",
      "        Y = sigmoid(z)\n",
      "        single_error = cross_entropy(T,Y)\n",
      "        error_array = np.append(error_array,single_error)\n",
      "    return error_array # at first I had this indented in the loop, it would only return the last one?\n",
      "\n",
      "results = lots_of_errors()\n",
      "\n",
      "max_error = np.max(results)\n",
      "mean_error = np.mean(results)\n",
      "min_error = np.min(results)\n",
      "print(np.round(min_error,2),np.round(mean_error,2),np.round(max_error,2))\n",
      "83/38:\n",
      "# now let's visualize\n",
      "# not sure what it's supposed to mean when we say we know the \"closed form solution\", but he says:\n",
      "# get the closed form solution:\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "# what it probably means is that we need a bias term of 0\n",
      "# then because the first 50 are centered at -2,-2 with an output of 0,\n",
      "# and the second 50 are centered at 2,2 with an output of 1,\n",
      "# then we need to multiply the weights by 4?\n",
      "\n",
      "# You will recall that the equation of a line is y = mx + b (or x2 = m*x1 + b)\n",
      "# In dot product form it's 0 = (0, 4, 4) dot (1, x1, x2)\n",
      "# Therefore 0 = x1 + x2\n",
      "# x2 = -x1\n",
      "# OR\n",
      "# y = -x\n",
      "# Therefore, for every x point, the corresponding y is the negative of that value.\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.scatter(X[:,1],X[:,2],c=T, s=100, alpha = 0.5)\n",
      "x_axis = np.linspace(-6,6,100)\n",
      "y_axis = -x_axis\n",
      "plt.plot(x_axis,y_axis)\n",
      "plt.show()\n",
      "\n",
      "# why does this work sometimes and not work sometimes?\n",
      "83/39:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(j, cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/40:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(j, cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/41:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(j, cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/42:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 1 == 0:\n",
      "        print(j, cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/43:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/44:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/45:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/46:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    #w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    w = w + learning_rate * X.T.dot(T - Y)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/47:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/48:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "#w = np.array([0,4,4])\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/49:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "#w = np.array([0,4,4])\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/50:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "#w = np.array([0,4,4])\n",
      "w = np.random.randn(d+1)\n",
      "\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/51:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "83/52:\n",
      "# now let's do gradient descent (covered in logistic3)\n",
      "\n",
      "learning_rate = .1\n",
      "w = np.array([0,4,4])\n",
      "\n",
      "for j in range(100):\n",
      "    if j % 10 == 0:\n",
      "        print(\"interation\", j, \":\", cross_entropy(T,Y))\n",
      "    w = w + learning_rate * np.dot((T-Y).T,X)\n",
      "    Y = sigmoid(np.dot(X,w))\n",
      "\n",
      "print(\"final weight:\",w)\n",
      "85/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "85/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from sklearn.utils import shuffle\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents//Udemy/machine_learning_examples/ann_logistic_extra/'\n",
      "\n",
      "# is mobile - whether user is visiting site on a mobile device (1)\n",
      "# n products viewed - integer indicating how many products user viewed during visit\n",
      "# visit duration - how long in minutes user was on site\n",
      "# returning vistor - 0 if new, 1 if returning\n",
      "# time of day, split into 6 hour increments 0 = midnight - 6AM, 1, 2, 3 = 6 PM to midnight\n",
      "# user action\n",
      "    # 1 = bounce/user left site\n",
      "    # 2 = add to cart\n",
      "    # 3 = being checkout\n",
      "    # 4 = finish checkout\n",
      "    \n",
      "# this time we're going to write a function to get the data\n",
      "\n",
      "def get_data():\n",
      "    df = pd.read_csv(path + 'ecommerce_data.csv', header = None)\n",
      "    print(df.info())\n",
      "    # turn it into a numpy matrix\n",
      "    m = df.values\n",
      "    # y is last column, x is everything up to the last column\n",
      "    x = m[:,:-1]\n",
      "    y = m[:,-1]\n",
      "    \n",
      "    # normalize the numerical columns\n",
      "    # second column (n products viewed) and third column (visit duration)\n",
      "    x[:,1] = (x[:,1] - x[:,1].mean())/x[:,1].std()\n",
      "    x[:,2] = (x[:,2] - x[:,2].mean())/x[:,2].std()\n",
      "\n",
      "    # now work on time of day (fifth column), make a new X because we need to go from one categorical values to four\n",
      "    N, D = x.shape\n",
      "    X2 = np.zeros((N,D+3))\n",
      "    # most of x is going to be the same, everything through the D-1 column\n",
      "    x2[:,0:(D-1)] = x[:0:(D-1)]\n",
      "    # one hot encoding for the other four columns\n",
      "    for n in range(N):\n",
      "        t = int(X[n,D-1]) # identifies the value of time of day\n",
      "        x2[n,D+t-1] = 1 # sets the correct column to 1\n",
      "    return x2, y\n",
      "\n",
      "# user action will be (filtered?) to be only include 1 and 2, we'll ignore 3 and 4\n",
      "\n",
      "def get_binary_data():\n",
      "    X,Y = getdata()\n",
      "    # filter to 0 and 1 for Y\n",
      "    X2 = X[Y <= 1]\n",
      "    Y2 = Y[Y <= 1]\n",
      "    return X2, Y2\n",
      "\n",
      "#print(df.head())\n",
      "#print(df.info())\n",
      "\n",
      "#print(m[:,0])\n",
      "\n",
      "#x = m[:,0]\n",
      "#y = m[:,1]\n",
      "85/3:\n",
      "# path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Udemy/3_Logistic_Regression/'\n",
      "\n",
      "# this is from process.py\n",
      "# it appears to redo the code we wrote above, but also splits it into test and training...\n",
      "\n",
      "from process import get_binary_data\n",
      "\n",
      "# call the function and assign it to the variables X and Y\n",
      "X,Y,X2test,Y2test = get_binary_data()\n",
      "\n",
      "# looks like get_binary_data does all the dirty processing work and splits into train/test\n",
      "# about 316 observations in X/Y (train)\n",
      "# about 82 observations in X2test/Y2test (test)\n",
      "\n",
      "# get the dimensionality of the data set\n",
      "D = X.shape[1]\n",
      "# then set some random weights based on that dimensionality\n",
      "W = np.random.randn(D)\n",
      "b = 0 # bias term\n",
      "\n",
      "print('X shape:',X.shape)\n",
      "print('Y shape:',Y.shape)\n",
      "print('X2test shape:',X2test.shape)\n",
      "print('Y2test shape:',Y2test.shape)\n",
      "\n",
      "# write functions to do our predictions\n",
      "def sigmoid(x):\n",
      "    return 1/(1 + np.exp(-x))\n",
      "\n",
      "def forward(X,W,b):\n",
      "    return sigmoid(np.dot(X,W) + b)\n",
      "\n",
      "P_Y_given_X = forward(X,W,b)\n",
      "training_predictions = np.round(P_Y_given_X,0)\n",
      "\n",
      "# now determine classification rate:\n",
      "def classification_rate(Y,P):\n",
      "    return np.mean(Y==P)\n",
      "\n",
      "print(\"training score:\", classification_rate(Y,training_predictions))\n",
      "\n",
      "# no surprise, with randomized weights the accuracy is low\n",
      "\n",
      "# can we repeat with test?\n",
      "D2 = X2test.shape[1]\n",
      "W2 = np.random.randn(D2)\n",
      "b2 = 0\n",
      "P_Y2test_given_X2test = forward(X2test,W2,b2)\n",
      "test_predictions = np.round(P_Y2test_given_X2test)\n",
      "print(\"test score:\", classification_rate(Y2test,test_predictions))\n",
      "90/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# try te replicate\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Clockwork_Files/Joel_Work/Udemy/machine_learning_examples/linear_regression_class/'\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# x1 = systolic blood pressure\n",
      "# x2 = age in years\n",
      "# x3 = weight in pounds\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "plt.scatter(xdata,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "90/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "m = df.values\n",
      "\n",
      "# x1 = systolic blood pressure\n",
      "# x2 = age in years\n",
      "# x3 = weight in pounds\n",
      "\n",
      "xdata = m[:,1:2]\n",
      "y = m[:,0]\n",
      "\n",
      "plt.scatter(xdata,y)\n",
      "plt.show()\n",
      "\n",
      "# looks kind of quadratic/exponential\n",
      "90/3:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/4:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "group_list = df.Group.unique\n",
      "print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/5:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/6:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "90/7:\n",
      "header_list = df.iloc[0]\n",
      "print(header_list)\n",
      "\n",
      "#group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/8:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "print(df.head())\n",
      "\n",
      "#group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/9:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "#group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/10:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/11:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/12:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "90/13:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "#group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/14:\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/15:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/16:\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/17:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "print(df.info())\n",
      "\n",
      "print(df.head())\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "print(header_list)\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/18:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'gao_protest_outcomes.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/19:\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/20:\n",
      "# create crosstab of group vs period\n",
      "crosstab = df.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "print(crosstab)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/21:\n",
      "# create crosstab of group vs period\n",
      "joel = df.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/22:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "pd.crosstab(group_list,period_list)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/23:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "pd.crosstab(df.Group,df.Period)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/24:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period)\n",
      "print(crosstab)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/25:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/26:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "count_crosstab = pd.crosstab(df.Group,df.Period)\n",
      "print(count_crosstab)\n",
      "\n",
      "sum_crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(sum_crosstab)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/27:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/28:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "count_crosstab = pd.crosstab(df.Group,df.Period)\n",
      "print(count_crosstab)\n",
      "\n",
      "sum_crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(sum_crosstab)\n",
      "\n",
      "#m = df.values\n",
      "\n",
      "#xdata = m[:,1:2]\n",
      "#y = m[:,0]\n",
      "\n",
      "#plt.scatter(xdata,y)\n",
      "#plt.show()\n",
      "90/29:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab.info())\n",
      "90/30:\n",
      "#\n",
      "for index, row in crosstab.head(n=2).iterrows():\n",
      "     print(index, row)\n",
      "90/31:\n",
      "#\n",
      "for index, row in crosstab.iterrows():\n",
      "     print(index, row)\n",
      "90/32:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/33:\n",
      "crosstab.plot()\n",
      "plt.show()\n",
      "#for index, row in crosstab.iterrows():\n",
      "#    print(index, row)\n",
      "#    plt.plot()\n",
      "90/34:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Period,df.Group, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/35:\n",
      "crosstab.plot()\n",
      "plt.show()\n",
      "#for index, row in crosstab.iterrows():\n",
      "#    print(index, row)\n",
      "#    plt.plot()\n",
      "90/36:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/37:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Period,df.Group, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/38:\n",
      "crosstab.plot()\n",
      "plt.show()\n",
      "#for index, row in crosstab.iterrows():\n",
      "#    print(index, row)\n",
      "#    plt.plot()\n",
      "90/39:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/40:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/41:\n",
      "crosstab.plot()\n",
      "plt.show()\n",
      "#for index, row in crosstab.iterrows():\n",
      "#    print(index, row)\n",
      "#    plt.plot()\n",
      "90/42:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/43:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot(kind = 'line')\n",
      "#print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/44:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot()\n",
      "#print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/45:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate_years.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/46:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot()\n",
      "#print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/47:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate_years.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/48:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "#crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot()\n",
      "\n",
      "# change objects to floats first\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot()\n",
      "crosstab = crosstab.apply(pd.to_numeric)\n",
      "crosstab.plot()\n",
      "#print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/49:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "#crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum').plot()\n",
      "\n",
      "# change objects to floats first\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "crosstab = crosstab.apply(pd.to_numeric)\n",
      "crosstab.plot()\n",
      "#print(crosstab)\n",
      "#print(crosstab.info())\n",
      "90/50:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/51:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "# change objects to floats first\n",
      "\n",
      "crosstab = pd.crosstab(df.Group,df.Period, values = df.Score, aggfunc = 'sum')\n",
      "crosstab = crosstab.apply(pd.to_numeric)\n",
      "crosstab.plot()\n",
      "90/52:\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/VA_OAL/Dashboard_Mockup/dashboard_practice/'\n",
      "\n",
      "df = pd.read_csv(path + 'facc_certification_rate.csv', header = None)\n",
      "\n",
      "header_list = df.iloc[0] #grab the first row for the header\n",
      "\n",
      "df = df[1:] #take the data less the header row\n",
      "df.columns = header_list #set the header row as the df header\n",
      "\n",
      "#print(df.head())\n",
      "print(df.info())\n",
      "90/53:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "# change objects to floats first\n",
      "\n",
      "crosstab = pd.crosstab(df.Period,df.Group, values = df.Score, aggfunc = 'sum')\n",
      "crosstab = crosstab.apply(pd.to_numeric)\n",
      "crosstab.plot()\n",
      "90/54:\n",
      "# create crosstab of group vs period\n",
      "#joel = pd.crosstab(Group,Period, rownames = ['Group'], colnames = ['Period'])\n",
      "#print(joel)\n",
      "\n",
      "# try doing it by hand for the heck of it...\n",
      "group_list = df.Group.unique()\n",
      "#print(group_list)\n",
      "\n",
      "period_list = df.Period.unique()\n",
      "\n",
      "# change objects to floats first\n",
      "\n",
      "crosstab = pd.crosstab(df.Period,df.Group, values = df.Score, aggfunc = 'sum')\n",
      "crosstab = crosstab.apply(pd.to_numeric)\n",
      "crosstab.plot()\n",
      "print(crosstab)\n",
      "91/1: import pandas as pd\n",
      "91/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pylot as plt\n",
      "91/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "91/4:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey'\n",
      "91/5:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = 1)\n",
      "91/6:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = 1)\n",
      "91/7:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = 1)\n",
      "\n",
      "responses.head()\n",
      "91/8:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = 1)\n",
      "\n",
      "responses.info()\n",
      "91/9:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = 0)\n",
      "\n",
      "responses.info()\n",
      "91/10:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "responses.info()\n",
      "91/11: length(responses)\n",
      "91/12: len(responses)\n",
      "91/13:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "len(qlist)\n",
      "91/14:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "len(qlist)\n",
      "91/15:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "yes_counts\n",
      "91/16:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in length(qlist):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/17:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(length(qlist))\n",
      "for i in length(qlist):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/18:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(qlist))\n",
      "for i in length(qlist):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/19:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in len(qlist):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/20:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in len(qlist):\n",
      "    yes_counts[0,i] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/21:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/22:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[0,1] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/23:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[0,i] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/24:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "\n",
      "yes_counts\n",
      "91/25:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "    print(qlist[i])\n",
      "\n",
      "\n",
      "#yes_counts\n",
      "91/26:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts(i,0) = qlist[i]\n",
      "    print(qlist[i])\n",
      "\n",
      "\n",
      "#yes_counts\n",
      "91/27:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "    print(qlist[i])\n",
      "\n",
      "#yes_counts\n",
      "91/28:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts[i,0] = qlist[i]\n",
      "    print(qlist[i])\n",
      "\n",
      "print(yes_counts)\n",
      "91/29:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_count_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.append(yes_counts_append)\n",
      "    \n",
      "print(yes_counts)\n",
      "91/30:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_count_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.join(yes_counts_append)\n",
      "    \n",
      "print(yes_counts)\n",
      "91/31:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_count_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "    \n",
      "print(yes_counts)\n",
      "91/32:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_count_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    print(yes_counts_append)\n",
      "91/33:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_count_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_count_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "91/34:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "91/35:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    #print(yes_counts_append)\n",
      "91/36:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "91/37:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],6])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "91/38:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "91/39:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i]], columns = ['Q'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/40:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i]], columns = ['Q'])\n",
      "    yes_counts.append(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/41:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i]], columns = ['Q'])\n",
      "    yes_counts.append(yes_counts_append)\n",
      "    print(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/42:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "#responses.info()\n",
      "91/43:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "91/44:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/45:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    #yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1])\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/46:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame()\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i]])\n",
      "    yes_counts.append(yes_counts_append)\n",
      "    print(yes_counts_append)\n",
      "    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/47:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame()\n",
      "\n",
      "appendme = pd.DataFrame([qlist[3]])\n",
      "yes_counts.append(appendme)\n",
      "\n",
      "#for i in range(len(qlist)):\n",
      "#    yes_counts_append = pd.DataFrame([qlist[i]])\n",
      "#    yes_counts.append(yes_counts_append)\n",
      "#    print(yes_counts_append)\n",
      "#    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/48:\n",
      "df = pd.DataFrame()\n",
      "data = pd.DataFrame({\"A\": range(3)})\n",
      "df.append(data)\n",
      "91/49:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame()\n",
      "\n",
      "appendme = pd.DataFrame({qlist[3]})\n",
      "yes_counts.append(appendme)\n",
      "\n",
      "#for i in range(len(qlist)):\n",
      "#    yes_counts_append = pd.DataFrame([qlist[i]])\n",
      "#    yes_counts.append(yes_counts_append)\n",
      "#    print(yes_counts_append)\n",
      "#    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/50:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame()\n",
      "\n",
      "appendme = pd.DataFrame({\"Q\": qlist[3]})\n",
      "yes_counts.append(appendme)\n",
      "\n",
      "#for i in range(len(qlist)):\n",
      "#    yes_counts_append = pd.DataFrame([qlist[i]])\n",
      "#    yes_counts.append(yes_counts_append)\n",
      "#    print(yes_counts_append)\n",
      "#    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/51:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame()\n",
      "\n",
      "appendme = pd.DataFrame({\"Q\": qlist(3)})\n",
      "yes_counts.append(appendme)\n",
      "\n",
      "#for i in range(len(qlist)):\n",
      "#    yes_counts_append = pd.DataFrame([qlist[i]])\n",
      "#    yes_counts.append(yes_counts_append)\n",
      "#    print(yes_counts_append)\n",
      "#    print('')\n",
      "\n",
      "print(yes_counts)\n",
      "91/52:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/53:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': 1, 'count': 2, ignore_index=True})\n",
      "    #    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/54:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': 1, 'count': 2}, ignore_index=True)\n",
      "    #    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/55:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': 1, 'count': 2}, ignore_index=True)\n",
      "    #    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/56:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': 2}, ignore_index=True)\n",
      "    #    yes_counts_append = pd.DataFrame([qlist[i],1], columns = ['Q','count'])\n",
      "    #yes_counts.concat(yes_counts_append)\n",
      "\n",
      "print(yes_counts)\n",
      "91/57:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': 2}, ignore_index=True)\n",
      "\n",
      "print(yes_counts.info())\n",
      "91/58:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': 2}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/59:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for i in range(len(responses)):\n",
      "            if responses[qlist[i]] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/60:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i][j]] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/61:\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/62:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(respondents)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': respondent[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/63:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': respondent[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/64:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/65:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "#    for j in range(len(qlist)):\n",
      "#            if responses[qlist[j]][i] == 'Yes':\n",
      "#                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/66:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "print(len(responses))\n",
      "#for i in range(len(responses)):\n",
      "#    total_yes = 0\n",
      "#    for j in range(len(qlist)):\n",
      "#            if responses[qlist[j]][i] == 'Yes':\n",
      "#                total_yes+=1\n",
      "#    yes_counts = yes_counts.append({'respondent': responses[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#print(yes_counts)\n",
      "91/67:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "#    for j in range(len(qlist)):\n",
      "#            if responses[qlist[j]][i] == 'Yes':\n",
      "#                total_yes+=1\n",
      "#    yes_counts = yes_counts.append({'respondent': responses[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#print(yes_counts)\n",
      "91/68:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "#    for j in range(len(qlist)):\n",
      "#            if responses[qlist[j]][i] == 'Yes':\n",
      "#                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#print(yes_counts)\n",
      "91/69:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#print(yes_counts)\n",
      "91/70:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "responses.info()\n",
      "91/71:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "#    for j in range(len(qlist)):\n",
      "#            if responses[qlist[j]][i] == 'Yes':\n",
      "#                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/72:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "print(yes_counts)\n",
      "91/73:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "91/74:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/75:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'])    \n",
      "    \n",
      "#print(yes_counts)\n",
      "91/76:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts)\n",
      "91/77:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts)\n",
      "91/78:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'])    \n",
      "    \n",
      "print(yes_counts)\n",
      "91/79:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'])    \n",
      "    \n",
      "#print(yes_counts)\n",
      "91/80:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts)\n",
      "91/81:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/82:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/83:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "binary_questions.head()\n",
      "91/84:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "#binary_questions.head()\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/85:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions.head()\n",
      "binary_questions.[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/86:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions.head()\n",
      "binary_questions.['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/87:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions.[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions.['Q3'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/88:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "#binary_questions['Q3'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/89:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/90:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "#binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/91:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/92:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/93:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.info()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/94:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3','Q5'].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.info()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/95:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.info()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/96:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/97:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/98:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/99:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.info()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/100:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/101:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/102:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/103:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/104:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/105:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/106:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/107:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3']].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/108:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/109:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/110:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/111:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3']].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/112:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/113:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/114:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/115:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions['Q3'].replace({\"Yes\":1,\"No\":0}, inplace = True)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/116:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/117:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/118:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/119:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/120:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/121:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/122:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/123:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/124:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "#binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "binary_questions[['Q3','Q5']].binary_questions[['Q3','Q5']].replace(\"Yes\":1)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/125:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/126:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/127:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/128:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "#binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "binary_questions[['Q3','Q5']].binary_questions[['Q3','Q5']].replace(\"Yes\",1)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/129:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/130:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/131:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/132:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "#binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "binary_questions[['Q3','Q5']] = binary_questions[['Q3','Q5']].replace(\"Yes\",1)\n",
      "\n",
      "binary_questions.head()\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/133:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/134:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/135:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/136:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "print(binary_questions.head())\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace({\"yes\":1,\"no\":0}, inplace = True)\n",
      "#binary_questions[['Q3','Q5']].replace({\"Yes\":1,\"No\":0})\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "\n",
      "print(binary_questions.head())\n",
      "\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/137:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/138:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/139:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/140:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions)\n",
      "91/141:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "91/142:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/143:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/144:\n",
      "# try k-means on binaries\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/145: kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16'])\n",
      "91/146:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/147:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/148:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/149:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "91/150:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset='Q1_1')\n",
      "91/151:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "91/152:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "\n",
      "return(responses)\n",
      "91/153:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "91/154:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/155:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/156:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/157:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/158:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/159:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "91/160:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/161:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/162:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/163:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses.dropna(subset=['Q1_1'])\n",
      "91/164:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "print(len(responses))\n",
      "91/165:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=[Q1_1])\n",
      "print(len(responses))\n",
      "91/166:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses.dropna(subset=['Q1_1'])\n",
      "print(len(responses))\n",
      "91/167:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/168:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(len(responses))\n",
      "91/169:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/170:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(len(responses))\n",
      "91/171:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/172:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/173:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/174:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/175:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(responses)\n",
      "91/176:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(responses.info())\n",
      "91/177:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/178:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(responses))\n",
      "#for i in range(len(qlist)):\n",
      "#    total_yes = 0\n",
      "#    for j in range(len(responses)):\n",
      "#            if responses[qlist[i]][j] == 'Yes':\n",
      "#                total_yes+=1\n",
      "#    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/179:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(responses.info())\n",
      "91/180:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(responses))\n",
      "#for i in range(len(qlist)):\n",
      "#    total_yes = 0\n",
      "#    for j in range(len(responses)):\n",
      "#            if responses[qlist[i]][j] == 'Yes':\n",
      "#                total_yes+=1\n",
      "#    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/181:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "91/182:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(responses))\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/183:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/184:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(responses))\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/185:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/186:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/187:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/188:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "print(len(responses))\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/189:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "#yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "#print(yes_counts_sorted)\n",
      "91/190:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/191:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/192:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/193:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/194:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/195:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/196:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/197:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/198:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/199:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "#responses = responses.dropna(subset=['Q1_1'])\n",
      "91/200:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/201:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/202:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "print(len(responses))\n",
      "91/203:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/204:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1'])\n",
      "\n",
      "print(responses)\n",
      "91/205:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(len(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(responses)\n",
      "91/206:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/207:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/208:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/209:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/210:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/211:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/212:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/213:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "91/214:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/215:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/216:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/217:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions.fillna(0)\n",
      "\n",
      "print(binary_questions)\n",
      "91/218:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/219:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions.fillna(0)\n",
      "\n",
      "print(binary_questions)\n",
      "91/220:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/221:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/222:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/223:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/224:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions.fillna(0)\n",
      "91/225:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/226:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/227:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/228:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/229:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/230:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "print(binary_questions)\n",
      "\n",
      "#kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/231:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/232:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "print(binary_questions['Q1_1'])\n",
      "91/233:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "binary_questions['Q1_1'] = kmeans.labels\n",
      "91/234:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "binary_questions['Q1_1'] = kmeans_binary.labels\n",
      "91/235:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "print(kmeans_binary.labels_)\n",
      "#binary_questions['Q1_1'] = kmeans_binary.labels\n",
      "91/236:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "print(kmeans_binary.labels_)\n",
      "kmeans_binary.labels_.info()\n",
      "91/237: print(binary_questions)\n",
      "91/238:\n",
      "print(binary_questions)\n",
      "binary_questions[cluster] = kmeans_binary.labels_\n",
      "91/239:\n",
      "binary_questions[cluster] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/240:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "print(kmeans_binary.labels_)\n",
      "91/241:\n",
      "binary_questions['cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/242:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/243:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/244:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/245:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/246:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/247:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/248:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/249:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/250:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/251:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/252:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "binary_questions['yes_responsese'] = resp_yes_counts\n",
      "print(binary_questions)\n",
      "91/253:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/254:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/255:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/256:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/257:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "binary_questions['yes_responses'] = resp_yes_counts\n",
      "print(binary_questions)\n",
      "91/258:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/259:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/260:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(yes_counts_sorted)\n",
      "91/261:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/262:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "#binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "binary_questions['yes_responses'] = resp_yes_counts\n",
      "print(binary_questions)\n",
      "91/263:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/264:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/265:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "91/266:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/267:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/268:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "91/269:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/270:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/271:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "91/272:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/273:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/274:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['respondent','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'respondent': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "   \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "responses = pd.concat([responses,resp_yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "91/275:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/276:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/277:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "responses = pd.concat([responses,resp_yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "91/278:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/279:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/280:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/281:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "responses = pd.concat([responses,resp_yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "91/282:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/283:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/284:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/285:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/286:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "responses = pd.concat([responses,resp_yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "#print(resp_yes_counts_sorted)\n",
      "91/287:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/288:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/289:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/290:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "responses = pd.concat([responses,resp_yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "#print(resp_yes_counts_sorted)\n",
      "91/291:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "#binary_questions = binary_questions.fillna(0)\n",
      "91/292:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "#binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "#binary_questions = binary_questions.fillna(0)\n",
      "91/293:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "#binary_questions = binary_questions.fillna(0)\n",
      "91/294:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/295:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/296:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/297:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/298:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "#print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "print(responses.head())\n",
      "91/299:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/300:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/301:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/302:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/303:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "91/304:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/305:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/306:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/307:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for use later\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "91/308:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/309:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions['Q1_1','binary_count','cluster'])\n",
      "91/310:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions[['Q1_1','binary_count','cluster']])\n",
      "91/311:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=3, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/312:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions)\n",
      "91/313:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions['Q1_1'])\n",
      "91/314:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions[['Q1_1','Q3']])\n",
      "91/315:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions[['Q1_1','binary_count']])\n",
      "91/316:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "print(binary_questions.info())\n",
      "#print(binary_questions[['Q1_1','binary_count']])\n",
      "91/317:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#print(binary_questions.info())\n",
      "print(binary_questions[['Q1_1','count']])\n",
      "91/318:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#print(binary_questions.info())\n",
      "print(binary_questions[['Q1_1','count','binary_cluster']])\n",
      "91/319:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#print(binary_questions.info())\n",
      "print(binary_questions[['Q1_1','count','binary_cluster']].sort_values(by=['binary_cluster']))\n",
      "91/320:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/321:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/322:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for later use\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "91/323:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/324:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "    kmeans = KMeans(n_clusters = k).fit(points)\n",
      "    centroids = kmeans.cluster_centers_\n",
      "    pred_clusters = kmeans.predict(points)\n",
      "    curr_sse = 0\n",
      "\n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "    for i in range(len(points)):\n",
      "      curr_center = centroids[pred_clusters[i]]\n",
      "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "    sse.append(curr_sse)\n",
      "\n",
      "  return sse\n",
      "\n",
      "print(sse)\n",
      "91/325:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "    kmeans = KMeans(n_clusters = k).fit(points)\n",
      "    centroids = kmeans.cluster_centers_\n",
      "    pred_clusters = kmeans.predict(points)\n",
      "    curr_sse = 0\n",
      "\n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "    for i in range(len(points)):\n",
      "      curr_center = centroids[pred_clusters[i]]\n",
      "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "    sse.append(curr_sse)\n",
      "    print(sse)\n",
      "    \n",
      "  return sse\n",
      "91/326:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "    kmeans = KMeans(n_clusters = k).fit(points)\n",
      "    centroids = kmeans.cluster_centers_\n",
      "    pred_clusters = kmeans.predict(points)\n",
      "    curr_sse = 0\n",
      "\n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "    for i in range(len(points)):\n",
      "      curr_center = centroids[pred_clusters[i]]\n",
      "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "    sse.append(curr_sse)\n",
      "    print(sse)\n",
      "    \n",
      "  return sse\n",
      "91/327:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "    kmeans = KMeans(n_clusters = k).fit(points)\n",
      "    centroids = kmeans.cluster_centers_\n",
      "    pred_clusters = kmeans.predict(points)\n",
      "    curr_sse = 0\n",
      "\n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "    for i in range(len(points)):\n",
      "      curr_center = centroids[pred_clusters[i]]\n",
      "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "    sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "joel = sse\n",
      "91/328:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "joel = sse\n",
      "91/329:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/330:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "print(points)\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/331:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "print(points.info())\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(1, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/332:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "points = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]\n",
      "kmax = 10\n",
      "#print(points.info())\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/333:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "\n",
      "df['DataFrame Column'] = df['DataFrame Column'].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/334:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "print(binary_questions.info())\n",
      "\n",
      "df['DataFrame Column'] = df['DataFrame Column'].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/335:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "print(binary_questions.info())\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/336:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "print(binary_questions.info())\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/337:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "\n",
      "print(len(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']]))\n",
      "91/338:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "91/339:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "\n",
      "print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/340:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "91/341:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "  sse = []\n",
      "  for k in range(3, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "  return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "91/342:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "91/343:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "91/344:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/345:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "            curr_sse += (points[i][0] - curr_center[0]) ** 2 + (points[i][1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "#calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/346:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "            curr_sse += (points[i][0] - curr_center[0]) ** 2 + (points[i][1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/347:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "            curr_sse += ([points[i]][0] - curr_center[0]) ** 2 + ([points[i]][1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/348:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "            curr_sse += (points.iloc[0,i] - curr_center[0]) ** 2 + (points.iloc[1,i] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/349:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
      "            curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/350:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points[i])):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/351:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points[i])):\n",
      "                joel = len(points[1])\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return joel\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/352:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points[i])):\n",
      "                joel = len(points[i])\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return joel\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/353:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points[i])):\n",
      "                print(len(points[1]))\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/354:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points[i])):\n",
      "                print(len(points[i]))\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],5)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/355:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(12):\n",
      "                print(len(points[i]))\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/356:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(12):\n",
      "                print(12)\n",
      "#                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/357:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(12):\n",
      "#                print(12))\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/358:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for later use\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "\n",
      "print(reponses.head())\n",
      "91/359:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for later use\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "\n",
      "print(responses.head())\n",
      "91/360:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(points.num_columns):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/361:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/362:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points.columns)+1):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/363:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],10)\n",
      "#print(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "91/364:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],20)\n",
      "plt.plot(sse)\n",
      "91/365:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            #curr_sse += (points.iloc[i,0] - curr_center[0]) ** 2 + (points.iloc[i,1] - curr_center[1]) ** 2\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "plt.plot(calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],20))\n",
      "plt.show()\n",
      "91/366:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "plt.plot(calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],50))\n",
      "plt.show()\n",
      "91/367:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "plt.plot(calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],40))\n",
      "plt.show()\n",
      "91/368:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/369:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/370:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for later use\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "91/371:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "91/372:\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "plt.plot(calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],40))\n",
      "plt.show()\n",
      "91/373:\n",
      "# That plot isn't as clear as I wanted it - perhaps the sse isn't being calculated correctly?\n",
      "# Thought it would look more like a scree plot (it's supposed to be an Elbow plot)\n",
      "# But let's suppose that we just wanted to do five clusters\n",
      "# We can label the cluster of each team member\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#print(binary_questions.info())\n",
      "print(binary_questions[['Q1_1','count','binary_cluster']].sort_values(by=['binary_cluster']))\n",
      "91/374:\n",
      "# That plot isn't as clear as I wanted it - perhaps the sse isn't being calculated correctly?\n",
      "# Thought it would look more like a scree plot (it's supposed to be an Elbow plot)\n",
      "# But let's suppose that we just wanted to do five clusters\n",
      "# We can label the cluster of each team member\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#reduce number of columns\n",
      "binary_cluster_results = binary_questions[['Q1_1','count','binary_cluster']]\n",
      "binary_cluster_results.to_csv()\n",
      "\n",
      "#print(binary_questions[['Q1_1','count','binary_cluster']].sort_values(by=['binary_cluster']))\n",
      "91/375:\n",
      "# That plot isn't as clear as I wanted it - perhaps the sse isn't being calculated correctly?\n",
      "# Thought it would look more like a scree plot (it's supposed to be an Elbow plot)\n",
      "# But let's suppose that we just wanted to do five clusters\n",
      "# We can label the cluster of each team member\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#reduce number of columns\n",
      "binary_cluster_results = binary_questions[['Q1_1','count','binary_cluster']]\n",
      "binary_cluster_results.to_csv('binary_cluster_results.csv')\n",
      "\n",
      "#print(binary_questions[['Q1_1','count','binary_cluster']].sort_values(by=['binary_cluster']))\n",
      "91/376: textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "91/377:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "91/378:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "91/379:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "91/380:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    joel = 0\n",
      "91/381:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "print(len(textlist))\n",
      "for i in range(len(textlist)):\n",
      "    joel = 0\n",
      "91/382:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text data = responses[textlist[i]]\n",
      "91/383:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "91/384:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "    print(len(text_data))\n",
      "91/385:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "    print(text_data.head())\n",
      "91/386:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "comment_words = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "print(stopwords)\n",
      "\n",
      "\n",
      "#for i in range(len(textlist)):\n",
      "#    text_data = responses[textlist[i]]\n",
      "#    for val in text_data.CONTENT:\n",
      "91/387:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "comment_words = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "#for i in range(len(textlist)):\n",
      "#    text_data = responses[textlist[i]]\n",
      "#    for val in text_data.CONTENT:\n",
      "91/388:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "    for val in text_data.CONTENT:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/389:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "    print(text_data)\n",
      "\n",
      "#    for val in text_data.CONTENT:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/390:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]]\n",
      "    print(text_data.info())\n",
      "\n",
      "#    for val in text_data.CONTENT:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/391:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "\n",
      "#    for val in text_data.CONTENT:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/392:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    for val in text_data.CONTENT:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/393:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    print(text_data)\n",
      "#    for val in text_data.CONTENT:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/394:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    print(text_data)\n",
      "    for val in text_data.CONTENT:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/395:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    #print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/396:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    #print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    \n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "91/397:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    #print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/398:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "#textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    #print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/399:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    #print(text_data)\n",
      "#    for val in text_data:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "#    plt.imshow(wordcloud)\n",
      "#    plt.show()\n",
      "91/400:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    print(text_data)\n",
      "#    for val in text_data:\n",
      "#        #set each val to a string\n",
      "#        val = str(val)\n",
      "#        #split the value\n",
      "#        tokens = val.split()\n",
      "#        #convert to lowercase\n",
      "#        for i in range(len(tokens)):\n",
      "#            tokens[i] = tokens[i].lower()\n",
      "#        corpus += \" \".join(tokens)+\" \"\n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "#    plt.imshow(wordcloud)\n",
      "#    plt.show()\n",
      "91/401:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna().reset_index()\n",
      "    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    print(corpus)\n",
      "\n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "#    plt.imshow(wordcloud)\n",
      "#    plt.show()\n",
      "91/402:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "    print(corpus)\n",
      "\n",
      "#    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "#    plt.imshow(wordcloud)\n",
      "#    plt.show()\n",
      "91/403:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/404:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, title = textlist[i], background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/405:\n",
      "# list of open-ended questions\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, title = 'cloud', background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/406:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, label = 'cloud', background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.show()\n",
      "91/407:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud, label = 'cloud')\n",
      "    plt.show()\n",
      "91/408:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud, title = 'cloud')\n",
      "    plt.show()\n",
      "91/409:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    ax.set_title('cloud')\n",
      "    plt.show()\n",
      "91/410:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.set_title('cloud')\n",
      "    plt.show()\n",
      "91/411:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.set_title('joel')\n",
      "    plt.show()\n",
      "91/412:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title('joel')\n",
      "    plt.show()\n",
      "91/413:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for i in range(len(tokens)):\n",
      "            tokens[i] = tokens[i].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/414:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "#textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "textlist = ['Q3a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "#    print(text_data)\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "#    print(corpus)\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/415:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "corpus = ''\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/416:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/417:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "91/418:\n",
      "# sum up \"yes\" responses by each question\n",
      "\n",
      "qlist = ['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']\n",
      "\n",
      "yes_counts = pd.DataFrame(columns = ['Q','count'])\n",
      "\n",
      "for i in range(len(qlist)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(responses)):\n",
      "            if responses[qlist[i]][j] == 'Yes':\n",
      "                total_yes+=1\n",
      "    yes_counts = yes_counts.append({'Q': qlist[i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "yes_counts_sorted = yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "    \n",
      "print(yes_counts_sorted)\n",
      "91/419:\n",
      "# count up \"yes\" responses by each respondent\n",
      "\n",
      "resp_yes_counts = pd.DataFrame(columns = ['Q1_1','count'])\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    total_yes = 0\n",
      "    for j in range(len(qlist)):\n",
      "            if responses[qlist[j]][i] == 'Yes':\n",
      "                total_yes+=1\n",
      "    resp_yes_counts = resp_yes_counts.append({'Q1_1': responses['Q1_1'][i], 'count': total_yes}, ignore_index=True)\n",
      "\n",
      "# sort results and show\n",
      "resp_yes_counts_sorted = resp_yes_counts.sort_values(by=['count'],ascending = False)    \n",
      "print(resp_yes_counts_sorted)\n",
      "\n",
      "# append the yes counts to the end of the responses data frame for later use\n",
      "yes_counts = resp_yes_counts['count']\n",
      "responses = pd.concat([responses,yes_counts],axis = 1)\n",
      "91/420:\n",
      "# try k-means on binaries. First prep data\n",
      "\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "\n",
      "binary_questions = responses[['Q1_1','Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16','count']]\n",
      "\n",
      "# replace yes with 1, no with 0\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"Yes\",1)\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].replace(\"No\",0)\n",
      "# replace null values with a 0\n",
      "binary_questions = binary_questions.fillna(0)\n",
      "\n",
      "# set everything to float\n",
      "binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']] = binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']].astype(float)\n",
      "91/421:\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "# Try up to 10 clusters, calculate elbow statistic and plot\n",
      "# adapted from https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
      "\n",
      "# function returns WSS score for k values from 1 to kmax\n",
      "\n",
      "def calculate_WSS(points, kmax):\n",
      "    sse = []\n",
      "    for k in range(2, kmax+1):\n",
      "        kmeans = KMeans(n_clusters = k).fit(points)\n",
      "        centroids = kmeans.cluster_centers_\n",
      "        pred_clusters = kmeans.predict(points)\n",
      "        curr_sse = 0\n",
      "        \n",
      "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
      "        for i in range(len(points)):\n",
      "            curr_center = centroids[pred_clusters[i]]\n",
      "            for j in range(len(points.columns)):\n",
      "                curr_sse += (points.iloc[i,j] - curr_center[j]) ** 2\n",
      "        sse.append(curr_sse)\n",
      "    \n",
      "    return sse\n",
      "\n",
      "plt.plot(calculate_WSS(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']],40))\n",
      "plt.show()\n",
      "91/422:\n",
      "# That plot isn't as clear as I wanted it - perhaps the sse isn't being calculated correctly?\n",
      "# Thought it would look more like a scree plot (it's supposed to be an Elbow plot)\n",
      "# But let's suppose that we just wanted to do five clusters\n",
      "# We can label the cluster of each team member\n",
      "\n",
      "kmeans_binary = KMeans(n_clusters=5, random_state=0).fit(binary_questions[['Q3','Q5','Q7','Q8','Q9','Q10','Q11','Q12','Q13','Q14','Q15','Q16']])\n",
      "\n",
      "#attach cluster to data frame\n",
      "binary_questions['binary_cluster'] = kmeans_binary.labels_\n",
      "#reduce number of columns\n",
      "binary_cluster_results = binary_questions[['Q1_1','count','binary_cluster']]\n",
      "binary_cluster_results.to_csv('binary_cluster_results.csv')\n",
      "\n",
      "#print(binary_questions[['Q1_1','count','binary_cluster']].sort_values(by=['binary_cluster']))\n",
      "91/423:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/424:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "stopwords.append(['data'])\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/425:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "#stopwords.append(['data'])\n",
      "print(stopwords.info())\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/426:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "stopwords = ['data'] + list(stopwords)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/427:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "stopwords = ['data', 'used', 'software', 'information'] + list(stopwords)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "91/428:\n",
      "# list of open-ended questions\n",
      "# https://www.geeksforgeeks.org/generating-word-cloud-python/\n",
      "\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "textlist = ['Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']\n",
      "\n",
      "stopwords = set(STOPWORDS)\n",
      "stopwords = ['data', 'used', 'software', 'information', 'database', 'system'] + list(stopwords)\n",
      "\n",
      "for i in range(len(textlist)):\n",
      "    text_data = responses[textlist[i]].dropna()\n",
      "    corpus = ''\n",
      "    for val in text_data:\n",
      "        #set each val to a string\n",
      "        val = str(val)\n",
      "        #split the value\n",
      "        tokens = val.split()\n",
      "        #convert to lowercase\n",
      "        for j in range(len(tokens)):\n",
      "            tokens[j] = tokens[j].lower()\n",
      "        corpus += \" \".join(tokens)+\" \"\n",
      "\n",
      "    wordcloud = WordCloud(width = 800, height = 800, background_color = 'white', stopwords = stopwords, min_font_size = 10).generate(corpus)\n",
      "    plt.imshow(wordcloud)\n",
      "    plt.title(textlist[i])\n",
      "    plt.show()\n",
      "94/1:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "94/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "94/3:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "94/4:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "print(responses.head())\n",
      "94/5:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "94/6:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses)\n",
      "94/7:\n",
      "# try lemmatizing\n",
      "\n",
      "responses_lemmatized = responses.lower()\n",
      "94/8:\n",
      "# try lemmatizing\n",
      "\n",
      "#responses_lemmatized = responses.lower()\n",
      "\n",
      "joel = \"Hi There, my NAME is joEL\"\n",
      "joel = joel.lower()\n",
      "print(joel)\n",
      "94/9:\n",
      "# try lemmatizing\n",
      "\n",
      "for i in len(responses):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/10:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(string)\n",
      "\n",
      "for i in len(responses):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/11:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "\n",
      "for i in len(responses):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/12:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/13:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    print(responses[i])\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/14:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    print(responses[i])\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/15:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "94/16:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses)\n",
      "94/17:\n",
      "# try lemmatizing\n",
      "\n",
      "#responses = responses.astype(str)\n",
      "#print(len(responses))\n",
      "\n",
      "for i in range(len(responses)):\n",
      "#    print(responses[i])\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/18:\n",
      "# try lemmatizing\n",
      "\n",
      "print(responses.info())\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/19:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "print(responses,info())\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/20:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "print(responses.info())\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/21:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "94/22:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.info())\n",
      "94/23:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame.responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.info())\n",
      "94/24:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame.(columns = responses['Q5a'])\n",
      "responses = responses.dropna()\n",
      "print(responses.info())\n",
      "94/25:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame(columns = responses['Q5a'])\n",
      "responses = responses.dropna()\n",
      "print(responses.info())\n",
      "94/26:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.info())\n",
      "94/27:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "94/28:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses)\n",
      "94/29:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.type())\n",
      "94/30:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "print(len(responses))\n",
      "94/31:\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.type())\n",
      "94/32:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = responses.dropna()\n",
      "print(responses.type())\n",
      "94/33:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses = pd.DataFrame(responses)\n",
      "print(responses.type())\n",
      "94/34:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "#responses = pd.DataFrame(responses)\n",
      "#print(responses.type())\n",
      "94/35:\n",
      "# try lemmatizing\n",
      "\n",
      "responses = responses.astype(str)\n",
      "#print(responses.info())\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/36:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "#responses = pd.DataFrame(responses)\n",
      "#print(responses.type())\n",
      "94/37:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "#responses = pd.DataFrame(responses)\n",
      "print(responses)\n",
      "94/38:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "#responses = pd.DataFrame(responses)\n",
      "print(responses)\n",
      "94/39:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "print(responses)\n",
      "94/40:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "print(responses.info())\n",
      "94/41:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "print(info(responses))\n",
      "94/42:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "print(responses.info())\n",
      "94/43:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "94/44:\n",
      "# try lemmatizing\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/45:\n",
      "# try lemmatizing\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "#print(responses)\n",
      "94/46:\n",
      "# try lemmatizing\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower()\n",
      "\n",
      "print(responses)\n",
      "94/47:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "responses = responses.dropna()\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "#print(responses)\n",
      "94/48:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "94/49:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "responses = responses.dropna()\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "#print(responses)\n",
      "94/50:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "94/51:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "responses = responses.dropna()\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "print(responses)\n",
      "94/52:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "responses = responses[!='nan']\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "print(responses)\n",
      "94/53:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "responses = responses[[responses!='nan']\n",
      "print(len(responses))\n",
      "\n",
      "#for i in range(len(responses)):\n",
      "#    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "print(responses)\n",
      "94/54:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "responses = responses[[responses!='nan']\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "                      \n",
      "#print(responses)\n",
      "94/55:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "responses = responses[[responses!='nan']]\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "                      \n",
      "#print(responses)\n",
      "94/56:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "print(len(responses))\n",
      "\n",
      "                      \n",
      "#print(responses)\n",
      "94/57:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "responses.remove('nan')\n",
      "print(len(responses))\n",
      "                      \n",
      "#print(responses)\n",
      "94/58:\n",
      "# try lemmatizing\n",
      "\n",
      "#get rid of blanks\n",
      "\n",
      "for i in range(len(responses)):\n",
      "    responses[i] = responses[i].lower() # make all text lowercase\n",
      "\n",
      "print(len(responses))\n",
      "                      \n",
      "print(responses)\n",
      "94/59:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "responses.dropna()\n",
      "print(len(responses))\n",
      "94/60:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "responses.dropna()\n",
      "print(len(responses))\n",
      "94/61:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a'].astype(str)\n",
      "responses.dropna()\n",
      "print(responses)\n",
      "94/62:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame(responses['Q5a'].astype(str))\n",
      "responses.dropna()\n",
      "print(responses)\n",
      "94/63:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame(responses['Q5a'].astype(str))\n",
      "responses.dropna()\n",
      "print(len(responses))\n",
      "94/64:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "#responses = pd.DataFrame(responses['Q5a'].astype(str))\n",
      "#responses.dropna()\n",
      "print(responses)\n",
      "94/65:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = pd.DataFrame(responses['Q5a'].astype(str))\n",
      "responses.dropna()\n",
      "print(responses)\n",
      "94/66:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses.dropna()\n",
      "print(responses)\n",
      "94/67:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses.dropna()\n",
      "print(len(responses))\n",
      "94/68:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "responses.dropna()\n",
      "print(len(responses))\n",
      "94/69:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "print(responses.info())\n",
      "#responses.dropna()\n",
      "#print(len(responses))\n",
      "94/70:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "print(type(responses))\n",
      "#responses.dropna()\n",
      "#print(len(responses))\n",
      "94/71:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "responses = pd.read_excel(path + 'Cloud-Based Technologies Survey_March 2, 2021_07.54.xlsx', index_col = None)\n",
      "print(type(responses))\n",
      "\n",
      "# drop rows with blank respondent name\n",
      "responses = responses.dropna(subset=['Q1_1']).reset_index()\n",
      "\n",
      "# restrict to open-ended questions\n",
      "\n",
      "#responses = responses[['Q1_1','Q3a','Q5a','Q7a','Q8a','Q9a','Q9a_3_TEXT','Q9b','Q10a','Q11a','Q12a','Q13a','Q14a','Q15a','Q16a']]\n",
      "responses = responses['Q5a']\n",
      "print(type(responses))\n",
      "#responses.dropna()\n",
      "#print(len(responses))\n",
      "95/1:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis', sheetname = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "95/2:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheetname = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "95/3:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheet_name = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "95/4:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheet_name = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "\n",
      "for name, sheet in sheets_dict.items():\n",
      "    sheet['sheet'] = name\n",
      "    sheet_list = sheet_list.append(sheet)\n",
      "\n",
      "print full_table\n",
      "95/5:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheet_name = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "\n",
      "for name, sheet in sheets_dict.items():\n",
      "    sheet['sheet'] = name\n",
      "    sheet_list = sheet_list.append(sheet)\n",
      "95/6:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheet_name = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "\n",
      "for name, sheet in sheets.items():\n",
      "    sheet['sheet'] = name\n",
      "    sheet = sheet.rename(columns=lambda x: x.split('\\n')[-1])\n",
      "    sheet_list = sheet_list.append(sheet)\n",
      "95/7:\n",
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "from wordcloud import WordCloud, STOPWORDS\n",
      "\n",
      "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
      "\n",
      "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DRAID_survey/'\n",
      "\n",
      "sheets = pd.read_excel(path + 'JAIC_DRAID_NLP_KMEANS_analysis.xlsx', sheet_name = None)\n",
      "\n",
      "# list of sheet names\n",
      "sheet_list = pd.DataFrame()\n",
      "\n",
      "for name, sheet in sheets.items():\n",
      "    sheet['sheet'] = name\n",
      "    sheet = sheet.rename(columns=lambda x: x.split('\\n')[-1])\n",
      "    sheet_list = sheet_list.append(sheet)\n",
      "    \n",
      "print(sheet_list)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    if len(hcl_value) == 6 and (hcl_value.isalnum() or hcl_value in ['a','b','c','d','e','f']) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/91:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    if len(hcl_value) == 6 and (hcl_value.isalnum() or hcl_value in ['a','b','c','d','e','f']) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/92:\n",
      "# List1\n",
      "List1 = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      " \n",
      "# List2\n",
      "List2 = ['a' , '1', '4']\n",
      "\n",
      "check =  all(item in List1 for item in List2)\n",
      " \n",
      "if check is True:\n",
      "    print(\"The list {} contains all elements of the list {}\".format(List1, List2))    \n",
      "else :\n",
      "    print(\"No, List1 doesn't have all elements of the List2.\")\n",
      "161/93:\n",
      "# List1\n",
      "List1 = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      " \n",
      "# List2\n",
      "List2 = ['a' , '104', '4']\n",
      "\n",
      "check =  all(item in List1 for item in List2)\n",
      " \n",
      "if check is True:\n",
      "    print(\"The list {} contains all elements of the list {}\".format(List1, List2))    \n",
      "else :\n",
      "    print(\"No, List1 doesn't have all elements of the List2.\")\n",
      "161/94:\n",
      "# List1\n",
      "List1 = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      " \n",
      "# List2\n",
      "oof = 'aaf013'\n",
      "List2 = list(oof)\n",
      "\n",
      "check =  all(item in List1 for item in List2)\n",
      " \n",
      "if check is True:\n",
      "    print(\"The list {} contains all elements of the list {}\".format(List1, List2))    \n",
      "else :\n",
      "    print(\"No, List1 doesn't have all elements of the List2.\")\n",
      "161/95:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/96:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/97:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "valid_counter = {}\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                valid_counter.update(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/98:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "key_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "161/99:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "key_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "\n",
      "import collections\n",
      "\n",
      "counter=collections.Counter(key_counter)\n",
      "161/100:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    if len(hcl_value) == 6 and all(item in character_list for item in list(hcl_value)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "key_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "\n",
      "import collections\n",
      "\n",
      "counter=collections.Counter(key_counter)\n",
      "print(counter)\n",
      "161/101:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[-2:]\n",
      "print(joel)\n",
      "161/102:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[-2:]\n",
      "print(right)\n",
      "161/103:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[2:]\n",
      "print(right)\n",
      "161/104:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[:1]\n",
      "print(right)\n",
      "161/105:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[:1]\n",
      "left = joel[1:]\n",
      "\n",
      "print(right)\n",
      "161/106:\n",
      "joel = '#014325'\n",
      "\n",
      "right = joel[:1]\n",
      "left = joel[1:]\n",
      "\n",
      "print(left)\n",
      "161/107:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char = '#' and len(rest_of_chars) == 6 and all(item in character_list for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "key_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "\n",
      "import collections\n",
      "\n",
      "counter=collections.Counter(key_counter)\n",
      "print(counter)\n",
      "161/108:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char == '#' and len(rest_of_chars) == 6 and all(item in character_list for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#joel = {'byr': '2000','iyr': '2015','eyr': '2025','ecl': 'oth','hgt': '70in','hcl': 'abcdef','pid': '029232223'}\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "valid_entries = []\n",
      "key_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "\n",
      "import collections\n",
      "\n",
      "counter=collections.Counter(key_counter)\n",
      "print(counter)\n",
      "161/109:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from operator import mul\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/4#part1\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week04_Day04/'\n",
      "\n",
      "column = ['passport_data']\n",
      "day4data = pd.read_csv(data_file_loc + 'day_04_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "day4data['passport_no'] = 0\n",
      "\n",
      "passport_number = 1\n",
      "\n",
      "for x in range(len(day4data)):\n",
      "    if (pd.isnull(day4data.iloc[x]['passport_data'])):\n",
      "    # If value is null, increment the passport number\n",
      "        passport_number +=1\n",
      "        day4data['passport_no'].iloc[x] = 0\n",
      "    else:\n",
      "    # If value is not null, apply the current group number\n",
      "        day4data['passport_no'].iloc[x] = passport_number\n",
      "\n",
      "#print(day4data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "manifesto = pd.DataFrame()\n",
      "\n",
      "# For manifesto, start two blanks lists that we'll append to, then they can become dataframe columns\n",
      "passport_number = []\n",
      "# Passport detail will be the concatenated text for each individual passport\n",
      "passport_detail = []\n",
      "\n",
      "# Setting the range this way to get rid of the zeroes\n",
      "for x in range(1,day4data['passport_no'].max()+1):\n",
      "    passport_number.append(x)\n",
      "    subset = day4data[day4data.passport_no == x]\n",
      "    passport_string = ''\n",
      "    for y in range(len(subset)):\n",
      "        passport_string = passport_string + ' ' + str(subset.iloc[y]['passport_data'])\n",
      "    passport_detail.append(passport_string.strip())\n",
      "        \n",
      "# Now append those lists as columns\n",
      "manifesto['passport_number'] = passport_number\n",
      "manifesto['passport_detail'] = passport_detail\n",
      "\n",
      "# End result is a list of all passport numbers, with all of the fields in one line\n",
      "161/110:\n",
      "# For the first challenge, go through that passport_detail field and see if each field name is found\n",
      "\n",
      "# Entries is a counter for how many of the fields are found for each passport\n",
      "entries = []\n",
      "# For the second part, I'm going to create a dictionary that should make it easy to extract the value associated with each key\n",
      "passport_dict = []\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "    # build a dictionary by splitting at spaces, then separating the key from value using the colon\n",
      "    d = dict(x.split(\":\") for x in manifesto.iloc[z]['passport_detail'].split(\" \"))\n",
      "    entry = 0\n",
      "    # count the number of passport fields for the passport\n",
      "    for search_item in search_list:\n",
      "        if search_item in manifesto.iloc[z]['passport_detail']:\n",
      "            entry +=1\n",
      "#    manifesto.iloc[z]['passport_dict'] = passport_dict\n",
      "    entries.append(entry)\n",
      "    passport_dict.append(d)\n",
      "    \n",
      "manifesto['passport_dict'] = passport_dict\n",
      "manifesto['entries'] = entries\n",
      "del manifesto['passport_detail']\n",
      "        \n",
      "#    search for each term in each row. This creates a T/F field for each term in the search list\n",
      "#    manifesto[search_item] = [search_item in x for x in manifesto['passport_detail']]\n",
      "\n",
      "# Create a total number of True values by row for every column in [search_list]\n",
      "#manifesto['passport_fields'] = manifesto[search_list].sum(axis=1)    \n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "# The number of rows with the same valid fields as the length of the search_list is the correct answer\n",
      "print('part 1 answer:',len(manifesto[manifesto.entries == len(search_list)]))\n",
      "161/111:\n",
      "# Set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from ecl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    #    print(\"Hello from hgt!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char == '#' and len(rest_of_chars) == 6 and all(item in character_list for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from hcl!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "#    print(\"Hello from pid!, valid\", valid)\n",
      "    return(valid)\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "# valid_entries keeps a counter of how many passport properties are valid according to the conditions in their functions\n",
      "valid_entries = []\n",
      "# key_counter keeps a list of how many times each property passes, so we can check if a certain property is always failing\n",
      "key_counter = []\n",
      "key_pass_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "#    print(manifesto.iloc[z]['passport_dict'])\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "            key_counter.append(key)\n",
      "    #        print(\"Key exists\")\n",
      "            function_name = key + '_test'\n",
      "    #        print(function_name)\n",
      "    #        print(joel[key])\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "#            print(function_name+'(\"'+function_parameter+'\")')\n",
      "            # if the key exists, eval will run the function with value for its parameter\n",
      "    #        valid = eval(function_name+\"(\"+function_parameter+\")\")\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_pass_counter.append(key)\n",
      "    #        print('valid?',valid)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "#print('part 2 answer:',len(manifesto[manifesto.valid_entries == 6]))\n",
      "\n",
      "import collections\n",
      "\n",
      "key_counter=collections.Counter(key_counter)\n",
      "print(key_counter)\n",
      "\n",
      "key_pass_counter=collections.Counter(key_pass_counter)\n",
      "print(key_pass_counter)\n",
      "161/112:\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "    #If cm, the number must be at least 150 and at most 193.\n",
      "    #If in, the number must be at least 59 and at most 76.\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "#cid (Country ID) - ignored, missing or not.\n",
      "\n",
      "test = '001551121'\n",
      "system = len(test)\n",
      "print(system)\n",
      "alnum = test.isdecimal()\n",
      "print(alnum)\n",
      "\n",
      "len(hcl_value) == 6 and (hcl_value.isalnum() or hcl_value in ['a','b','c','d','e','f']) is True:\n",
      "161/113:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "\n",
      "print(type(key_pass_counter))\n",
      "161/114:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "\n",
      "print(type(key_pass_counter))\n",
      "161/115:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    print(key)\n",
      "    print(key_pass_counter[key])\n",
      "    print(key_counter[key])\n",
      "    print('')\n",
      "161/116:\n",
      "#manifesto = pd.DataFrame()\n",
      "#\n",
      "## For manifesto, start two blanks lists that we'll append to, then they can become dataframe columns\n",
      "#passport_number = []\n",
      "## Passport detail will be the concatenated text for each individual passport\n",
      "#passport_detail = []\n",
      "#\n",
      "#for x in range(day4data['passport_no'].max()+1):\n",
      "#    passport_number.append(x)\n",
      "#    subset = day4data[day4data.passport_no == x]\n",
      "#    string = ''\n",
      "#    for y in range(len(subset)):\n",
      "#        string = string + ' ' + str(subset.iloc[y]['passport_data'])\n",
      "#    passport_detail.append(string)\n",
      "#        \n",
      "## Now append those lists as columns\n",
      "#manifesto['passport_number'] = passport_number\n",
      "#manifesto['passport_detail'] = passport_detail\n",
      "#\n",
      "#search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "#for search_item in search_list:\n",
      "#    # search for each term in each row. This creates a T/F field for each term in the search list\n",
      "#    manifesto[search_item] = [search_item in x for x in manifesto['passport_detail']]\n",
      "#\n",
      "## Create a total number of True values by row for every column in [search_list]\n",
      "#manifesto['passport_fields'] = manifesto[search_list].sum(axis=1)    \n",
      "#\n",
      "#print(manifesto.head())\n",
      "#\n",
      "## The number of rows with all Ts is the correct answer\n",
      "#print('part 1 answer:',len(manifesto[manifesto.passport_fields == len(search_list)]))\n",
      "161/117:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    print(key)\n",
      "    print(key_pass_counter[key]/key_counter[key])\n",
      "    print('')\n",
      "161/118:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    print(key)\n",
      "    print(round(key_pass_counter[key]/key_counter[key],2))\n",
      "    print('')\n",
      "161/119:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    print(key)\n",
      "    print(round(key_pass_counter[key]/key_counter[key]*100,2))\n",
      "    print('')\n",
      "161/120:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "key_pass_rate = {}\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    key_pass_rate[key] = round(key_pass_counter[key]/key_counter[key]*100,2)\n",
      "    print(key)\n",
      "    print(round(key_pass_counter[key]/key_counter[key]*100,2))\n",
      "    print('')\n",
      "print(key_pass_rate)\n",
      "161/121:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "key_pass_rate = {}\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    key_pass_rate[key] = round(key_pass_counter[key]/key_counter[key]*100,2)\n",
      "print(key_pass_rate)\n",
      "161/122:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "key_pass_rate = {}\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    key_pass_rate[key] = str(round(key_pass_counter[key]/key_counter[key]*100,2)+'%')\n",
      "print(key_pass_rate)\n",
      "161/123:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "key_pass_rate = {}\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    key_pass_rate[key] = str(round(key_pass_counter[key]/key_counter[key]*100,2),'%')\n",
      "print(key_pass_rate)\n",
      "161/124:\n",
      "# And for a final bonus, let's get a success rate for the individual passport fields\n",
      "# Turn them into regular dictionaries\n",
      "\n",
      "key_counter = dict(key_counter)\n",
      "key_pass_counter = dict(key_pass_counter)\n",
      "key_pass_rate = {}\n",
      "\n",
      "for key in key_pass_counter:\n",
      "    key_pass_rate[key] = round(key_pass_counter[key]/key_counter[key]*100,2)\n",
      "print(key_pass_rate)\n",
      "161/125:\n",
      "# For the second challenge, set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = ['0','1','2','3','4','5','6','7','8','9','a','b', 'c','d','e','f']\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char == '#' and len(rest_of_chars) == 6 and all(item in character_list for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "# valid_entries keeps a counter of how many passport properties are valid according to the conditions in their functions\n",
      "valid_entries = []\n",
      "# key_counter keeps a list of how many times each property passes, so we can check if a certain property is always failing\n",
      "key_counter = []\n",
      "key_pass_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "            key_counter.append(key)\n",
      "            function_name = key + '_test'\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "            # if the key exists, eval will run the key-specific function with value for its parameter\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_pass_counter.append(key)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "\n",
      "key_counter=collections.Counter(key_counter)\n",
      "key_pass_counter=collections.Counter(key_pass_counter)\n",
      "161/126:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/4#part1\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week04_Day04/'\n",
      "\n",
      "column = ['passport_data']\n",
      "day4data = pd.read_csv(data_file_loc + 'day_04_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "day4data['passport_no'] = 0\n",
      "\n",
      "passport_number = 1\n",
      "\n",
      "for x in range(len(day4data)):\n",
      "    if (pd.isnull(day4data.iloc[x]['passport_data'])):\n",
      "    # If value is null, increment the passport number\n",
      "        passport_number +=1\n",
      "        day4data['passport_no'].iloc[x] = 0\n",
      "    else:\n",
      "    # If value is not null, apply the current group number\n",
      "        day4data['passport_no'].iloc[x] = passport_number\n",
      "\n",
      "#print(day4data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "manifesto = pd.DataFrame()\n",
      "\n",
      "# For manifesto, start two blanks lists that we'll append to, then they can become dataframe columns\n",
      "passport_number = []\n",
      "# Passport detail will be the concatenated text for each individual passport\n",
      "passport_detail = []\n",
      "\n",
      "# Setting the range this way to get rid of the zeroes\n",
      "for x in range(1,day4data['passport_no'].max()+1):\n",
      "    passport_number.append(x)\n",
      "    subset = day4data[day4data.passport_no == x]\n",
      "    passport_string = ''\n",
      "    for y in range(len(subset)):\n",
      "        passport_string = passport_string + ' ' + str(subset.iloc[y]['passport_data'])\n",
      "    passport_detail.append(passport_string.strip())\n",
      "        \n",
      "# Now append those lists as columns\n",
      "manifesto['passport_number'] = passport_number\n",
      "manifesto['passport_detail'] = passport_detail\n",
      "\n",
      "# End result is a list of all passport numbers, with all of the fields in one line\n",
      "161/127:\n",
      "# For the first challenge, go through that passport_detail field and see if each field name is found\n",
      "\n",
      "# Entries is a counter for how many of the fields are found for each passport\n",
      "entries = []\n",
      "# For the second part, I'm going to create a dictionary that should make it easy to extract the value associated with each key\n",
      "passport_dict = []\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "    # build a dictionary by splitting at spaces, then separating the key from value using the colon\n",
      "    d = dict(x.split(\":\") for x in manifesto.iloc[z]['passport_detail'].split(\" \"))\n",
      "    entry = 0\n",
      "    # count the number of passport fields for the passport\n",
      "    for search_item in search_list:\n",
      "        if search_item in manifesto.iloc[z]['passport_detail']:\n",
      "            entry +=1\n",
      "    entries.append(entry)\n",
      "    passport_dict.append(d)\n",
      "    \n",
      "manifesto['passport_dict'] = passport_dict\n",
      "manifesto['entries'] = entries\n",
      "del manifesto['passport_detail']\n",
      "        \n",
      "# Create a total number of True values by row for every column in [search_list]\n",
      "#manifesto['passport_fields'] = manifesto[search_list].sum(axis=1)    \n",
      "\n",
      "print(manifesto.head())\n",
      "\n",
      "# The number of rows with the same valid fields as the length of the search_list is the correct answer\n",
      "print('part 1 answer:',len(manifesto[manifesto.entries == len(search_list)]))\n",
      "161/128:\n",
      "# For the second challenge, set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = [0123456789abcdef]\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char == '#' and len(rest_of_chars) == 6 and all(item in list(character_list) for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "# valid_entries keeps a counter of how many passport properties are valid according to the conditions in their functions\n",
      "valid_entries = []\n",
      "# key_counter keeps a list of how many times each property passes, so we can check if a certain property is always failing\n",
      "key_counter = []\n",
      "key_pass_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "            key_counter.append(key)\n",
      "            function_name = key + '_test'\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "            # if the key exists, eval will run the key-specific function with value for its parameter\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_pass_counter.append(key)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "# Now just append that list of valid entry counts to the manifesto data frame\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "# # Answer is the number of records where all keys in the search list have valid entries\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "\n",
      "key_counter=collections.Counter(key_counter)\n",
      "key_pass_counter=collections.Counter(key_pass_counter)\n",
      "161/129:\n",
      "# For the second challenge, set up individual functions to call and test each of the requirements\n",
      "\n",
      "#byr (Birth Year) - four digits; at least 1920 and at most 2002.\n",
      "def byr_test(byr_value):\n",
      "    if int(byr_value) >= 1920 and int(byr_value) <= 2002:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#iyr (Issue Year) - four digits; at least 2010 and at most 2020.\n",
      "def iyr_test(iyr_value):\n",
      "    if int(iyr_value) >= 2010 and int(iyr_value) <= 2020:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#eyr (Expiration Year) - four digits; at least 2020 and at most 2030.\n",
      "def eyr_test(eyr_value):\n",
      "    if int(eyr_value) >= 2020 and int(eyr_value) <= 2030:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#ecl (Eye Color) - exactly one of: amb blu brn gry grn hzl oth.\n",
      "def ecl_test(ecl_value):\n",
      "    ecl_list = ['amb', 'blu', 'brn', 'gry', 'grn', 'hzl', 'oth']\n",
      "    if ecl_value in ecl_list:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hgt (Height) - a number followed by either cm or in:\n",
      "#If cm, the number must be at least 150 and at most 193.\n",
      "#If in, the number must be at least 59 and at most 76.\n",
      "def hgt_test(hgt_value):\n",
      "    try:\n",
      "        system = hgt_value[-2:]\n",
      "        measurement = int(hgt_value[:-2])\n",
      "        if system == 'cm':\n",
      "            if measurement >= 150 and measurement <= 193:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        elif system == 'in':\n",
      "            if measurement >= 59 and measurement <= 76:\n",
      "                valid = 1\n",
      "            else:\n",
      "                valid = 0\n",
      "        else:\n",
      "            valid = 0\n",
      "    except:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#hcl (Hair Color) - a # followed by exactly six characters 0-9 or a-f.\n",
      "def hcl_test(hcl_value):\n",
      "    character_list = '0123456789abcdef'\n",
      "    first_char = hcl_value[:1]\n",
      "    rest_of_chars = hcl_value[1:]\n",
      "    if first_char == '#' and len(rest_of_chars) == 6 and all(item in list(character_list) for item in list(rest_of_chars)) is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "#pid (Passport ID) - a nine-digit number, including leading zeroes.\n",
      "def pid_test(pid_value):\n",
      "    if len(pid_value) == 9 and pid_value.isdecimal() is True:\n",
      "        valid = 1\n",
      "    else:\n",
      "        valid = 0\n",
      "    return(valid)\n",
      "\n",
      "search_list = ['byr:','iyr:','eyr:','hgt:','hcl:','ecl:','pid:']\n",
      "\n",
      "# The creation of the dictionary removed colons, so that also needs to be done to loop through the keys\n",
      "new_search_list = [s.replace(\":\", \"\") for s in search_list]\n",
      "\n",
      "# valid_entries keeps a counter of how many passport properties are valid according to the conditions in their functions\n",
      "valid_entries = []\n",
      "# key_counter keeps a list of how many times each property passes, so we can check if a certain property is always failing\n",
      "key_counter = []\n",
      "key_pass_counter = []\n",
      "\n",
      "for z in range(len(manifesto)):\n",
      "    valid_counter = 0\n",
      "    for key in new_search_list:\n",
      "        if key in manifesto.iloc[z]['passport_dict']:\n",
      "            key_counter.append(key)\n",
      "            function_name = key + '_test'\n",
      "            function_parameter = manifesto.iloc[z]['passport_dict'][key]\n",
      "            # if the key exists, eval will run the key-specific function with value for its parameter\n",
      "            valid = eval(function_name+'(\"'+function_parameter+'\")')\n",
      "            if valid == 1:\n",
      "                key_pass_counter.append(key)\n",
      "            valid_counter += valid\n",
      "    valid_entries.append(valid_counter)\n",
      "\n",
      "# Now just append that list of valid entry counts to the manifesto data frame\n",
      "manifesto['valid_entries'] = valid_entries\n",
      "\n",
      "# # Answer is the number of records where all keys in the search list have valid entries\n",
      "print('part 2 answer:',len(manifesto[manifesto.valid_entries == len(search_list)]))\n",
      "\n",
      "key_counter=collections.Counter(key_counter)\n",
      "key_pass_counter=collections.Counter(key_pass_counter)\n",
      "162/1:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "#column = ['passport_data']\n",
      "day5data = pd.read_csv(data_file_loc + 'day_05_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day5data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "162/2:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day5data = pd.read_csv(data_file_loc + 'day_05_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day5data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "162/3:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day7data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "162/4:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day7data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "162/5:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day7data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "\n",
      "for x in range(len(day7data):\n",
      "    print('hi!'')\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "162/6:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day7data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    print('hi!'')\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "162/7:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "print(day7data.head())\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    print('hi!')\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "162/8:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    position = day7data.iloc[x]['luggage_rules'].index('luggage_rule_string')\n",
      "    keys.append(position)\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "day7data['luggage_key'] = keys\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/9:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    keys.append(position)\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "day7data['luggage_key'] = keys\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/10:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position]\n",
      "    keys.append(rule_key)\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "day7data['luggage_key'] = keys\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/11:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "keys = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    keys.append(rule_key)\n",
      "#    keybag = day7data.iloc[x]['luggage_rules'].split(sep='contain')\n",
      "\n",
      "day7data['luggage_key'] = keys\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/12:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value =  = day7data.iloc[x]['luggage_rules'][position:].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/13:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value =  = day7data.iloc[x]['luggage_rules'][-position:].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/14:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value =  = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/15:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-position:].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/16:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-position-len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/17:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-position+len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_key'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/18:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-position+len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/19:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-position-len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/20:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = -position-len(luggage_rule_string)\n",
      "#    rule_value = day7data.iloc[x]['luggage_rules'][-position-len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/21:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "    rule_value = len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)\n",
      "#    rule_value = day7data.iloc[x]['luggage_rules'][-position-len(luggage_rule_string):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/22:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters\n",
      "#    rule_value = -(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string))\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/23:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "    re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/24:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "    re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/25:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/26:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/27:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/28:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/29:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bag', ' bags', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '')\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/30:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.'\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '')\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/31:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '')\n",
      "        rule_value = rule_value.replace(char, '')\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/32:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '')\n",
      "        rule_value = rule_value.replace(char, '')\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/33:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    \n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/34:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_values = rule_values.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/35:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    print(type(rule_values))\n",
      "#    rule_values = rule_values.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/36:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/37:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',').strip()\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/38:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data.head())\n",
      "162/39:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "for x in range(len(day4data)):\n",
      "    print(day7data.iloc[x]['luggage_values'])\n",
      "#    for z in range(len)\n",
      "162/40:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    print(day7data.iloc[x]['luggage_values'])\n",
      "#    for z in range(len)\n",
      "162/41:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len)\n",
      "162/42:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "        print('hi')\n",
      "162/43:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/44:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/45:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = 'shiny gold'\n",
      "bag_count = 0\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    backtrack_list = []\n",
      "    if luggage_color in day7data.iloc[x]['luggage_values']:\n",
      "        bag_count += 1\n",
      "        backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(back_track_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/46:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = 'shiny gold'\n",
      "bag_count = 0\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    backtrack_list = []\n",
      "    if luggage_color in day7data.iloc[x]['luggage_values']:\n",
      "        bag_count += 1\n",
      "        backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/47:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = 'shiny gold'\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    if luggage_color in day7data.iloc[x]['luggage_values']:\n",
      "        bag_count += 1\n",
      "        backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/48:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "    if luggage_color[z] in day7data.iloc[x]['luggage_values']:\n",
      "        bag_count += 1\n",
      "        backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/49:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "            if luggage_color[z] in day7data.iloc[x]['luggage_values']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/50:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        if luggage_color[z] in day7data.iloc[x]['luggage_values']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['luggage_key'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/51:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "rule_keys = []\n",
      "rule_values = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    rule_key = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    rule_value = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        rule_key = rule_key.replace(char, '').strip()\n",
      "        rule_value = rule_value.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    rule_value = rule_value.split(',')\n",
      "    rule_keys.append(rule_key)\n",
      "    rule_values.append(rule_value)\n",
      "\n",
      "day7data['luggage_key'] = rule_keys\n",
      "day7data['luggage_values'] = rule_values\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/52:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "bag_parent = []\n",
      "bag_child = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parent.append(bag_parent)\n",
      "    bag_child.append(bag_child)\n",
      "\n",
      "day7data['bag_parent'] = bag_parent\n",
      "day7data['bag_child'] = bag_child\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/53:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_childdren.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/54:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/55:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        if luggage_color[z] in day7data.iloc[x]['bag_children']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['bag_parents'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/56:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = DataFrame()\n",
      "162/57:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "162/58:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_children'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(pairwise_df)\n",
      "162/59:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(pairwise_df)\n",
      "162/60:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(pairwise_df)\n",
      "162/61:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "162/62:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/63:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        if luggage_color[z] in day7data.iloc[x]['bag_children']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['bag_parents'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/64:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "162/65:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df.head())\n",
      "print(pairwise.df.tail())\n",
      "162/66:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df.head())\n",
      "print(pairwise_df.tail())\n",
      "162/67:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in len(bag_color):\n",
      "        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "        print(subset)\n",
      "162/68:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in len(bag_color):\n",
      "        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/69:\n",
      "def number_of_bags(bag_color):\n",
      "    print(bag_color)\n",
      "#    for z in len(bag_color):\n",
      "#        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/70:\n",
      "def number_of_bags(bag_color):\n",
      "    print(bag_color)\n",
      "    for z in range(len(bag_color)):\n",
      "        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/71:\n",
      "def number_of_bags(bag_color):\n",
      "    print(bag_color)\n",
      "#    for z in range(len(bag_color)):\n",
      "#        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/72:\n",
      "def number_of_bags(bag_color):\n",
      "    print(bag_color)\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "#        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/73:\n",
      "def number_of_bags(bag_color):\n",
      "    print(bag_color)\n",
      "    for z in range(len(bag_color)):\n",
      "        print(bag_color)\n",
      "        print(z)\n",
      "#        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold'])\n",
      "162/74:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "#        subset = pairwise_df[pairwise_df.bag_child in bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/75:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child in bag_color = bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/76:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child in bag_color == bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/77:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "#        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/78:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/79:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "#print(day4data['passport_no'].max())\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/80:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        if luggage_color[z] in day7data.iloc[x]['bag_children']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['bag_parents'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/81:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df.head())\n",
      "print(pairwise_df.tail())\n",
      "162/82:\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/83:\n",
      "print(pairwise_df)\n",
      "\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny_gold','muted gray'])\n",
      "162/84:\n",
      "print(pairwise_df)\n",
      "\n",
      "def number_of_bags(bag_color):\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags(['shiny gold','muted gray'])\n",
      "162/85:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += subset\n",
      "        print_counter\n",
      "\n",
      "number_of_bags(['shiny gold','muted gray'])\n",
      "162/86:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += subset\n",
      "        print(counter)\n",
      "\n",
      "number_of_bags(['shiny gold','muted gray'])\n",
      "162/87:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        print(counter)\n",
      "\n",
      "number_of_bags(['shiny gold','muted gray'])\n",
      "162/88:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold','muted gray'])\n",
      "162/89:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold','muted yellow'])\n",
      "162/90:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/91:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/92:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/93:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print(type(parents))\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/94:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print(len(parents))\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/95:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/96:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.append(subset['bag_parent'].tolist())\n",
      "        print(subset['bag_parent'].tolist())\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/97:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/98:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold','dotted black'])\n",
      "162/99:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold','faded blue'])\n",
      "162/100:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/101:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold','faded blue'])\n",
      "162/102:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "#        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print(parents)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/103:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "#        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list = list(set(master_list.extend(parents)))\n",
      "        print(parents)\n",
      "        print(master_list)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/104:\n",
      "def number_of_bags(bag_color):\n",
      "    counter = 0\n",
      "    parents = []\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "#        counter += len(subset)\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print(parents)\n",
      "        print(master_list)\n",
      "        print('counter:',counter)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/105:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print(parents)\n",
      "        print(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'])in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset\n",
      "162/106:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print(parents)\n",
      "        print(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/107:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print(parents)\n",
      "        print(master_list)\n",
      "    print(len(parents))\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/108:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "    if len(parents) == 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/109:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) == 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/110:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/111:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/112:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/113:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/114:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/115:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "#        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/116:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "    return(master_list)\n",
      "\n",
      "final_answer = []\n",
      "final_answer.append(number_of_bags(['shiny gold']))\n",
      "162/117:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "    return(master_list)\n",
      "\n",
      "final_answer = []\n",
      "final_answer.append(number_of_bags(['shiny gold']))\n",
      "print(final_answer)\n",
      "162/118:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "    return(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/119:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "        return(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/120:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "    return(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/121:\n",
      "def number_of_bags(bag_color):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        try:\n",
      "            master_list.extend(parents)\n",
      "        except:\n",
      "            master_list = []\n",
      "            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    return(master_list)\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "\n",
      "number_of_bags(['shiny gold'])\n",
      "162/122:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents)\n",
      "    return(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'],[])\n",
      "162/123:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(master_list)\n",
      "\n",
      "number_of_bags(['shiny gold'],[])\n",
      "162/124:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "number_of_bags(['shiny gold'],[])\n",
      "162/125:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "print('part 1 answer:',len(number_of_bags(['shiny gold'],[]))\n",
      "162/126:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final answer = number_of_bags(['shiny gold'],[])\n",
      "162/127:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "        print('parents:',parents)\n",
      "        print('master_list:',master_list)\n",
      "        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_answer = number_of_bags(['shiny gold'],[])\n",
      "162/128:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    parents = []\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "#    master_list = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print('z: ',z)\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        try:\n",
      "        master_list.extend(parents)\n",
      "#        except:\n",
      "#            master_list = []\n",
      "#            master_list.extend(parents)            \n",
      "        #master_list = list(set(master_list))\n",
      "#        print('parents:',parents)\n",
      "#        print('master_list:',master_list)\n",
      "#        print(len(parents))\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print(final_master_list)\n",
      "162/129:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.','1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/130:\n",
      "# Now I have my keys in one column, and a list of values in another.\n",
      "# I'm going to...sigh...loop through each value and create a key value pair based on each.\n",
      "\n",
      "\n",
      "#lugggage_dictionary = ''\n",
      "\n",
      "#for x in range(len(day7data)):\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "\n",
      "\n",
      "# First, look for the color as the value\n",
      "\n",
      "luggage_color = ['shiny gold']\n",
      "bag_count = 0\n",
      "backtrack_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(luggage_color)):\n",
      "        if luggage_color[z] in day7data.iloc[x]['bag_children']:\n",
      "            bag_count += 1\n",
      "            backtrack_list.append(day7data.iloc[x]['bag_parents'])\n",
      "\n",
      "print(bag_count)\n",
      "print(backtrack_list)\n",
      "#    print(len(day7data.iloc[x]['luggage_values']))\n",
      "#    for z in range(len(day7data.iloc[x]['luggage_values'])):\n",
      "#        print('hi')\n",
      "162/131:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/132:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print(final_master_list)\n",
      "162/133:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print(len(final_master_list))\n",
      "162/134:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "# create a column that will become the key - split at \"contain\"\n",
      "print(day7data)\n",
      "162/135:\n",
      "# Option 2 - break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/136:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/137:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].index(' ')\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/138:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].index(' ')\n",
      "        print(position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/139:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but we're rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data)\n",
      "162/140:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].index(' ')\n",
      "        print(position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/141:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].index(' ')\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "#print(pairwise_df)\n",
      "162/142:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z]).s\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "#print(pairwise_df)\n",
      "162/143:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "#print(pairwise_df)\n",
      "162/144:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/145:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list = day7data.iloc[x]['bag_children'][z][:position].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/146:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/147:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/148:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        position.append(position)\n",
      "#        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/149:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/150:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position])\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/151:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip)\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/152:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip)\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z])\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/153:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/154:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "#        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z][:position].strip())\n",
      "        bag_quantity_list.append(position)\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/155:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "#        position.append(position)\n",
      "        print(day7data.iloc[x]['bag_children'][z], position)\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "#        bag_quantity_list.append(position)\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/156:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/157:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/158:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1)):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/159:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/160:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z])):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/161:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        bag_child_list.append = day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/162:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/163:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-10:].strip())\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/164:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(position+1):].strip())\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/165:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['luggage_rules'])\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/166:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['luggage_rules']))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/167:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['bag_children'][z]))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/168:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/169:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip()+position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/170:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())+position+1):].strip())\n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/171:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])\n",
      "        print(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "#        bag_child_list.append = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/172:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/173:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        print(len(day7data.iloc[x]['bag_children'][z].strip()))\n",
      "#        print(day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip())\n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "#        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/174:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "#position = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "#pairwise_df['position'] = position\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/175:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = dpairwise_df['bag_quantity'].replace(['no'],0)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/176:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/177:\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # filter pairwise_df down to any rows that have the bag color as the child\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print(len(final_master_list))\n",
      "162/178:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "    # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "    subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "    print(subset)\n",
      "\n",
      "number_of_bags('shiny gold',0)\n",
      "162/179:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags('shiny gold',0)\n",
      "162/180:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags('shiny gold',0)\n",
      "162/181:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags_inside('shiny gold',0)\n",
      "162/182:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):]\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags_inside('shiny gold',0)\n",
      "162/183:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags_inside('shiny gold',0)\n",
      "162/184:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/185:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        print(subset)\n",
      "        print(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/186:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "162/187:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # if there are parents, add them to the list and make the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print(len(final_master_list))\n",
      "162/188:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        print(subset)\n",
      "        print(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/189:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        \n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_children'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/190:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "#        children.extend(subset['bag_children'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/191:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/192:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        print(z)\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/193:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        print(children)\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/194:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        print(children)\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/195:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/196:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "        print(total_bags)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/197:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print(subset)\n",
      "        print('total bags:',total_bags)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/198:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "number_of_bags_inside(['shiny gold'],0)\n",
      "162/199:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "part_2_answer = number_of_bags_inside(['shiny gold'],0)\n",
      "print('part_2_answer:',part_2_answer)\n",
      "162/200:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # if there are parents, add them to the list and make the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/201:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "        print(master_list)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/202:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print(parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "        print(master_list)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/203:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "        print('master list:',master_list)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/204:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print('bag color:',bag_color[z])\n",
      "        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "        print('master list:',master_list)\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/205:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "        print('bag color:',bag_color[z])\n",
      "        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "        print('master list:',master_list)\n",
      "        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/206:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/207:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are children we need to look up, run the function again and pass in total_bags as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    print('final total_bags', total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "part_2_answer = number_of_bags_inside(['shiny gold'],0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/208:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are children we need to look up, run the function again and pass in total_bags as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    else:\n",
      "        print('final total_bags:',total_bags)\n",
      "        return(total_bags)\n",
      "\n",
      "part_2_answer = number_of_bags_inside(['shiny gold'],0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/209:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are children we need to look up, run the function again and pass in total_bags as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    else:\n",
      "        print('final total_bags:',total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "part_2_answer = number_of_bags_inside(['shiny gold'],0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/210:\n",
      "# for part 2, we'll need to do something similar, but we'll be going seeing what is inside the bag and how many\n",
      "\n",
      "def number_of_bags_inside(bag_color, total_bags):\n",
      "    children = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the parent\n",
      "        subset = pairwise_df[pairwise_df.bag_parent == bag_color[z]]\n",
      "        print('total bags:',total_bags)\n",
      "        print(subset)\n",
      "        total_bags += subset['bag_quantity'].sum()\n",
      "        # if there are children, add them to the list and make the list unique\n",
      "        children.extend(subset['bag_child'].tolist())\n",
      "        children = list(set(children))\n",
      "        # master_list just keeps growing with the number of parents\n",
      "#        master_list.extend(parents)\n",
      "    # If there are children we need to look up, run the function again and pass in total_bags as the second argument\n",
      "    if len(children) > 0:\n",
      "        number_of_bags_inside(children,total_bags)\n",
      "    print('final total_bags:',total_bags)\n",
      "    return(total_bags)\n",
      "\n",
      "part_2_answer = number_of_bags_inside(['shiny gold'],0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/211:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/212:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "\n",
      "# Now let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "pairwise\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_quantity == 0\n",
      "162/213:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "\n",
      "# Now let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_quantity == 0\n",
      "162/214:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(len(pairwise_df))\n",
      "print(pairwise_df)\n",
      "\n",
      "# Now let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "162/215:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "\n",
      "# Now let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(joel)\n",
      "162/216:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "joel_dict = {}\n",
      "joel = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(joel)\n",
      "162/217:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "for k in entries:\n",
      "    full_dict[entries.iloc[k]['bag_parents']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/218:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in entries:\n",
      "    full_dict[entries.iloc[k]['bag_parents']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/219:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in entries:\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/220:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in entries:\n",
      "    print([entries.iloc[k]['bag_parent'])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/221:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in entries:\n",
      "    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/222:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/223:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "162/224:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "list_of_parents = pairwise_df['bag_parent'].tolist()\n",
      "list_of_children = pairwise_df['bag_child'].tolist()\n",
      "print(list_of_parents)\n",
      "print(list_of_children)\n",
      "162/225:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = pairwise_df['bag_child'].tolist()\n",
      "print(list_of_parents)\n",
      "print(list_of_children)\n",
      "162/226:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "print(list_of_parents)\n",
      "print(list_of_children)\n",
      "162/227:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "print(list_of_parents)\n",
      "print(list_of_children)\n",
      "\n",
      "level_1_list = list(list_of_parents - list_of_children)\n",
      "print(level_1_list)\n",
      "162/228:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "print(list_of_parents)\n",
      "print(list_of_children)\n",
      "\n",
      "#level_1_list = list(list_of_parents - list_of_children)\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "162/229:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "print(level_1_bags)\n",
      "162/230:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/231:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/232:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/233:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data)\n",
      "162/234:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/235:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/236:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/237:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "162/238:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/239:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/240:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "162/241:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/242:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/243:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k] = 1\n",
      "print(full_dict)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/244:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/245:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict\n",
      "print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/246:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/247:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "level_2 = pairwise_df[pairwise_df['bag_parent']] in level_1_list\n",
      "\n",
      "print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/248:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "\n",
      "print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/249:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "full_dict = {}\n",
      "entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "print(level_1_list)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "    full_dict[level_1_list[k]] = 1\n",
      "print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "162/250:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print(mydf)\n",
      "162/251:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "target1 = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print(mydf)\n",
      "\n",
      "target2 = list(set(my_df['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/252:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "target1 = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print(mydf)\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/253:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target1 = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print(mydf)\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    print([mydf.iloc[k]['bag_child']])\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print(full_dict)\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/254:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target1 = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf:',mydf)\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    print([mydf.iloc[k]['bag_child']])\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/255:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target1 = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    print([mydf.iloc[k]['bag_child']])\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/256:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    print([mydf.iloc[k]['bag_child']])\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/257:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target2 = list(set(mydf['bag_child'].tolist()))\n",
      "print(target2)\n",
      "162/258:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "print(target)\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "162/259:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "162/260:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/261:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 6\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/262:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/263:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/264:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/265:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = full_dict[mydf.iloc[k]['bag_child']] + mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/266:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/267:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/268:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/269:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/270:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/271:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/272:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        print(multiplier)\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/273:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    print(multiplier)\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/274:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print(mydf.iloc[k]['bag_quantity'])\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/275:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "    print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/276:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print('child:',full_dict[mydf.iloc[k]['bag_child']])\n",
      "    print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "    print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/277:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = 0\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print('child:',full_dict[mydf.iloc[k]['bag_child']])\n",
      "    print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "    print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/278:\n",
      "# For part 2, let's figure out a total fit for each bag color\n",
      "# Start with the bags that have nothing in them\n",
      "\n",
      "#def determine_fit(bags):\n",
      "#    new_bags = []\n",
      "    \n",
      "#full_dict = {}\n",
      "#entries = pairwise_df[pairwise_df.bag_quantity == 0]\n",
      "#print(entries)\n",
      "\n",
      "#for k in range(len(entries)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[entries.iloc[k]['bag_parent']] = 1\n",
      "\n",
      "#print(full_dict)\n",
      "\n",
      "# Now go back in, filter original df to those with\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# Maybe we start with ones that are highest assembly? It's a parent but never a child?\n",
      "# Make a unique list of parents and children\n",
      "#list_of_parents = list(set(pairwise_df['bag_parent'].tolist()))\n",
      "#list_of_children = list(set(pairwise_df['bag_child'].tolist()))\n",
      "\n",
      "# Now find a list of bags that are never children\n",
      "#level_1_list = np.setdiff1d(list_of_parents,list_of_children)\n",
      "#print(level_1_list)\n",
      "\n",
      "#full_dict = {}\n",
      "\n",
      "#for k in range(len(level_1_list)):\n",
      "#    print([entries.iloc[k]['bag_parent']])\n",
      "#    full_dict[level_1_list[k]] = 1\n",
      "#print(full_dict)\n",
      "\n",
      "# Now find level 2 - bags that are the child of a level 1 parent:\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']] in full_dict.keys()\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent']].isin(level_1_list)\n",
      "#level_2 = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_2)\n",
      "\n",
      "#level_1_bags = pairwise_df[pairwise_df['bag_parent'].isin(level_1_list)]\n",
      "#print(level_1_bags)\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf1:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    else:\n",
      "        multiplier = 1\n",
      "    full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "print('dictionary 1:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "target = list(set(mydf['bag_child'].tolist()))\n",
      "mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "print('mydf2:',mydf)\n",
      "print('')\n",
      "\n",
      "for k in range(len(mydf)):\n",
      "    # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "    if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "        full_dict[mydf.iloc[k]['bag_child']] = 0\n",
      "    # Now we increment each child's key value as we iterate through the data frame.\n",
      "    # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "#    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "    multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "    # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "#    else:\n",
      "#        multiplier = 1\n",
      "    # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "    full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "    print('child:',full_dict[mydf.iloc[k]['bag_child']])\n",
      "    print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "    print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "print('dictionary 2:',full_dict)\n",
      "print('')\n",
      "\n",
      "\n",
      "# if bag child isn't in dictionary, add it.\n",
      "# Multiply it by the dictionary value of its parent, and if it doesn't exist multiply it by 1\n",
      "162/279:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(mydf)\n",
      "#    for k in range(len(mydf)):\n",
      "#        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "#        if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = 0\n",
      "#        # Now we increment each child's key value as we iterate through the data frame.\n",
      "#        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "#    #    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "#        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "#        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "#    #    else:\n",
      "#    #        multiplier = 1\n",
      "#        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "#        full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "#        print(full_dict)\n",
      "162/280:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(mydf)\n",
      "#    for k in range(len(mydf)):\n",
      "#        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "#        if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = 0\n",
      "#        # Now we increment each child's key value as we iterate through the data frame.\n",
      "#        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "#    #    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "#        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "#        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "#    #    else:\n",
      "#    #        multiplier = 1\n",
      "#        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "#        full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "#        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/281:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    mydf = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(mydf)\n",
      "#    for k in range(len(mydf)):\n",
      "#        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "#        if mydf.iloc[k]['bag_child'] not in full_dict:\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = mydf.iloc[k]['bag_quantity']\n",
      "#            full_dict[mydf.iloc[k]['bag_child']] = 0\n",
      "#        # Now we increment each child's key value as we iterate through the data frame.\n",
      "#        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "#    #    if (mydf.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "#        multiplier = full_dict[mydf.iloc[k]['bag_parent']]\n",
      "#        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "#    #    else:\n",
      "#    #        multiplier = 1\n",
      "#        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "#        full_dict[mydf.iloc[k]['bag_child']] += mydf.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',mydf.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "#        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/282:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "    #    if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "    #    else:\n",
      "    #        multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/283:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "    #    if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        print(full_dict)\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "    #    else:\n",
      "    #        multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/284:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "    #    if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "        print(full_dict)\n",
      "        multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "    #    else:\n",
      "    #        multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/285:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            print(full_dict)\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/286:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(next_round)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "        print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/287:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(next_round)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/288:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/289:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "    print(full_dict)\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/290:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "    print(full_dict)\n",
      "    return(sum(full_dict.values))\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/291:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "determine_fit(['shiny gold'])\n",
      "162/292:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/293:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "162/294:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/295:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/296:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/297:\n",
      "# Now move it to a function:\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    print(next_round)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/298:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    print(len(next_round))\n",
      "    print(next_round)\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/299:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/300:\n",
      "paths = 0\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent in target]\n",
      "\n",
      "print(joel)\n",
      "162/301:\n",
      "paths = 0\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "\n",
      "print(joel)\n",
      "162/302:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "#for k in len(joel):\n",
      "#    parent_multiplier = joel[k]['bag_quantity']\n",
      "#    joel2 = \n",
      "\n",
      "print(joel)\n",
      "162/303:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in len(joel):\n",
      "    parent_multiplier = joel[k]['bag_quantity']\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel[k]['bag_child']]\n",
      "    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/304:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel[k]['bag_quantity']\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel[k]['bag_child']]\n",
      "    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/305:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "#    joel2 = pairwise_df[pairwise_df.bag_parent == joel[k]['bag_child']]\n",
      "#    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/306:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "#    joel2 = pairwise_df[pairwise_df.bag_parent == joel[k]['bag_child']]\n",
      "#    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/307:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel[k]['bag_child']]\n",
      "    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/308:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/309:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/310:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    print(joel2)\n",
      "    \n",
      "\n",
      "print(joel)\n",
      "162/311:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "print(joel)\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    joel2 = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    print(joel2)\n",
      "162/312:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials = pd.DataFrame()\n",
      "print(joel)\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    print(to_append)\n",
      "162/313:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "print(joel)\n",
      "\n",
      "multiplied_quantity = []\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    print(to_append)\n",
      "162/314:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "print(joel)\n",
      "\n",
      "multiplied_quantity = []\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#    df['quantity'] = df['quantity'].apply(lambda x: x*-1)\n",
      "    print(to_append)\n",
      "162/315:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "multiplied_quantity = []\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = df['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append)\n",
      "162/316:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "multiplied_quantity = []\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append)\n",
      "162/317:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append)\n",
      "162/318:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "#    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "162/319:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "#    print(parent_multiplier)\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    # start out by duplicating the column\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "print(level_bom)\n",
      "162/320:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "print(level_bom)\n",
      "162/321:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "print(level_bom)\n",
      "print(len(level_bom))\n",
      "162/322:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "print(level_bom)\n",
      "print(len(level_bom))\n",
      "162/323:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "print(level_bom)\n",
      "print(len(level_bom))\n",
      "162/324:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/325:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    print(to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/326:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    print(to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/327:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/328:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print(level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/329:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:'level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/330:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    print(to_append2)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/331:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append2)\n",
      "print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/332:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append.iloc[k]['bag_quantity'], to_append2)\n",
      "print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/333:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append.iloc[k]['bag_parent'], to_append2)\n",
      "print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/334:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(to_append)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = to_append.iloc[k]['bag_quantity']\n",
      "    to_append2 = pairwise_df[pairwise_df.bag_parent == to_append.iloc[k]['bag_child']]\n",
      "    to_append2['multiplied_quantity'] = to_append2['bag_quantity']\n",
      "    to_append2['multiplied_quantity'] = to_append2['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append.iloc[k]['bag_parent'], to_append2)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/335:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "    print(level_bom)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/336:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "162/337:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "162/338:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "162/339:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "162/340:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "162/341:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/1:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/2:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/3:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/4:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/5:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/6:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/7:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/8:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/9:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "#print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/10:\n",
      "paths = 0\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/11:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "\n",
      "    for k in range(len(level_bom)):\n",
      "    #    print('to append:',to_append)\n",
      "        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    #    level_bom = level_bom.append(to_append2)\n",
      "    #    print(to_append)\n",
      "    #print(level_bom)\n",
      "    #print(len(level_bom))\n",
      "164/12:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/13:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "level_bom = pd.DataFrame()\n",
      "\n",
      "def total_fit(target):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "\n",
      "total_fit('shiny gold')\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/14:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "level_bom = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,level_bom):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/15:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "level_bom = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,level_bom):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_column'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/16:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "level_bom = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,level_bom):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/17:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,level_bom):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/18:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "\n",
      "#joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "#joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "#print(joel)\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,level_bom):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/19:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/20:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/21:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/22:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/23:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pd.DataFrame()\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "print(joel)\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/24:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "print(bill_of_materials)\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/25:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/26:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/27:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/28:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/29:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/30:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/31:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/32:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/33:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/34:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/35:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/36:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "bill_of_materials = bill_of_materials\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/37:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',pd.DataFrame())\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/38:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print(bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/39:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(level_bom['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/40:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/41:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_bigger.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/42:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/43:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/44:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/45:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/46:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)    \n",
      "    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/47:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "        if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/48:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/49:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/50:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/51:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    initial_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = initial_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == initial_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "#    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/52:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/53:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/54:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/55:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/56:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/57:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/58:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again')\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/59:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =,'len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/60:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "#    for k in range(len(level_bom)):\n",
      "#    #    print('to append:',to_append)\n",
      "#        parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    #    level_bom = level_bom.append(to_append2)\n",
      "#    #    print(to_append)\n",
      "#    #print(level_bom)\n",
      "#    #print(len(level_bom))\n",
      "164/61:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/62:\n",
      "# convert to a function\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/63:\n",
      "# convert to a function\n",
      "\n",
      "print(pairwise_df)\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/64:\n",
      "# convert to a function\n",
      "\n",
      "print(pairwise_df)\n",
      "\n",
      "target = 'shiny gold'\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent == target]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print(level_bom)\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/65:\n",
      "# convert to a function\n",
      "\n",
      "print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent == target]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/66:\n",
      "# convert to a function\n",
      "\n",
      "print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit('shiny gold',bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/67:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(level_bom,bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/68:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/69:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_bigger.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/70:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/71:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/72:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/73:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/74:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/75:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/76:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print('hi!')\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.bag_parent.isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/77:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df.['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.['bag_parent'].isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/78:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df.['bag_parent'].isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/79:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/80: bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "164/81:\n",
      "print(type(target))\n",
      "#bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "164/82:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "#    print(bill_of_materials)\n",
      "#    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print('target_data')\n",
      "#    print(target_data)\n",
      "#    level_bom = pd.DataFrame()\n",
      "#    for k in range(len(joel)):\n",
      "#        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "#    print(len(level_bom))\n",
      "#    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print('')\n",
      "#        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "##    print('level_bom:',level_bom)\n",
      "##    print('bill_of_materials:',bill_of_materials)\n",
      "#    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/83:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "#def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "#    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print('target_data')\n",
      "#    print(target_data)\n",
      "#    level_bom = pd.DataFrame()\n",
      "#    for k in range(len(joel)):\n",
      "#        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "#    print(len(level_bom))\n",
      "#    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print('')\n",
      "#        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "##    print('level_bom:',level_bom)\n",
      "##    print('bill_of_materials:',bill_of_materials)\n",
      "#    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/84:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "#def total_fit(target,bill_of_materials):\n",
      "#    print(target)\n",
      "#    print(bill_of_materials)\n",
      "#    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print('target_data')\n",
      "#    print(target_data)\n",
      "#    level_bom = pd.DataFrame()\n",
      "#    for k in range(len(joel)):\n",
      "#        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "#        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "#        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "#    print(len(level_bom))\n",
      "#    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print('')\n",
      "#        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "##    print('level_bom:',level_bom)\n",
      "##    print('bill_of_materials:',bill_of_materials)\n",
      "#    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "#part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "#print('Part 2 answer:',part_2_answer)\n",
      "164/85:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(target)\n",
      "    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print('target_data')\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/86:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print(bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/87:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/88:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/89:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/90:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/91:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/92:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_bigger.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/93:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/94:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/95:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/96:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/97:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    print(len(level_bom))\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/98:\n",
      "print(type(target))\n",
      "#bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "164/99:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/100:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/101:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/102:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/103:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/104:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/105:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/106:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/107:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_bigger.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/108:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/109:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/110:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/111:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/112:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/113:\n",
      "print(type(target))\n",
      "#bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "164/114:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        print('parent_multiplier:',parent_multiplier)\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/115:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "        print('to_append:',to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/116:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/117:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/118:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/119:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(joel)):\n",
      "#        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/120:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_parent']==]\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/121:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "#        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/122:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "        print('look here:',level_bom)\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/123:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/124:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/125:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/126:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, add keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/127:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/128:\n",
      "target = 'shiny gold'\n",
      "\n",
      "joel = pairwise_df[pairwise_df.bag_parent == target]\n",
      "joel['multiplied_quantity'] = joel['bag_quantity']\n",
      "print(joel)\n",
      "\n",
      "bill_of_materials = pd.DataFrame()\n",
      "\n",
      "level_bom = pd.DataFrame()\n",
      "for k in range(len(joel)):\n",
      "    parent_multiplier = joel.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == joel.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "    level_bom = level_bom.append(to_append)\n",
      "#    print(to_append)\n",
      "print('level_bom:',level_bom)\n",
      "\n",
      "for k in range(len(level_bom)):\n",
      "#    print('to append:',to_append)\n",
      "    parent_multiplier = level_bom.iloc[k]['bag_quantity']\n",
      "    to_append = pairwise_df[pairwise_df.bag_parent == level_bom.iloc[k]['bag_child']]\n",
      "    to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "    to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "#    level_bom = level_bom.append(to_append2)\n",
      "#    print(to_append)\n",
      "#print(level_bom)\n",
      "#print(len(level_bom))\n",
      "164/129:\n",
      "# convert to a function\n",
      "\n",
      "#print(pairwise_df)\n",
      "\n",
      "target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/130:\n",
      "print(type(target))\n",
      "#bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "164/131:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/132:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "data_file_name = 'day_07_input_small.csv'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + data_file_name, \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/133:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(data_file_name +'.csv', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/134:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(data_file_name +'_bill_of_materials.csv', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/135:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(data_file_name +'_bill_of_materials', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/136:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "data_file_name = 'day_07_input.csv'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + data_file_name, \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/137:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(data_file_name +'_bill_of_materials', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/138:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + data_file_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/139:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/140:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/141:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + data_file_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/142:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/143:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/144:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/145:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "164/146:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/147:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/148:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/149:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/150:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/151:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input_biggest'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/152:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "164/153:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/154:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/155:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/156:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_child'].tolist())),bill_of_materials,level)\n",
      "#    print('level_bom:',level_bom)\n",
      "    bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/157:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/158:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "    else:\n",
      "        bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/159:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/160:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "164/161:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/162:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/163:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "    else:\n",
      "        bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/164:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input_small'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/165:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "164/166:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/167:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/168:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "164/169:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "164/170:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "if subset.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "    quantity[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "164/171:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if subset.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "164/172:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "164/173:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "# Make a list of all parents who have a child in the dictionary\n",
      "parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "        \n",
      "print(quantity_dict)\n",
      "164/174:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary\n",
      "parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "print(parent_candidates)\n",
      "164/175:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary\n",
      "parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "# And now we've got to see all children for those\n",
      "print(list(set(parent_candidates['bag_parent'].tolist())))\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/176:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "# And now we've got to see all children for those\n",
      "print(list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist())))\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/177:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candiates(list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for l in range(len(parent_candidates)):\n",
      "                 print('hi!')\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/178:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candiates = (list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for l in range(len(parent_candidates)):\n",
      "                    print('hi!')\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/179:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = (list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "print(len(parent_candidates))\n",
      "                    #for l in range(len(parent_candidates)):\n",
      "#                    print('hi!')\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/180:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "print(len(parent_candidates))\n",
      "                    #for l in range(len(parent_candidates)):\n",
      "#                    print('hi!')\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/181:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for l in range(len(parent_candidates)):\n",
      "    print(parent_candidates[l])\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/182:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[k]]\n",
      "    print(joel)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/183:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidate)\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[k]]\n",
      "    print(joel)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/184:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[k]]\n",
      "    print(joel)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/185:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    print(joel)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/186:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    print(joel)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/187:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/188:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    for p in range(len(joellist)):\n",
      "        print('Hi')\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/189:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/190:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "#        for \n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/191:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += first.iloc[k]['bag_quantity'] * (1+quantity_dict['bag_child'])\n",
      "            \n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/192:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            \n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/193:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        print(quantity)\n",
      "            \n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/194:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "#    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        quantity_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(quantity)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/195:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "#    print(joel)\n",
      "#    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        quantity_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(quantity)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/196:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        quantity_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(quantity)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/197:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    temp_dict = {}\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(quantity)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/198:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    temp_dict = {}\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(quantity)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/199:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    temp_dict = {}\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/200:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    temp_dict = {}\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/201:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/202:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "#quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/203:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "                 \n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/204:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/205:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/206:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/207:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_list = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_list.append(a)\n",
      "print(filtered_list)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/208:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/209:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/210:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/211:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/212:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/213:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/214:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict:\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/215:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/216:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/217:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    print(quantity_dict.keys())\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/218:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!\")\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/219:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print(joellist)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/220:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/221:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/222:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "temp_dict={}\n",
      "# And now we've got to go one by one to see all children for those\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # filter the data to all entries with each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Is there where I should eliminate parents who are already in the dictionary?\n",
      "    # turn the children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print(joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print(quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity in the bag\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/223:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"yay!, We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/224:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print(temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/225:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/226:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "    print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/227:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "    print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "    print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/228:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/229:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for y in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/230:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in joellist for x in quantity_dict.keys()):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/231:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/232:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "        \n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "164/233:\n",
      "def bag_quantities(quantity_dict)        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "#print(quantity_dict)\n",
      "164/234:\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "#print(quantity_dict)\n",
      "164/235:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/236:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if len(filtered_parent_candidates) == 0:\n",
      "        break\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/237:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/238:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/239:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/240:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/241:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/242:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            print('len(joellist):'len(joellist))\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/243:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            print('len(joellist):',len(joellist))\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/244:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "#            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/245:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                print('hi')\n",
      "#                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/246:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start with all the 0 bags\n",
      "\n",
      "#first = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "\n",
      "quantity_dict = {}\n",
      "first = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "#print(first)\n",
      "for k in range(len(first)):\n",
      "    if first.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[first.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "print('')\n",
      "print('next iteration')\n",
      "print('')\n",
      "\n",
      "#print(quantity_dict)\n",
      "# Make a list of all parents who have a child in the dictionary.\n",
      "parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "print(parent_candidates)\n",
      "# Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "filtered_parent_candidates = []\n",
      "for a in parent_candidates:\n",
      "    if a not in quantity_dict:\n",
      "        filtered_parent_candidates.append(a)\n",
      "print(filtered_parent_candidates)\n",
      "# Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "temp_dict={}\n",
      "# Now look at each parent in the filtered parent candidates\n",
      "for j in range(len(filtered_parent_candidates)):\n",
      "    # Filter the data to all entries for each parent\n",
      "    joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "    # Turn the list of the parent's children into a list\n",
      "    joellist = list(set(joel['bag_child'].tolist()))\n",
      "    print('')\n",
      "    print('joel:',joel)\n",
      "    print('joellist before test:',joellist)\n",
      "    print('quantity_dict before test:',quantity_dict)\n",
      "    # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "    if all(x in quantity_dict.keys() for x in joellist):\n",
      "        print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "        quantity = 0\n",
      "        for x in range(len(joellist)):\n",
      "            quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "        temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "        print('temp_dict:',temp_dict)\n",
      "quantity_dict.update(temp_dict)\n",
      "\n",
      "print(quantity_dict)\n",
      "\n",
      "\n",
      "#parent_candidates = pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]\n",
      "\n",
      "\n",
      "#print(parent_candidates)\n",
      "164/247:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                print('hi')\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[k]['bag_parent']] = quantity\n",
      "            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/248:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "        print('')\n",
      "        print('joel:',joel)\n",
      "        print('joellist before test:',joellist)\n",
      "        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                print('hi')\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/249:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/250:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        bag_quantities(quantity_dict)\n",
      "\n",
      "bag_quantities(quantity_dict)\n",
      "164/251:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials,level):\n",
      "    level += 1\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      " #       print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        to_append['level'] = level\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials,level)\n",
      "    else:\n",
      "        bill_of_materials.to_csv(csv_name +'_bill_of_materials.csv', index=False)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials,0)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/252:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        bag_quantities(quantity_dict)\n",
      "    return(bag_quantities[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/253:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(bag_quantities[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/254:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantities_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/255:\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print('start:',quantity_dict)\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "#    print(parent_candidates)\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "#    print(filtered_parent_candidates)\n",
      "    # Create a temporary dictionary just for this iteration, we'll append it to the quantity_dict\n",
      "    temp_dict={}\n",
      "    # Now look at each parent in the filtered parent candidates\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/256:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        joel = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(joel['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('joel:',joel)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += joel.iloc[x]['bag_quantity'] * (1+quantity_dict[joel.iloc[x]['bag_child']])\n",
      "            temp_dict[joel.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/257:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        joellist = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('joellist before test:',joellist)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in joellist):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(joellist)):\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print('part_2_answer:',part_2_answer)\n",
      "164/258:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/259:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    print('end:',quantity_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'bright white')\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/260:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'bright white')\n",
      "print(quantity_dict)\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/261:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']]),\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'bright white')\n",
      "print(quantity_dict)\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/262:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'bright white')\n",
      "print(quantity_dict)\n",
      "print('part_2_answer:',part_2_answer)\n",
      "164/263:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input_biggest'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "164/264:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "164/265:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/266:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'bright white')\n",
      "print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/267:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/268:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/269:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input_biggest'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data.head())\n",
      "164/270:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "#pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "\n",
      "print(pairwise_df.head())\n",
      "164/271:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data.head())\n",
      "164/272:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "#pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "\n",
      "print(pairwise_df.head())\n",
      "164/273:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/274:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/275:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every colored bag\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "quantity_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        quantity_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(quantity_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/276:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/277:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data.head())\n",
      "164/278:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "#pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "\n",
      "print(pairwise_df.head())\n",
      "164/279:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/280:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/281:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in quantity_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(zero_dict)\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/282:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "print(zero_dict)\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/283:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "joel = pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside'])\n",
      "print(joel.head())\n",
      "164/284:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "joel = pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside'])\n",
      "print(joel.tail())\n",
      "164/285:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color):        \n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold')\n",
      "#print(quantity_dict)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "#joel = pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside'])\n",
      "#print(joel.tail())\n",
      "\n",
      "pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)\n",
      "164/286:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color,iterations):\n",
      "    iterations +=1\n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color,iterations)\n",
      "    print(iterations)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold',0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "# Write my final dictionary to a csv\n",
      "#pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)\n",
      "164/287:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "csv_name = 'day_07_input'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + csv_name + '.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "print(day7data.head())\n",
      "164/288:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "# Print it to a csv\n",
      "pairwise_df.to_csv(csv_name +'_pairwise.csv', index=False)\n",
      "\n",
      "print(pairwise_df.head())\n",
      "164/289:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "164/290:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color,iterations):\n",
      "    iterations +=1\n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color,iterations)\n",
      "#    print(iterations)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold',0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "# Write my final dictionary to a csv\n",
      "#pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)\n",
      "164/291:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "# Long story, but it's a cool function.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "164/292:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color,iterations):\n",
      "    iterations +=1\n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color,iterations)\n",
      "    print(iterations)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold',0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "# Write my final dictionary to a csv\n",
      "#pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)\n",
      "164/293:\n",
      "# Finally going to do part 2 right!\n",
      "\n",
      "# Start by passing creating a dictionary made from all the bags that have no bags in them.\n",
      "# Those are the only bags that we initially know the contents of (0 bags).\n",
      "# This is the lowest level, we'll work our way up to determine how many bags are in every bag by color\n",
      "# The trick is we have to iterate upwards, and can only determine the total of the bags inside after\n",
      "# we know how many bags are inside ALL of its children\n",
      "# We will create a dictionary that gets populated with the final number of bags contained by each colored bag\n",
      "\n",
      "# Start by creating a dictionary with each of the bags that have no bags inside them\n",
      "# They go into the dictionary with a value of zero in the dictionary.\n",
      "zero_dict = {}\n",
      "lowest_level = pairwise_df[pairwise_df['bag_quantity'] == 0]\n",
      "for k in range(len(lowest_level)):\n",
      "    if lowest_level.iloc[k]['bag_parent'] not in zero_dict:\n",
      "        zero_dict[lowest_level.iloc[k]['bag_parent']] = 0\n",
      "\n",
      "def bag_quantities(quantity_dict,color,iterations):\n",
      "    iterations +=1\n",
      "    # Make a list of all parents who have a child in the dictionary.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    # Create a temporary dictionary just for each iteration, we'll append it to the quantity_dict at the end\n",
      "    temp_dict={}\n",
      "    # Now go through each parent in the filtered parent candidates individually\n",
      "    for j in range(len(filtered_parent_candidates)):\n",
      "        # Filter the data to all entries for each parent, not just the entries with children already in the dictionary\n",
      "        parent_df = pairwise_df[pairwise_df['bag_parent'] == filtered_parent_candidates[j]]\n",
      "        # Turn the list of the parent's children into a list\n",
      "        parent_list = list(set(parent_df['bag_child'].tolist()))\n",
      "#        print('')\n",
      "#        print('parent_df:',parent_df)\n",
      "#        print('parent_list before test:',parent_list)\n",
      "#        print('quantity_dict before test:',quantity_dict)\n",
      "        # If ALL the children of this parent are already in the dictionary, then we can calculate the total quantity and add this parent to the dictionary\n",
      "        if all(x in quantity_dict.keys() for x in parent_list):\n",
      "#            print(\"Yay! We can do\", filtered_parent_candidates[j])\n",
      "            quantity = 0\n",
      "            for x in range(len(parent_list)):\n",
      "                # So for each parent, we want to add the quantity of each child bag\n",
      "                # PLUS the number of bags each of those child bags contain (which we already know from the dictionary)\n",
      "                quantity += parent_df.iloc[x]['bag_quantity'] * (1+quantity_dict[parent_df.iloc[x]['bag_child']])\n",
      "            # Then append this entry to the temporary dictionary\n",
      "            temp_dict[parent_df.iloc[x]['bag_parent']] = quantity\n",
      "#            print('temp_dict:',temp_dict)\n",
      "    # We can't append the temp_dict to the quantity_dict until after we've gone through all the parent candidates\n",
      "    # Otherwise it could mess up the next candidate in the list\n",
      "    quantity_dict.update(temp_dict)\n",
      "    # probably inefficient but now we need to determine whether to run it through again, repeating the top step to see if there are still parents to test.\n",
      "    parent_candidates = list(set(pairwise_df[pairwise_df['bag_child'].isin(quantity_dict)]['bag_parent'].tolist()))\n",
      "    # Remove parent_candidates who are already in the dictionary, we don't need to do them again\n",
      "    filtered_parent_candidates = []\n",
      "    for a in parent_candidates:\n",
      "        if a not in quantity_dict:\n",
      "            filtered_parent_candidates.append(a)\n",
      "    if (len(filtered_parent_candidates)) > 0:\n",
      "        color = color\n",
      "        bag_quantities(quantity_dict,color,iterations)\n",
      "#    print(iterations)\n",
      "    return(quantity_dict[color])\n",
      "\n",
      "part_2_answer = bag_quantities(zero_dict,'shiny gold',0)\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "\n",
      "# Write my final dictionary to a csv\n",
      "#pd.DataFrame(quantity_dict.items(), columns=['bag', 'total_bags_inside']).to_csv(csv_name +'_quantity_dict.csv', index=False)\n",
      "   1:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input_small.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "   2:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "   3:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "   4:\n",
      "import itertools\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import collections\n",
      "import re\n",
      "\n",
      "# Challenge: https://adventofcode.com/2020/day/7\n",
      "\n",
      "# Import csv\n",
      "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/Advent_of_Code/Advent_of_Code/Week05_Day07/'\n",
      "\n",
      "column = ['luggage_rules']\n",
      "day7data = pd.read_csv(data_file_loc + 'day_07_input.csv', \n",
      "                       header = None, \n",
      "                       names = column, \n",
      "                       skip_blank_lines=False\n",
      "                      )\n",
      "\n",
      "bag_parents = []\n",
      "bag_children = []\n",
      "luggage_rule_string = 'contain'\n",
      "characters_to_remove = [' bags', ' bag', '.']\n",
      "#,'1','2','3','4','5','6','7','8','9','  ']\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    # First, find the position of the separator between key and value\n",
      "    position = day7data.iloc[x]['luggage_rules'].index(luggage_rule_string)\n",
      "    # Now take the left-most number of characters based on that value\n",
      "    bag_parent = day7data.iloc[x]['luggage_rules'][:position].strip()\n",
      "    # The value is made up of the right characters, probably an easier way to do this but I'm rolling with it\n",
      "    bag_child = day7data.iloc[x]['luggage_rules'][-(len(day7data.iloc[x]['luggage_rules'])-position-len(luggage_rule_string)):].strip()\n",
      "    # From both columns, remove a list of strings that we'll no longer need - ' bag', ' bags','.', all numbers\n",
      "    # Has to be an easier way!\n",
      "    for char in characters_to_remove:\n",
      "        bag_parent = bag_parent.replace(char, '').strip()\n",
      "        bag_child = bag_child.replace(char, '').strip()\n",
      "#    rule_key.translate({ord(z): '' for z in characters_to_remove})\n",
      "#    rule_key = re.sub(\"|\".join(characters_to_remove), \"\", rule_key)\n",
      "#    rule_value = re.sub(\"|\".join(characters_to_remove), \"\", rule_value)\n",
      "    # turn the values into a list\n",
      "    bag_child = bag_child.split(',')\n",
      "    bag_parents.append(bag_parent)\n",
      "    bag_children.append(bag_child)\n",
      "\n",
      "day7data['bag_parents'] = bag_parents\n",
      "day7data['bag_children'] = bag_children\n",
      "\n",
      "#print(day7data)\n",
      "   5:\n",
      "# Now break apart the lists of children into individual rows\n",
      "\n",
      "pairwise_df = pd.DataFrame()\n",
      "bag_quantity_list = []\n",
      "bag_parent_list = []\n",
      "bag_child_list = []\n",
      "\n",
      "for x in range(len(day7data)):\n",
      "    for z in range(len(day7data.iloc[x]['bag_children'])):\n",
      "        bag_parent_list.append(day7data.iloc[x]['bag_parents'])\n",
      "        position = day7data.iloc[x]['bag_children'][z].strip().index(' ')\n",
      "        # Now separate the quantity from the name of the color, same exercise as above\n",
      "        bag_quantity_list.append(day7data.iloc[x]['bag_children'][z].strip()[:position])        \n",
      "        bag_child_list.append(day7data.iloc[x]['bag_children'][z].strip()[-(len(day7data.iloc[x]['bag_children'][z].strip())-position-1):].strip())\n",
      "\n",
      "pairwise_df['bag_parent'] = bag_parent_list\n",
      "pairwise_df['bag_child'] = bag_child_list\n",
      "pairwise_df['bag_quantity'] = bag_quantity_list\n",
      "\n",
      "# Replace 'no' with zero in the bag_quantity field\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].replace(['no'],0)\n",
      "# Then make sure it's an integer\n",
      "pairwise_df['bag_quantity'] = pairwise_df['bag_quantity'].astype(int)\n",
      "#print(pairwise_df)\n",
      "   6:\n",
      "# for part 1:\n",
      "\n",
      "def number_of_bags(bag_color,master_list):\n",
      "    # parents will keep a list of all the parents of the bag color we're looking up so that we can rerun the fuction\n",
      "    # master_list is the overall list of all bag colors that could contain the original bag color\n",
      "    parents = []\n",
      "    for z in range(len(bag_color)):\n",
      "        # filter pairwise_df up to any rows that have the bag color as the child\n",
      "        subset = pairwise_df[pairwise_df.bag_child == bag_color[z]]\n",
      "        # Add the parents to the list of parents, and keep the list unique\n",
      "        parents.extend(subset['bag_parent'].tolist())\n",
      "        parents = list(set(parents))\n",
      "#        print('bag color:',bag_color[z])\n",
      "#        print('parents:',parents)\n",
      "        # The master_list just keeps growing with the number of parents\n",
      "        master_list.extend(parents)\n",
      "#        print('master list:',master_list)\n",
      "#        print('')\n",
      "    # If there are parents we need to look up, run the function again and pass in master_list as the second argument\n",
      "    # This clears out the list of parents for the next iteration when the function starts, but keeps the master list\n",
      "    if len(parents) > 0:\n",
      "        number_of_bags(parents,master_list)\n",
      "    return(list(set(master_list)))\n",
      "\n",
      "final_master_list = number_of_bags(['shiny gold'],[])\n",
      "print('Part 1 answer:',len(final_master_list))\n",
      "   7:\n",
      "# Part 2 - this function doesn't work on the big problem, but does for the small problem.\n",
      "\n",
      "full_dict = {}\n",
      "\n",
      "target = ['shiny gold']\n",
      "\n",
      "def determine_fit(target):\n",
      "    subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(subset)\n",
      "    # Create the next round of children:\n",
      "    next_round = list(set(subset['bag_child'].tolist()))\n",
      "    for k in range(len(subset)):\n",
      "        # If the child does not have an entry in the dictionary, add it with a value of 0\n",
      "        if subset.iloc[k]['bag_child'] not in full_dict:\n",
      "            full_dict[subset.iloc[k]['bag_child']] = 0\n",
      "        # Now we increment each child's key value as we iterate through the data frame.\n",
      "        # If the child's parent is also in the dictionary, multiply the child's quantity by the parent's total quantity\n",
      "        if (subset.iloc[k]['bag_parent'] in full_dict.keys()):\n",
      "            multiplier = full_dict[subset.iloc[k]['bag_parent']]\n",
      "        # If the parent doesn't exist, just start with a value of 1 (not sure this is ever needed)\n",
      "        else:\n",
      "            multiplier = 1\n",
      "        # Now increment the child's value by it's quantity times the multiplier (which is how many parents exist)\n",
      "        full_dict[subset.iloc[k]['bag_child']] += subset.iloc[k]['bag_quantity'] * multiplier\n",
      "#        print('quantity:',subset.iloc[k]['bag_quantity'])\n",
      "#        print('multiplier:',multiplier)\n",
      "    if len(next_round) > 0:\n",
      "        determine_fit(next_round)\n",
      "#    print(full_dict)\n",
      "    return(sum(full_dict.values()))\n",
      "\n",
      "part_2_answer = determine_fit(['shiny gold'])\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "   8:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "   9:\n",
      "# Part 2 again\n",
      "\n",
      "#target = ['shiny gold']\n",
      "bill_of_materials = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#subset = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "bill_of_materials['multiplied_quantity'] = bill_of_materials['bag_quantity']\n",
      "#print('bill of materials outside function',bill_of_materials)\n",
      "\n",
      "def total_fit(target,bill_of_materials):\n",
      "#    print('first bill of materials',bill_of_materials)\n",
      "    target_data = pairwise_df[pairwise_df['bag_parent'].isin(target)]\n",
      "#    print(target_data)\n",
      "    level_bom = pd.DataFrame()\n",
      "    for k in range(len(target_data)):\n",
      "        parent_multiplier = target_data.iloc[k]['bag_quantity']\n",
      "#        parent_multiplier = level_bom[level_bom['bag_child']==target_data.iloc[k]['bag_parent']]['multiplied_quantity']\n",
      "        to_append = pairwise_df[pairwise_df.bag_parent == target_data.iloc[k]['bag_child']]\n",
      "        to_append['multiplied_quantity'] = to_append['bag_quantity']\n",
      "        print('to_append:',to_append)\n",
      "        to_append['multiplied_quantity'] = to_append['multiplied_quantity'].apply(lambda x: x*parent_multiplier)\n",
      "        level_bom = level_bom.append(to_append)\n",
      "    print('level_bom:',level_bom)\n",
      "    bill_of_materials = bill_of_materials.append(level_bom)\n",
      "    if len(level_bom) > 0:\n",
      "#        print('doing it again because len(level_bom) =',len(level_bom))\n",
      "#        print(list(set(level_bom['bag_parent'].tolist())))\n",
      "#        print('')\n",
      "        total_fit(list(set(level_bom['bag_parent'].tolist())),bill_of_materials)\n",
      "#    print('level_bom:',level_bom)\n",
      "#    print('bill_of_materials:',bill_of_materials)\n",
      "    return(bill_of_materials['multiplied_quantity'].sum())\n",
      "\n",
      "part_2_answer = total_fit(['shiny gold'],bill_of_materials)\n",
      "\n",
      "print('Part 2 answer:',part_2_answer)\n",
      "  10: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
