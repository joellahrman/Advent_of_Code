{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DoD:882\n",
      "DoC:68\n",
      "DoA:309\n",
      "ED simple:295\n",
      "DHS full:310\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 34 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-b26c37a41ca6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;31m# If a position does not report to another position, then T2 needs to be incremented down so that the Y value of the start node gets incremented down to the previous level\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;31m# final_data_with_coords['T2'].where(~(final_data_with_coords.end_node_level == final_data_with_coords.start_node_level), other=final_data_with_coords.T2 + 2 * T_initializer, inplace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mfinal_data_with_coords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_data_with_coords'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdepartment_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[0mbranch_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 34 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "# T:\n",
    "# T2:\n",
    "# P1: This is the x-coordinate of the parent position\n",
    "# P2: This is the x-coordinate of the position\n",
    "# Branch:\n",
    "# Stat/Dyn - stat shows the whole org chart, dyn only allows nodes to expand/contract\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
    "\n",
    "path = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DoS_Viz/Data/2021_04_08/'\n",
    "\n",
    "sheets = pd.read_excel(path + 'dept_row_data_for_testing.xlsx', sheet_name = None)\n",
    "\n",
    "# blank data frame to append every sheet\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "# rename level to 'end_node_level'\n",
    "for name, sheet in sheets.items():\n",
    "    sheet['sheet'] = name\n",
    "    sheet = sheet.rename(columns=lambda x: x.split('\\n')[-1])\n",
    "    sheet.columns = ['row_id','end_node_level','position_category','position_name','details', 'detail_count', 'pos_no','inner_count','reports_to_row_id','to_plot', 'department']\n",
    "    final_data = final_data.append(sheet)\n",
    "    \n",
    "#final_data['details'] = re.sub(\\\"[^\\\\w\\\\d'\\\\s]+\\\", '')\n",
    "\n",
    "# start a loop here to handle each individual department, and a data frame to hold all final data\n",
    "final_curve_data = pd.DataFrame()\n",
    "department_list = final_data.department.unique()\n",
    "\n",
    "for x in range(len(department_list)):\n",
    "    \n",
    "    department_data = final_data[final_data.department == department_list[x]]\n",
    "    \n",
    "    print(department_list[x] + \":\" + str(len(department_data)))\n",
    "    \n",
    "    # iterate through the levels in reverse order - will probably be better to do it this way when trying to center the parent nodes\n",
    "\n",
    "    # determine maximum level in the data\n",
    "    max_level = int(department_data[\"end_node_level\"].max())\n",
    "    # create blank data frame to append the records with updated fields\n",
    "    final_data_with_coords = pd.DataFrame()\n",
    "    # create a counter for the end_node_id, this just needs to stay unique in the data frame\n",
    "    end_node_id = 1\n",
    "\n",
    "    # Set a value to iterate the P2 coordinate for each row\n",
    "    P2_static_iterator = 3\n",
    "    # Set the value of T to interate both T and T2\n",
    "    T_initializer = 6\n",
    "    T_iterator = 6 # I think this should be right, it's just 6?\n",
    "    \n",
    "    # set up a list to eventually turn into a data frame that we'll use to center the points on each row\n",
    "    level_max = []\n",
    "\n",
    "    for i in range(max_level):\n",
    "        level_i = max_level - i\n",
    "        # we also only need to attach these coordinates to values that we want to plot\n",
    "        subset = department_data[(department_data.end_node_level == level_i) & (department_data.to_plot == 1)]\n",
    "        # start P2 at 0, then append values, iterating by the P2_iterator value\n",
    "        P2_static_coord = 0\n",
    "        P2_static_set = []\n",
    "        end_node_id_set = []\n",
    "        # for each entry, increment P2 (the x-coordinate of the node) by 3\n",
    "        # for each entry, increment end_node_id by 1\n",
    "        for j in range(len(subset)):\n",
    "            P2_static_set.append(P2_static_coord)\n",
    "            end_node_id_set.append(end_node_id)\n",
    "            P2_static_coord += P2_static_iterator\n",
    "            end_node_id += 1\n",
    "        if len(subset) > 0:\n",
    "            # have to subtract that iterator back out because it already added the iterator value before the loop ended...\n",
    "            level_max.append([department_list[x],level_i,P2_static_coord - P2_static_iterator]) \n",
    "\n",
    "    # Append the individual lists we just created as new columns of the data set using 'assign'\n",
    "        subset = subset.assign(P2_static = P2_static_set)\n",
    "        subset = subset.assign(end_node_id = end_node_id_set)\n",
    "        # add the T_initializer value for the initial T\n",
    "        subset = subset.assign(T = -T_initializer)\n",
    "        final_data_with_coords = final_data_with_coords.append(subset)\n",
    "\n",
    "    # here is where I use that level_max data that we compiled above to adjust the P2s to be centered on the page \n",
    "    # we have to finalize the P2s here because everyone's P1 coordinate is based on the P2 of their parent position\n",
    "    # determine the maximum P2 value for each level\n",
    "    level_max = pd.DataFrame(level_max,columns=['department','level','max_P2_static'])\n",
    "    # now find the overall maximum value for the department\n",
    "    overall_max = level_max['max_P2_static'].max()\n",
    "    # to horizontally center, determine a level-specific adjustment factor\n",
    "    level_max['P2_static_adjustment'] = (overall_max - level_max['max_P2_static'])/2\n",
    "\n",
    "    # maybe this could be combined with the loop below but I'll keep separate for now.\n",
    "    # go through each record in final_data_with_coords, look up the appropriate P2_adjustment for the level, and add it to P2\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        # go through each row individually, save the row as a series\n",
    "        lookup = final_data_with_coords.iloc[j]\n",
    "        # find the P2_static value for that value\n",
    "        end_node_level_lookup = lookup.get(key = 'end_node_level')\n",
    "        # now find it in the level_max_data frame, and add that value to the existing value of P2\n",
    "        final_data_with_coords.at[final_data_with_coords.index[j], 'P2_static'] += level_max[level_max['level']==end_node_level_lookup]['P2_static_adjustment']\n",
    "    \n",
    "    # now we need to get some information from the position to which each position reports to:\n",
    "    # to get the start node level and id\n",
    "    # reports to details\n",
    "    # And P2 of the parent's node becomes P1 of the position\n",
    "    # we have to look up the end node level and id of the position listed in the 'reports_to_row_id' field\n",
    "\n",
    "    reports_to_details_set = []\n",
    "    P1_static_set = []\n",
    "    sni_set = []\n",
    "    snl_set = []\n",
    "    T2_set = []\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        # go through each row individually, save the row as a series\n",
    "        lookup = final_data_with_coords.iloc[j]\n",
    "        # now determine the reports_to_row_id value within the series\n",
    "        lookup_row_id = lookup.get(key = 'reports_to_row_id')\n",
    "\n",
    "        if pd.isna(lookup_row_id):\n",
    "            # if this position doesn't have a known report up, just list the position's name as the one it reports to\n",
    "            # Start node level and id are the same as the position's end node level and id - hold on there, it needs to be decremented back one but we'll handle that later\n",
    "            # And we can't create T2 until here, the first value has to be based on its parent's position unless we want to increment backwards\n",
    "            rtd_item = lookup.get(key = 'details')\n",
    "            P1_static_item = lookup.get(key = 'P2_static')\n",
    "            sni_item = lookup.get(key = 'end_node_id')\n",
    "            snl_item = lookup.get(key = 'end_node_level')\n",
    "            T2_item = -T_initializer + lookup.get(key = 'end_node_level') * 2 * T_iterator\n",
    "            #    subset = subset.assign(T2 = -T_initializer + (level_i-1)*(T_iterator)*2)\n",
    "        else:\n",
    "            # if this position does have a report up, narrow the original data frame to the row_id with the lookup value\n",
    "            reports_to_info = final_data_with_coords[final_data_with_coords['row_id'] == lookup_row_id]\n",
    "            rtd_item = reports_to_info['details'].iloc[0]\n",
    "            P1_static_item = reports_to_info['P2_static'].iloc[0]\n",
    "            sni_item = reports_to_info['end_node_id'].iloc[0]\n",
    "            snl_item = reports_to_info['end_node_level'].iloc[0]\n",
    "            T2_item = -T_initializer + reports_to_info['end_node_level'].iloc[0] * 2 * T_iterator\n",
    "        reports_to_details_set.append(rtd_item)\n",
    "        P1_static_set.append(P1_static_item)\n",
    "        sni_set.append(sni_item)\n",
    "        snl_set.append(snl_item)\n",
    "        T2_set.append(T2_item)\n",
    "\n",
    "    final_data_with_coords = final_data_with_coords.assign(reports_to_details = reports_to_details_set)\n",
    "    final_data_with_coords = final_data_with_coords.assign(P1_static = P1_static_set)\n",
    "    final_data_with_coords = final_data_with_coords.assign(start_node_id = sni_set)\n",
    "    final_data_with_coords = final_data_with_coords.assign(start_node_level = snl_set)\n",
    "    final_data_with_coords = final_data_with_coords.assign(T2 = T2_set)\n",
    "    \n",
    "    # Build a loop that creates P1_dynamic and P2_dynamic, as well as compiles the Path.\n",
    "    # Also add an empty column of lists to which end_node_ids can be appended to build the path variable\n",
    "    \n",
    "    final_data_with_coords['path_builder'] = [[] for x in range(len(final_data_with_coords))]\n",
    "    \n",
    "    # start by setting P1_ and P2_dynamic to equal their static counterparts for level 1 positions, and add the end_node_id to the path\n",
    "    # actually, let's do that for any node whose end_node_id = start_node id (ie, does not report to anyone)\n",
    "    for y in range(len(final_data_with_coords)):\n",
    "#        if final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_level'] == 1:\n",
    "        if final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_level'] == final_data_with_coords.at[final_data_with_coords.index[y], 'start_node_level']:\n",
    "            final_data_with_coords.at[final_data_with_coords.index[y], 'P1_dynamic'] = final_data_with_coords.at[final_data_with_coords.index[y], 'P1_static']\n",
    "            final_data_with_coords.at[final_data_with_coords.index[y], 'P2_dynamic'] = final_data_with_coords.at[final_data_with_coords.index[y], 'P2_static']\n",
    "            final_data_with_coords.at[final_data_with_coords.index[y], 'path_builder'].append(int(final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_id']))\n",
    "    \n",
    "    # for levels 2 - n, find the number of records that have that level and have a report (IE, they do NOT report to themselves)\n",
    "    P2_dataset = final_data_with_coords[(final_data_with_coords['end_node_level'] > 1) & (final_data_with_coords['end_node_id'] != final_data_with_coords['start_node_id'])]\n",
    "    P2_dataset_levels = P2_dataset.end_node_level.unique()\n",
    "    P2_dataset_levels.sort()\n",
    "    \n",
    "    # Joel: Start here, you're ready to go through the P2 datasets individually.\n",
    "    # Filter final_data_with_coords by the individual P2_dataset_levels values and end_node_id <> start_node_id and also determine the length\n",
    "    \n",
    "    for x in range(len(P2_dataset_levels)):\n",
    "        iterator = 0\n",
    "        for y in range(len(final_data_with_coords)):\n",
    "            if (final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_level'] == P2_dataset_levels[x]) & (final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_level'] != final_data_with_coords.at[final_data_with_coords.index[y], 'start_node_level']):\n",
    "                # look up the P2_dynamic of the parent\n",
    "                lookup = final_data_with_coords.iloc[y]\n",
    "                lookup_row_id = lookup.get(key = 'reports_to_row_id')\n",
    "                reports_to_info = final_data_with_coords[final_data_with_coords['row_id'] == lookup_row_id]\n",
    "                # assign it to both the P1_dynamic and P2_dynamic of the child\n",
    "                # got to restart the iterator if the parent id changes, need to somehow identify that set.\n",
    "                # maybe easier to get this done, then go back and subset every combination and somehow iterate through?\n",
    "                final_data_with_coords['P1_dynamic'].iloc[y] = 1234\n",
    "                final_data_with_coords['P2_dynamic'].iloc[y] = reports_to_info['P2_dynamic'].iloc[0]\n",
    "                \n",
    "                # Bring in the Path from the parent\n",
    "                final_data_with_coords['path_builder'].iloc[y] = reports_to_info['path_builder'].iloc[0]\n",
    "                # append the end_node_id of the child - why is it bringing in everything?\n",
    "                final_data_with_coords['path_builder'].iloc[y].append(int(final_data_with_coords['end_node_id'].iloc[y]))\n",
    "#                final_data_with_coords.at[final_data_with_coords.index[y], 'path_builder'].append(int(final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_id']))\n",
    "#                final_data_with_coords['path_builder'].iloc[y].append(final_data_with_coords['end_row_id'].iloc[y])\n",
    "                iterator += 1\n",
    "\n",
    "# Then increment P2_dynamic by 1. Should be easy to double check. Adjust it by subtracting the length of the data - 1, then dividing by two\n",
    "\n",
    "    # If a position does not report to another position, then T2 needs to be incremented down so that the Y value of the start node gets incremented down to the previous level\n",
    "    # final_data_with_coords['T2'].where(~(final_data_with_coords.end_node_level == final_data_with_coords.start_node_level), other=final_data_with_coords.T2 + 2 * T_initializer, inplace=True)\n",
    "    final_data_with_coords.to_csv('final_data_with_coords' + department_list[x] + '.csv',index=False)\n",
    "\n",
    "    branch_set = []\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        lookup = final_data_with_coords.iloc[j]\n",
    "        branch_item = \"Level \" + str(int(lookup.get(key = 'start_node_level'))) + \" (\" + str(int(lookup.get(key = 'start_node_id'))) + \") - Level \" + str(int(lookup.get(key = 'end_node_level'))) + \" (\" + str(int(lookup.get(key = 'end_node_id'))) + \")\"\n",
    "        branch_set.append(branch_item)\n",
    "\n",
    "    final_data_with_coords = final_data_with_coords.assign(branch = branch_set)\n",
    "\n",
    "    # add in a T2 iterator\n",
    "    final_data_with_coords['T2_iterator'] = .25 * (final_data_with_coords['end_node_level'] - final_data_with_coords['start_node_level'])\n",
    "\n",
    "    department_curve_data = pd.DataFrame()\n",
    "    number_of_rows = 49\n",
    "#    final_data_with_coords.to_csv('final_data_with_coords' + department_list[x] + '.csv',index=False)\n",
    "\n",
    "    # annoying but I'm going to create a unique list of IDs and then filter the data frame by every unique value\n",
    "    # should have the same effect as selecting individual rows (which is a pain because those rows are saved as series and I lose the column names)\n",
    "\n",
    "    # create 49 of each record:\n",
    "    for ind in final_data_with_coords.index:\n",
    "        curve_data = final_data_with_coords.loc[final_data_with_coords.index.repeat(number_of_rows)]\n",
    "\n",
    "    # now go through each individual set of 49 records and iterate T1 (by 0.25) and T2 (by 0.25 * the number of levels)\n",
    "\n",
    "    row_id_list = final_data_with_coords['row_id'].tolist()\n",
    "    for k in range(len(row_id_list)):\n",
    "        iterated_data = curve_data[curve_data['row_id'] == row_id_list[k]]\n",
    "        T_new_set = []\n",
    "        T2_new_set = []\n",
    "        for l in range(len(iterated_data)):\n",
    "            lookup = iterated_data.iloc[l]\n",
    "            T_new_item = lookup.get(key = 'T') + l * .25\n",
    "            T2_new_item = lookup.get(key = 'T2') + l * lookup.get(key = 'T2_iterator')\n",
    "            T_new_set.append(T_new_item)\n",
    "            T2_new_set.append(T2_new_item)\n",
    "        iterated_data = iterated_data.assign(T = T_new_set)\n",
    "        iterated_data = iterated_data.assign(T2 = T2_new_set)\n",
    "        department_curve_data = department_curve_data.append(iterated_data)\n",
    "    \n",
    "    # now append back the records that are not being appended, so that we can still include them in headounts\n",
    "    unplotted_data = department_data[department_data.to_plot != 1]\n",
    "    department_curve_data = department_curve_data.append(unplotted_data)\n",
    "    final_curve_data = final_curve_data.append(department_curve_data)\n",
    "\n",
    "# remove unnecessary fields\n",
    "#final_curve_data.drop(columns=\"T2_iterator\", inplace = True)\n",
    "\n",
    "# attempt to get rid of trailing zeroes (.0)\n",
    "#final_curve_data_fixed = final_curve_data.applymap(lambda cell: int(cell) if str(cell).endswith('.0') else cell)\n",
    "\n",
    "# publish final curve data\n",
    "final_curve_data.to_csv('final_curve_data.csv',index=False)\n",
    "#final_curve_data_fixed.to_csv('final_curve_data_fixed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come up with a way to set the P2 increment to make the plot a roughly consistently rectangular shape\n",
    "# space when reporting to different positions\n",
    "# don't create 49 records for positions that don't report up, just one\n",
    "# create Path logic\n",
    "# Tableau - exploding nodes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final_data_with_coords.to_csv('final_data_with_coords.csv',index=False)\n",
    "\n",
    "#final_data.to_csv('2_DoD_2_level_example_final_data.csv',index=False)\n",
    "\n",
    "#iterated_data.to_csv('iterated_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
