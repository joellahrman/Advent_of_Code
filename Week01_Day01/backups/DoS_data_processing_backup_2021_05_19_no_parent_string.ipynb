{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "department 1 of 10 : DHS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:372: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\jlahrman\\.conda\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:373: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "department 2 of 10 : DoC\n",
      "department 3 of 10 : DoC orig\n",
      "department 4 of 10 : DoE\n",
      "department 5 of 10 : DoE orig\n",
      "department 6 of 10 : DoI\n",
      "department 7 of 10 : DoJ\n",
      "department 8 of 10 : ED\n",
      "department 9 of 10 : USDA\n",
      "department 10 of 10 : zTest 1 simple\n",
      "exception report: 0 rows\n"
     ]
    }
   ],
   "source": [
    "# T: Just runs from -6 to 6, creates the curve shape when passed through the sigmoid function\n",
    "# T2: Sets up the x-coordinates to run from position to parent position, gets iterated similar to T but specific to the end_node_level and start_node_level\n",
    "# P1: This is the y-coordinate of the parent position\n",
    "# P2: This is the y-coordinate of the position\n",
    "# Branch: Still trying to figure out exactly why this field is so important to the Tableau viz...\n",
    "# Stat/Dyn - stat shows the whole org chart, dyn only allows nodes to expand/contract\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# https://stackoverflow.com/questions/44549110/python-loop-through-excel-sheets-place-into-one-df\n",
    "\n",
    "data_file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DoS_Viz/Data/data_for_tableau/'\n",
    "filename = '2020_05_14.xlsx'\n",
    "\n",
    "sheets = pd.read_excel(data_file_loc + filename, sheet_name = None)\n",
    "\n",
    "# blank data frame to append every sheet\n",
    "final_data = pd.DataFrame()\n",
    "\n",
    "# rename level to 'end_node_level'\n",
    "for name, sheet in sheets.items():\n",
    "    sheet['sheet'] = name\n",
    "    sheet = sheet.rename(columns=lambda x: x.split('\\n')[-1])\n",
    "    if name != 'query_results':\n",
    "        sheet.columns = ['row_id','track','position_category','position_name','composition', 'detail_count', 'pos_no','to_plot','inner_count','reports_to_row_id','end_node_level','department']\n",
    "        final_data = final_data.append(sheet)\n",
    "\n",
    "# start a loop here to handle each individual department, and a data frame to hold all final data\n",
    "final_curve_data = pd.DataFrame()\n",
    "department_list = np.sort(final_data.department.unique())\n",
    "exception_report = pd.DataFrame()\n",
    "\n",
    "for x in range(len(department_list)):\n",
    "    print('department', x+1, 'of', len(department_list),':',department_list[x])\n",
    "    department_data = final_data[final_data.department == department_list[x]]\n",
    "   \n",
    "    # add in the \"unknown\" level 0 node, restart each department with an end_node_id of 1000 so they're completely unique\n",
    "    zero_row = {'row_id': 0,\n",
    "                'track': 0,\n",
    "                'position_category': 'Unknown',\n",
    "                'position_name': 'Unknown',\n",
    "                'composition': 'Unknown',\n",
    "                'detail_count': 0,\n",
    "                'pos_no': 0,\n",
    "                'inner_count': 0,\n",
    "                'reports_to_row_id': 0,\n",
    "                'to_plot': 1,\n",
    "                'department': department_list[x],\n",
    "                'end_node_level': 0\n",
    "               }\n",
    "    # append_zero_row\n",
    "    department_data = department_data.append(zero_row, ignore_index = True)\n",
    "    # Joel: Add records that are plotted but do not have a track number to the exception report\n",
    "    # These records don't actually have to be removed, it won't mess up the plots, but they should be called out\n",
    "    no_level_records = department_data[department_data['end_node_level'].isnull() & department_data['to_plot'] == 1]\n",
    "    no_level_records['exception_report_composition'] = 'plotted record with no level'\n",
    "    exception_report = exception_report.append(no_level_records)\n",
    "    # have any record with a blank 'reports_to_id' report to the unknown position (with row_id 0)\n",
    "    department_data['reports_to_row_id'] = department_data['reports_to_row_id'].fillna(0)\n",
    "    # sort by row_id\n",
    "    department_data = department_data.sort_values(by = ['row_id'], ascending = True)\n",
    "    \n",
    "    # Joel: Resort the records to align the points.\n",
    "    #1. Add two blank fields to the data frame: \"sorter\" and \"reports_to_sorter\"\n",
    "    #2. Filter the data to be only the positions to plot\n",
    "    #3. Create a unique sorted list of the tracks in the data, to be used to loop through top to bottom\n",
    "    #4. First row of n positions doesn't need to be sorted, gets sorter value and reports_to_sorter values of 1 through n. Hopefully n will always be 1 (i.e., only one position will be at very top)\n",
    "    #5. Save that first row as a new data frame.\n",
    "    #6. Loop through each track value in the unique list (sorted ascending)\n",
    "        #a. Create a subsetted data frame with track matching the list value\n",
    "        #b. For each value, look up the sorter of the record of the parent in the new data frame, and enter that as the reports_to_sorter of the record\n",
    "        #c. Sort the subsetted data by the reports_to_sorter ascending\n",
    "        #d. Loop through the records, incrementing the sorter by 1 each time (keeping the incrementer from the previous subset)\n",
    "        #e. Append the data to the original 0 data frame\n",
    "    #6. Sort the whole data frame by sorter ascending\n",
    "    #7. Replace the origial department_data data frame with this new sorted one.\n",
    "    #8. Replace row_id with sorter, and reports_to_row_id with reports_to_sorter\n",
    "    #9. Now append all the records that we don't want to plot. Look up their reports_to_row_id values in the sorter and replace.\n",
    "    \n",
    "    # iterate through the levels in reverse order - will probably be better to do it this way when trying to center the parent nodes\n",
    "\n",
    "    # determine maximum level in the data\n",
    "    max_level = int(department_data['end_node_level'].max())\n",
    "    # create blank data frame to append the records with updated fields\n",
    "    final_data_with_coords = pd.DataFrame()\n",
    "    # create a counter for the end_node_id, this just needs to stay unique in the data frame. Restart each department at 1000 so they're unique\n",
    "    end_node_id = (x+1)*1000+1\n",
    "\n",
    "    # Set a value to iterate the P2 coordinate for each row\n",
    "    P2_static_iterator = 3\n",
    "    # Set the value of T to interate both T and T2\n",
    "    T_initializer = -6\n",
    "    T_iterator = 6 # I think this should be right, it's just 6? Probably only need one variable here\n",
    "    \n",
    "    # set up a list to eventually turn into a data frame that we'll use to center the points on each row\n",
    "    level_max = []\n",
    "    \n",
    "    for i in range(0,max_level+1): # got to add 1 here, otherwise it won't get down to the level 0 \"unknown\"\n",
    "        level_i = max_level - i\n",
    "        # we also only need to attach these coordinates to values that we want to plot\n",
    "        subset = department_data[(department_data.end_node_level == level_i) & (department_data.to_plot == 1)]\n",
    "        # start P2 at 0, then append values, iterating by the P2_iterator value\n",
    "        P2_static_coord = 0\n",
    "        P2_static_set = []\n",
    "        end_node_id_set = []\n",
    "        # for each entry, increment P2 (the x-coordinate of the node) by 3\n",
    "        # for each entry, increment end_node_id by 1\n",
    "        for j in range(len(subset)):\n",
    "            P2_static_set.append(P2_static_coord)\n",
    "            end_node_id_set.append(end_node_id)\n",
    "            P2_static_coord += P2_static_iterator\n",
    "            end_node_id += 1\n",
    "        if len(subset) > 0:\n",
    "            # have to subtract that iterator back out because it already added the iterator value before the loop ended...\n",
    "            level_max.append([department_list[x],level_i,P2_static_coord - P2_static_iterator]) \n",
    "\n",
    "    # Append the individual lists we just created as new columns of the data set using 'assign'\n",
    "        subset = subset.assign(P2_static = P2_static_set)\n",
    "        subset = subset.assign(end_node_id = end_node_id_set)\n",
    "        # add the T_initializer value for the initial T\n",
    "        subset = subset.assign(T = T_initializer)\n",
    "        final_data_with_coords = final_data_with_coords.append(subset)\n",
    "                \n",
    "    # here is where I use that level_max data that we compiled above to adjust the P2s to be centered on the page \n",
    "    # we have to finalize the P2s here because everyone's P1 coordinate is based on the P2 of their parent position\n",
    "    # determine the maximum P2 value for each level\n",
    "    level_max = pd.DataFrame(level_max,columns=['department','level','max_P2_static'])\n",
    "    # now find the overall maximum value for the department\n",
    "    overall_max = level_max['max_P2_static'].max()\n",
    "    # to horizontally center, determine a level-specific adjustment factor\n",
    "    level_max['P2_static_adjustment'] = (overall_max - level_max['max_P2_static'])/2\n",
    "\n",
    "    # maybe this could be combined with the loop below but I'll keep separate for now.\n",
    "    # go through each record in final_data_with_coords, look up the appropriate P2_adjustment for the level, and add it to P2\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        # go through each row individually, save the row as a series\n",
    "        lookup = final_data_with_coords.iloc[j]\n",
    "        # find the P2_static value for that value\n",
    "        end_node_level_lookup = lookup.get(key = 'end_node_level')\n",
    "        # now find it in the level_max_data frame, and add that value to the existing value of P2\n",
    "        final_data_with_coords.at[final_data_with_coords.index[j], 'P2_static'] += level_max[level_max['level']==end_node_level_lookup]['P2_static_adjustment']\n",
    "    \n",
    "    # now we need to get some information from the position to which each position reports to:\n",
    "    # to get the start node level and id\n",
    "    # reports to composition\n",
    "    # And P2 of the parent'scomposition node becomes P1 of the position\n",
    "    # we have to look up the end node level and id of the position listed in the 'reports_to_row_id' field\n",
    "\n",
    "    final_data_with_coords['reports_to_composition'] = ''\n",
    "    final_data_with_coords['P1_static'] = ''\n",
    "    final_data_with_coords['start_node_id'] = ''\n",
    "    final_data_with_coords['start_node_level'] = ''\n",
    "    final_data_with_coords['T2'] = ''\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        # go through each row individually, save the row as a series\n",
    "        try:\n",
    "            lookup = final_data_with_coords.iloc[j]\n",
    "            # now determine the reports_to_row_id value within the series\n",
    "            lookup_row_id = lookup.get(key = 'reports_to_row_id')\n",
    "\n",
    "            if pd.isna(lookup_row_id):\n",
    "                # if this position doesn't have a known report up, just list the position's name as the one it reports to\n",
    "                # Start node level and id are the same as the position's end node level and id - hold on there, it needs to be decremented back one but we'll handle that later\n",
    "                # And we can't create T2 until here, the first value has to be based on its parent's position unless we want to increment backwards\n",
    "\n",
    "                final_data_with_coords['reports_to_composition'].iloc[j] = lookup.get(key = 'composition')\n",
    "                final_data_with_coords['P1_static'].iloc[j] = lookup.get(key = 'P2_static')\n",
    "                final_data_with_coords['start_node_id'].iloc[j] = lookup.get(key = 'end_node_id')\n",
    "                final_data_with_coords['start_node_level'].iloc[j] = lookup.get(key = 'end_node_level')\n",
    "\n",
    "                # subtracting 1 from the end_node_level eventually allows T2 to start at -6\n",
    "                # I'm not sure there is a good reason for this except to have it match T which runs from -6 to 6\n",
    "                final_data_with_coords['T2'].iloc[j] = T_initializer + (lookup.get(key = 'end_node_level')-1) * 2 * T_iterator\n",
    "            else:\n",
    "                # if this position does have a report up, narrow the original data frame to the row_id with the lookup value\n",
    "                reports_to_info = final_data_with_coords[final_data_with_coords['row_id'] == lookup_row_id]\n",
    "                final_data_with_coords['reports_to_composition'].iloc[j] = reports_to_info['composition'].iloc[0]\n",
    "                final_data_with_coords['P1_static'].iloc[j] = reports_to_info['P2_static'].iloc[0]\n",
    "                final_data_with_coords['start_node_id'].iloc[j] = reports_to_info['end_node_id'].iloc[0]\n",
    "                final_data_with_coords['start_node_level'].iloc[j] = reports_to_info['end_node_level'].iloc[0]\n",
    "                final_data_with_coords['T2'].iloc[j] = T_initializer + (reports_to_info['end_node_level'].iloc[0]-1) * 2 * T_iterator        \n",
    "        except:\n",
    "            final_data_with_coords['reports_to_composition'].iloc[j] = 'exception'\n",
    "                            \n",
    "    # Send \"exception\" to exception report\n",
    "    reports_to_unplotted_node = final_data_with_coords[final_data_with_coords.reports_to_composition == 'exception']\n",
    "    reports_to_unplotted_node = reports_to_unplotted_node[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "    reports_to_unplotted_node['exception_report_composition'] = 'reports to unplotted parent'\n",
    "    exception_report = exception_report.append(reports_to_unplotted_node)\n",
    "    # Then remove \"remove\" from final_data_with_coords\n",
    "    final_data_with_coords = final_data_with_coords[final_data_with_coords.reports_to_composition != 'exception']\n",
    "                        \n",
    "    # These fields were initiated as text, need to convert to numeric\n",
    "    final_data_with_coords[['P1_static', 'start_node_id', 'start_node_level','T2']] = final_data_with_coords[['P1_static', 'start_node_id', 'start_node_level','T2']].apply(pd.to_numeric)\n",
    "        \n",
    "    # Sort by row_id ascending.\n",
    "    final_data_with_coords = final_data_with_coords.sort_values(by = ['row_id'], ascending = True)\n",
    "    # If no parent, or parent marked with \"to remove\", then mark as \"remove\".\n",
    "    # Have to ignore row_id 0 though, otherwise it will cascade down to everything!\n",
    "    for w in range(len(final_data_with_coords)):\n",
    "        lookup = final_data_with_coords['start_node_id'].iloc[w]\n",
    "        end_node_id_lookup = final_data_with_coords[final_data_with_coords.end_node_id == lookup]\n",
    "        try:\n",
    "            if end_node_id_lookup['row_id'].iloc[0] != 0 and end_node_id_lookup['reports_to_composition'].iloc[0] == 'remove':\n",
    "                final_data_with_coords['reports_to_composition'].iloc[w] = 'remove'\n",
    "        except:\n",
    "            final_data_with_coords['reports_to_composition'].iloc[w] = 'remove'\n",
    "    # Send \"remove\" to exception report\n",
    "    ascending_wash_upstream_unplotted_parent = final_data_with_coords[final_data_with_coords.reports_to_composition == 'remove']\n",
    "    ascending_wash_upstream_unplotted_parent = ascending_wash_upstream_unplotted_parent[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "    ascending_wash_upstream_unplotted_parent['exception_report_composition'] = 'unplotted upstream parent - ascending wash'\n",
    "    exception_report = exception_report.append(ascending_wash_upstream_unplotted_parent)\n",
    "    # Then remove \"remove\" from final_data_with_coords\n",
    "    final_data_with_coords = final_data_with_coords[final_data_with_coords.reports_to_composition != 'remove']\n",
    "            \n",
    "    # Sort by row_id descending and repeat\n",
    "    # Joel - if you were even remotely competent you would build a function and pass the sorted data frame in twice\n",
    "    final_data_with_coords = final_data_with_coords.sort_values(by = ['row_id'], ascending = False)\n",
    "    # If no parent, or parent marked with \"to remove\", then mark as \"to remove\"\n",
    "    for w in range(len(final_data_with_coords)):\n",
    "        lookup = final_data_with_coords['start_node_id'].iloc[w]\n",
    "        end_node_id_lookup = final_data_with_coords[final_data_with_coords.end_node_id == lookup]\n",
    "        try:\n",
    "            if end_node_id_lookup['row_id'].iloc[0] != 0 and end_node_id_lookup['reports_to_composition'].iloc[0] == 'remove':\n",
    "                final_data_with_coords['reports_to_composition'].iloc[w] = 'remove'\n",
    "        except:\n",
    "            final_data_with_coords['reports_to_composition'].iloc[w] = 'remove'\n",
    "    # Send \"remove\" to exception report\n",
    "    descending_wash_upstream_unplotted_parent = final_data_with_coords[final_data_with_coords.reports_to_composition == 'remove']\n",
    "    descending_wash_upstream_unplotted_parent = descending_wash_upstream_unplotted_parent[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "    descending_wash_upstream_unplotted_parent['exception_report_composition'] = 'unplotted upstream parent - descending wash'\n",
    "    exception_report = exception_report.append(descending_wash_upstream_unplotted_parent)\n",
    "    # Then remove \"remove\" from final_data_with_coords\n",
    "    final_data_with_coords = final_data_with_coords[final_data_with_coords.reports_to_composition != 'remove']    \n",
    "\n",
    "    # Build a loop that creates P1_dynamic and P2_dynamic, as well as compiles the Path.\n",
    "    # Also add an empty column of lists to which end_node_ids can be appended to build the path variable\n",
    "    # Add a parent_path in case we want to toggle between paths on the viz\n",
    "    \n",
    "    final_data_with_coords['path'] = ''\n",
    "    final_data_with_coords['parent_path'] = ''\n",
    "    \n",
    "    # start by setting P1_ and P2_dynamic to equal their static counterparts for level 1 positions and add the end_node_id to the path\n",
    "    # Zctually, let's do that for any node whose end_node_id = start_node id (ie, does not report to anyone)\n",
    "    # BUT, because we now have the \"unknown\" zero level, the block below only really gets applied to level 0\n",
    "    # since that is the only node for which the start node and end node have the same level\n",
    "    # if we wanted to \"lock\" additional levels at the top of the viz we could do it in this section, probably wouldn't be too hard to adjust\n",
    "    for y in range(len(final_data_with_coords)):\n",
    "        if final_data_with_coords.at[final_data_with_coords.index[y], 'end_node_level'] == final_data_with_coords.at[final_data_with_coords.index[y], 'start_node_level']:\n",
    "            final_data_with_coords.at[final_data_with_coords.index[y], 'P1_dynamic'] = final_data_with_coords.at[final_data_with_coords.index[y], 'P1_static']\n",
    "            final_data_with_coords.at[final_data_with_coords.index[y], 'P2_dynamic'] = final_data_with_coords.at[final_data_with_coords.index[y], 'P2_static']\n",
    "\n",
    "    # for levels not at the head, find the number of records that have that level and have a report (IE, they do NOT report to themselves)\n",
    "    # HAVE to sort by start_node_id descending, that way it starts with the higher end_node_id values which are the higher levels\n",
    "    P2_dataset = final_data_with_coords[(final_data_with_coords['end_node_level'] != final_data_with_coords['end_node_level'].min()) & (final_data_with_coords['end_node_id'] != final_data_with_coords['start_node_id'])]\n",
    "    P2_start_node_id = P2_dataset['start_node_id'].value_counts().rename_axis('start_node_id').reset_index(name = 'total')\n",
    "    P2_start_node_id['counter'] = 0\n",
    "    P2_start_node_id = P2_start_node_id.sort_values(by = ['start_node_id'], ascending = False)\n",
    "        \n",
    "    # Now with P2_start_node_id, we have to loop through the original data set to find all records with the start_node_id\n",
    "    for g in range(len(P2_start_node_id)):\n",
    "        for h in range(len(final_data_with_coords)):\n",
    "            if ((final_data_with_coords['start_node_id'].iloc[h] == P2_start_node_id['start_node_id'].iloc[g]) and (final_data_with_coords['start_node_id'].iloc[h] != final_data_with_coords['end_node_id'].iloc[h])):\n",
    "                # set P1 to match P2 of parent. Look up start_row_id of the position\n",
    "                lookup = final_data_with_coords.iloc[h]\n",
    "                end_node_id_lookup = lookup.get(key = 'start_node_id')\n",
    "                # now find the matching end_node_id for the parent, and set that parent's P2 to be the child's P1\n",
    "                parent_info = final_data_with_coords[final_data_with_coords.end_node_id == end_node_id_lookup]\n",
    "                try:\n",
    "                    final_data_with_coords['P1_dynamic'].iloc[h] = parent_info['P2_dynamic'].iloc[0]\n",
    "                    # also set it to be the initial P2 for the child, but have to adjust so that the row are spread horizontally\n",
    "                    final_data_with_coords['P2_dynamic'].iloc[h] = parent_info['P2_dynamic'].iloc[0] + P2_start_node_id['counter'].iloc[g] - (P2_start_node_id['total'].iloc[g]-1)/2\n",
    "                    P2_start_node_id['counter'].iloc[g] += 1\n",
    "                    # bring in the path from the parent\n",
    "                    final_data_with_coords['path'].iloc[h] = parent_info['path'].iloc[0]\n",
    "                    final_data_with_coords['parent_path'].iloc[h] = parent_info['path'].iloc[0]\n",
    "                    # append the end_node_id of the current row\n",
    "                    # in case of skipping levels, add in '-0' so that positions on the same level always have the same number of characters on their path\n",
    "                    filler_length = (final_data_with_coords['end_node_level'].iloc[h] - final_data_with_coords['start_node_level'].iloc[h]-1).astype(int)\n",
    "                    filler_string = '-0'\n",
    "                    if filler_length > 0:\n",
    "                        filler = filler_string * filler_length\n",
    "                    else:\n",
    "                        filler = ''\n",
    "                    final_data_with_coords['path'].iloc[h] = final_data_with_coords['path'].iloc[h] + filler_string * filler_length + '-' + str(int(final_data_with_coords['end_node_id'].iloc[h]))\n",
    "                except:\n",
    "                    exception_report_item = final_data_with_coords.iloc[h]\n",
    "                    exception_report_item = exception_report_item[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "                    exception_report['exception_report_composition'] = 'unplotted upstream parent - how did this slip through'\n",
    "                    exception_report = exception_report.append(exception_report_item)\n",
    "                    final_data_with_coords['reports_to_composition'].iloc[h] = 'exception'\n",
    "                    \n",
    "   # Remove data from final_data_with_coords thave have an upstream unplotted parent\n",
    "    final_data_with_coords = final_data_with_coords[(final_data_with_coords.reports_to_composition != 'exception')]\n",
    "\n",
    "    # Add exceptions for positions that report laterally\n",
    "    lateral_reports = final_data_with_coords[(final_data_with_coords.end_node_level == final_data_with_coords.start_node_level) & (final_data_with_coords.end_node_id != final_data_with_coords.start_node_id)]\n",
    "    if len(lateral_reports) > 0:\n",
    "        lateral_reports = lateral_reports[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "        lateral_reports['exception_report_composition'] = 'reports laterally'\n",
    "        exception_report = exception_report.append(lateral_reports)\n",
    "    # Add exceptions for positions that report laterally\n",
    "    downward_reports = final_data_with_coords[final_data_with_coords.end_node_level < final_data_with_coords.start_node_level]\n",
    "    if len(downward_reports) > 0:\n",
    "        downward_reports = downward_reports[['department','composition','end_node_id','end_node_level','pos_no','position_category','position_name','reports_to_composition','reports_to_row_id','row_id','start_node_id','start_node_level']]\n",
    "        downward_reports['exception_report_composition'] = 'reports downward'\n",
    "        exception_report = exception_report.append(downward_reports)\n",
    "        \n",
    "    # Need to add final hyphens, allow for 50 hyphens which requires 51 total hyphens\n",
    "    allowed_levels = 50\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        needed_hyphens = int(allowed_levels + 1 - final_data_with_coords['end_node_level'].iloc[j])\n",
    "        final_data_with_coords['path'].iloc[j] = str(final_data_with_coords['path'].iloc[j]) + '-' * needed_hyphens\n",
    "        needed_parent_hyphens = int(allowed_levels + 1 - final_data_with_coords['start_node_level'].iloc[j])\n",
    "        final_data_with_coords['parent_path'].iloc[j] = str(final_data_with_coords['parent_path'].iloc[j]) + '-' * needed_parent_hyphens\n",
    " \n",
    "    # Now we've got all the coordinates set, we can build the 49 records needed to create the sigmoid effect by incrementing T2\n",
    "    # If a position does not report to another position, then T2 needs to be incremented down so that the Y value of the start node gets incremented down to the previous level\n",
    "    # final_data_with_coords['T2'].where(~(final_data_with_coords.end_node_level == final_data_with_coords.start_node_level), other=final_data_with_coords.T2 + 2 * T_initializer, inplace=True)\n",
    "\n",
    "    branch_set = []\n",
    "\n",
    "    for j in range(len(final_data_with_coords)):\n",
    "        lookup = final_data_with_coords.iloc[j]\n",
    "        branch_item = \"Level \" + str(int(lookup.get(key = 'start_node_level'))) + \" (\" + str(int(lookup.get(key = 'start_node_id'))) + \") - Level \" + str(int(lookup.get(key = 'end_node_level'))) + \" (\" + str(int(lookup.get(key = 'end_node_id'))) + \")\"\n",
    "        branch_set.append(branch_item)\n",
    "    final_data_with_coords = final_data_with_coords.assign(branch = branch_set)\n",
    "\n",
    "    # add in a T2 iterator\n",
    "    final_data_with_coords['T2_iterator'] = .25 * (final_data_with_coords['end_node_level'] - final_data_with_coords['start_node_level'])\n",
    "\n",
    "    # annoying but I'm going to create a unique list of IDs and then filter the data frame by every unique value\n",
    "    # should have the same effect as selecting individual rows (which is a pain because those rows are saved as series and I lose the column names)\n",
    "    \n",
    "    # first we can remove \"head\" nodes - positions to which other positions report but do not report to any positions themselves\n",
    "    # actually, let's not remove them, which would throw off the headcounts. But we won't iterate them.\n",
    "    # they will have nodes drawn by the positions that report to them\n",
    "    # these will be identified by two characteristics:\n",
    "    # 1) start_node_id = end_node_id, indicating that the position does not report to another position\n",
    "    # 2) the count of start_node_ids in the dataset matching the position's dataset is greater than one, indicating that the position has subordinates\n",
    "\n",
    "    # first determine the number of reports each position has\n",
    "    number_reporting_to_start_node_id_set = []    \n",
    "    \n",
    "    # strip leading and trailing whitespace from selected fields\n",
    "    final_data_with_coords['position_category'] = final_data_with_coords['position_category'].str.strip()\n",
    "    \n",
    "    # determine number of reports for each position, append to number_reporting_to_start_node_id_set\n",
    "    for i in range(len(final_data_with_coords)):\n",
    "        #Should pos_snid be end_node_id, not start_node_id?\n",
    "        #Otherwise we're finding how many people report to the person this position reports to.\n",
    "        #OK ONLY if we're using to find whether it's a head node.\n",
    "        pos_snid = final_data_with_coords['start_node_id'].iloc[i]\n",
    "        reports_count = final_data_with_coords.apply(lambda x: True if x['start_node_id']==pos_snid else False, axis = 1)\n",
    "        num_reports = len(reports_count[reports_count==True].index)\n",
    "        number_reporting_to_start_node_id_set.append(num_reports)\n",
    "    # append the number_reporting_to_start_node_id_set to final_data_with coords.\n",
    "    final_data_with_coords = final_data_with_coords.assign(number_reporting_to_start_node_id = number_reporting_to_start_node_id_set)\n",
    "    # move the \"unknown\" nodes with an end_node_level of 0 to the upper left corner\n",
    "    # actually maybe keep them in the middle, might make it easier to plot things later\n",
    "#    final_data_with_coords.loc[final_data_with_coords['end_node_level'] == 0, ['P2_static','P2_dynamic']] = 0    \n",
    "#    final_data_with_coords.loc[final_data_with_coords['start_node_level'] == 0, ['P1_static','P1_dynamic']] = 0    \n",
    "    # create a boolean to find records where number of reports > 1 and start_node_id = end_node_id\n",
    "    final_data_with_coords['head_position'] = (final_data_with_coords['start_node_id']==final_data_with_coords['end_node_id']) & (final_data_with_coords['number_reporting_to_start_node_id']>1)\n",
    "    # records to remove are TRUE, only keep records where that boolean is FALSE\n",
    "    head_nodes = final_data_with_coords[final_data_with_coords['head_position']==True]\n",
    "    head_nodes['node'] = 1\n",
    "    head_nodes['node_type'] = 'head'\n",
    "\n",
    "    # Filter head nodes out, remove all coordinates, and append them back in later\n",
    "    # At this point only the \"Unknown\" level 0 nodes should be head nodes\n",
    "    final_data_with_coords = final_data_with_coords[final_data_with_coords['head_position']==False]\n",
    "\n",
    "    # determine number of PLOTTED reports for each position, append to data frame\n",
    "    # this is to make it easy for Tableau to differentiate between managers and non-managers for visual purposes\n",
    "    # determine number of nodes in the level for each position, append to data frame\n",
    "    # this will probably come in handy in Tableau\n",
    "    final_data_with_coords['plotted_reports'] = ''\n",
    "    final_data_with_coords['nodes_in_level'] = ''    \n",
    "    for i in range(len(final_data_with_coords)):\n",
    "        reportees_count = final_data_with_coords.apply(lambda x: True if x['start_node_id']==final_data_with_coords['end_node_id'].iloc[i] else False, axis = 1)\n",
    "        nodes_in_level_count = final_data_with_coords.apply(lambda x: True if x['end_node_level']==final_data_with_coords['end_node_level'].iloc[i] else False, axis = 1)\n",
    "        final_data_with_coords['plotted_reports'].iloc[i] = len(reportees_count[reportees_count==True].index)\n",
    "        final_data_with_coords['nodes_in_level'].iloc[i] = len(nodes_in_level_count[nodes_in_level_count==True].index)\n",
    "\n",
    "    final_data_with_coords.to_csv('final_data_with_coords' + department_list[x] + '.csv',index=False)\n",
    "    \n",
    "    # now the only rows left where the start_node_id matches the end_node_id should be \"floating\" points\n",
    "    # filter those out, they won't have a curve so then don't need to be iterated below\n",
    "    floating_nodes = final_data_with_coords[(final_data_with_coords.end_node_id == final_data_with_coords.start_node_id)]\n",
    "    floating_nodes['node'] = 1\n",
    "    floating_nodes['node_type'] = 'floating'\n",
    "    line_data = final_data_with_coords[(final_data_with_coords.end_node_id != final_data_with_coords.start_node_id)]\n",
    "    line_data['node'] = 1    \n",
    "    line_data['node_type'] = 'connector'\n",
    "    \n",
    "    # but now let's relabel end nodes and overwrite that 'connector' value, will probably make life easier\n",
    "    identify_end_nodes_set = []\n",
    "    for i in range(len(line_data)):\n",
    "        reports_count = line_data.apply(lambda x: True if x['start_node_id']==line_data['end_node_id'].iloc[i] else False, axis = 1)\n",
    "        num_reports = len(reports_count[reports_count==True].index)\n",
    "        identify_end_nodes_set.append(num_reports)\n",
    "    # append the number_reporting_to_start_node_id_set to final_data_with coords. We'll remove it later after filtering out the rows\n",
    "    line_data = line_data.assign(identify_end_nodes = identify_end_nodes_set)\n",
    "    # create a boolean to find records where number of reports 0\n",
    "    line_data['end_position'] = (line_data['identify_end_nodes']==0)   \n",
    "    # change node_type for those records, delete unnecessary fields\n",
    "    line_data.loc[line_data['end_position'] == True, 'node_type'] = 'end'\n",
    "    line_data.drop(columns=\"identify_end_nodes\", inplace = True)\n",
    "    line_data.drop(columns=\"end_position\", inplace = True)\n",
    "    \n",
    "# now create the iterated points for the sigmoid lines    \n",
    "\n",
    "    department_curve_data = pd.DataFrame()\n",
    "    number_of_rows = 49\n",
    "    \n",
    "    # create 49 of each record:\n",
    "    for ind in line_data.index:\n",
    "        curve_data = line_data.loc[line_data.index.repeat(number_of_rows)]\n",
    "        curve_data['node'] = 0\n",
    "        curve_data['node_type'] = 'na'\n",
    "        \n",
    "    # now go through each individual set of 49 records and iterate T1 (by 0.25) and T2 (by 0.25 * the number of levels)\n",
    "\n",
    "    row_id_list = final_data_with_coords['row_id'].tolist()\n",
    "    for k in range(len(row_id_list)):\n",
    "        iterated_data = curve_data[curve_data['row_id'] == row_id_list[k]]\n",
    "        T_new_set = []\n",
    "        T2_new_set = []\n",
    "        for l in range(len(iterated_data)):\n",
    "            lookup = iterated_data.iloc[l]\n",
    "            T_new_item = lookup.get(key = 'T') + l * .25\n",
    "            T2_new_item = lookup.get(key = 'T2') + l * lookup.get(key = 'T2_iterator')\n",
    "            T_new_set.append(T_new_item)\n",
    "            T2_new_set.append(T2_new_item)\n",
    "        iterated_data = iterated_data.assign(T = T_new_set)\n",
    "        iterated_data = iterated_data.assign(T2 = T2_new_set)\n",
    "        department_curve_data = department_curve_data.append(iterated_data)\n",
    "    \n",
    "    # now append back the records that are not being appended, so that we can still include them in headounts\n",
    "    unplotted_data = department_data[department_data.to_plot != 1]\n",
    "    department_curve_data = department_curve_data.append(unplotted_data)\n",
    "    # then put in the floating nodes and head nodes: DON'T APPEND IF WE'RE DOING THE Unknown at the top\n",
    "    department_curve_data = department_curve_data.append(floating_nodes)\n",
    "#    department_curve_data = department_curve_data.append(head_nodes)\n",
    "    # put the line_data back in, to reattach the standard notes\n",
    "    # only need this if we're going to make additional \"node\" points for the end and connector nodes\n",
    "#    department_curve_data = department_curve_data.append(line_data)\n",
    "    # and append to the master set\n",
    "    final_curve_data = final_curve_data.append(department_curve_data)\n",
    "\n",
    "# remove unnecessary fields\n",
    "final_curve_data.drop(columns='T2_iterator', inplace = True) # don't need to show\n",
    "final_curve_data.drop(columns='number_reporting_to_start_node_id', inplace = True) # this was just to filter some records out, we don't need it in the final data\n",
    "final_curve_data.drop(columns='head_position', inplace = True) # this was just to filter some records out, we don't need it in the final data\n",
    "final_curve_data.drop(columns=['node','node_type'], inplace = True) # this was just to filter some records out, we don't need it in the final data\n",
    "exception_report.drop(columns=['start_node_id','start_node_level','reports_to_composition'], inplace = True) # this was just to filter some records out, we don't need it in the final data\n",
    "# convert a few columns to integer\n",
    "final_curve_data[['row_id', 'end_node_level','detail_count','pos_no','inner_count','reports_to_row_id','start_node_id','start_node_level']] = final_curve_data[[\"row_id\", \"end_node_level\",\"detail_count\",\"pos_no\",\"inner_count\",\"reports_to_row_id\",\"start_node_id\",\"start_node_level\"]].apply(pd.to_numeric)\n",
    "\n",
    "# publish final curve data\n",
    "#final_curve_data.to_csv('final_curve_data.csv',index=False)\n",
    "\n",
    "# send a copy over to the dashboard file\n",
    "file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DoS_Viz/4_dashboard/'\n",
    "final_curve_data.to_csv(file_loc + 'final_curve_data.csv',index=False)\n",
    "exception_report.to_csv(file_loc + 'exception_report.csv',index=False)\n",
    "\n",
    "print('exception report:',len(exception_report), 'rows')\n",
    "# send a copy over to the dashboard file\n",
    "#file_loc = 'C:/Users/jlahrman/OneDrive - LMI/Documents/DoS_Viz/4_dashboard_recast/'\n",
    "#final_curve_data.to_csv(file_loc + 'final_curve_data_recast.csv',index=False)\n",
    "\n",
    "# if we process the data more to remove zeroes we can copy it here...\n",
    "# attempt to get rid of trailing zeroes (.0)\n",
    "# final_curve_data_fixed = final_curve_data.applymap(lambda cell: int(cell) if str(cell).endswith('.0') else cell)\n",
    "# final_curve_data_fixed.to_csv('final_curve_data_fixed.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# come up with a way to set the P2 increment to make the plot a roughly consistently rectangular shape? Maybe not necessary\n",
    "\n",
    "# space when reporting to different positions\n",
    "# additional items to exception report:\n",
    "    # start_node_level higher than end_node level (position reports to a lower level)\n",
    "    # end_node_level reports to same end_node_level, different end_node_id (position reports horizontally)\n",
    "\n",
    "# Do we need one point for head nodes, would that make aspects of the Tableau viz easier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
